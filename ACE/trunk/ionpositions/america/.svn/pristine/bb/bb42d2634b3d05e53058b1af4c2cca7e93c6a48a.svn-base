%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
NOTE:  These notes have been moved to

ionstream:~jraines/ace/axlv2/devel/AXLV2_Devel_Notes.txt

No further entries should be made in this file.

Jim Raines, 21Jul2003
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

===> Keep-at-Top To Do

 for Boxrates for TZ
 -------------------

 o 02Jul2003 - Verify that these boxrates are what I was getting before
 o 02Jul2003 - Verify the condition of the experimental FM for these ions
 - 02Jul2003 - Verify that these numbers seem reasonable
      07Jul -- They are continuous peaks, see axlv2_br_
      09Jul -- The fit of O, Fe and C looks good; so this is good enough.
 o 02Jul2003 - If easy, change boxrates to box counts for Thomas.  
   (I guess I could multiply by the number of cycles * 12s/cyc.)
 x 07Jul2003 - Build a plotter for these rates from the matrix rate
	       plotter I just made.

 for FM esd correction
 ---------------------
 - figure out why Carbon gets worse after C5+ fitting.  See 
   axfmtweak/runs/18Oct2002-writing-fmfit/ins_C_coef_again/{fmcomp,fmfit}.*.ps

 - fix fmcomp so that it looks at both fm centers and pc centers to
   determine plotting frames

 Rest:
 -----

 x 5Aug2002 - disentangle libhef and ACE-specific stuff from
   AnalysisData.  This libhef stuff could be put in class Pha (along
   with loadPha while we are at it).  The values could still be stored
   in an AnalysisData object, by passing an instance into loadPha and
   using a bunch of set functions for pav, fm, etc.  (Care would need
   to be taken to minimize the risk of misaligning fm data and the
   ad.ions vector.)


 x 16May2002 - check out forward model:
   o using right PAPS; use Simon's?
   o read FM technote
   o run on other, nominal days
   o trace through code to eliminate mishandling

 - Change functions with pointer arguments (e.g. the .calc functions)
 so they receive constant references, e.g.

   const vector<Ion>& grp

   [This means the functions have to be const if they are to be
   called.]

 - clean up method of returning single rates (by name maybe?)

===> Keep-at-Top Outstanding Problems

 - forward model is missing H, He, Mg and S and rest are mis-aligned
   after pav increase

 - reading over multiple analysis intervals causes number of pha's not
   to align with svsbare.pl anymore.  The difference between
   svsbare.pl and axlv2 is +/- for different intervals
   -- is this fixed now? 19Feb2002 --

 - 30Sep2002 -- why using USWX deltaE/q = .043 instead of Simon's of
.035 from sxd (see dist. func. calc.; top of file lists .043)?  

===> Keep-at-Top Plan for 1 month of good data in January 2003
[File under current date when finished.]
Plan made:       8Nov2002
Plan completed:  we can only hope by 15Jan2002.

 o Tune FM to reasonable fit for important species
   o existing atoms -- Si(again),C(again), N, Ne
   o add another subsection in fmcomp
   o existing atoms -- He
   o non-existing atoms -- H, Mg, S

 o Include duty cycle
   o find aspect angle
   o find angle between bore sight vector and ACE spin axis from eng. docs
   o include ACE orbit data into axlv2

 o Try out Ruedi's random assigment method in Prob. rates

 o Verify that Ruedi's SpillRates method is what we used

 o Fix densities

 o Validate results -- how!!!!!?????
   o pray that C/O and O7+/O6+ look reasonable with better FM
   o try Thomas' suggestion of watching spill rates for isolated Fe species
 
 o Further implement C/O ratio as a tracer of method performance

 o Include calibration data -- efficiencies
    o assess current state of our knowledge 
    o find incident beam for all files
    o turn the crank and find efficiency curves for each ion
    o fit eff. curves to functions
    o incorporate this into DistFunc

 o Finish axlv2_2hr

 o Make a runner perl program for this data that smartly restarts
   after failures

 o Run code -- approx. 10 days required

===> 10Jul2003 re-wrote axlv2.cc to process hourly

After successfully producing hourly boxrates (12Jun2003 entry), I
moved axlv2_boxrates.cc to axlv2.cc thus making it the main branch of
development.  I preserved the old, well-used and trust-worthy axlv2.cc
as axlv2_daily.cc.  I updated CVS with these changes.  

I also copied rp_axlv2_br.pro into axlv2/tools/axlv2_rates.pro and put
it into CVS, since it seemed useful.  When updating CVS, I noticed
that fmcomp.pro had been modified, a fair amount, but not commited.  I
went ahead and did the commit, because I remember doing the mod.s
early this year.  I think the new version still works fine since I ran
it to check the forward model fit for the boxrates (12Jun2003 entry).

===> 12Jun2003 producing hourly boxrates for Thomas to check FM

I looked at axlv2_2hr.cc to see if that was a useable staring point
but found that it was pretty out of date with respect to axlv2.cc,
especially with regard to AnalysisData changes.

I decided to start with axlv2.cc and set it up to produce hourly
boxrates.  Essentially that means running the boxrates calc. after
each analysis interval (set at 5 gives 5cyc*12min/cyc = 60min time res.).

In doing so, I took out some of the debugging features that
no longer made sense, .i.e., 

loading MA -- would take longer than accum. again for such a small
number of cycles

looping over days? -- should this be possible?  What is gained?  I
don't know but I left it in. 

I put the new version in axlv2_boxrates.cc (rather than a new version
of axlv2.cc) since it is not yet complete.

02Jul2003:
----------
The code (axlv2_boxrates) works now.  I had make some modifications to
swindal::AceSwicsData.cc, which made me nervous because I'm rusty.
Changed the time stored in each Pha structure (AceSwicsData::load) to
seconds since 1970 (like libhef) from seconds since 1950.  The latter
was for compatibility with Nathan's code, which has no bearing now.  I
alsow added a conversion function, ss1970_to_doyfr, to convert to day
of year fraction.  It is really just a wrapper for libhef::sec70_time.

I double checked that one analysis interval at a time was being output
into the boxrates file:  There are 29 ions and 58 steps (59 - 2 + 1).
As such, there should be 1682 lines in the file if *one* analysis
interval is represented.  There is one header line and 1683 lines in
the file, which is just right.

I also tried the code over a whole day -- it seems to work fine.  For
2002100, there are 33641 lines in the file:

33641 - 1 header = 33640

33640 / 1682 = 20 (exactly)

So, there should be 100 good cycles in the file, at 5 cycles per
analysis interval, which is appropriate for 1hr resolution
(5cyc*12min/cyc=60min).

Now I need to do the following:

1.  Verify that these boxrates are what I was getting before
2.  Verify the condition of the experimental FM for these ions
3.  Verify that these numbers seem reasonable

I committed the changes to CVS for swindal, though it took some work.
I had never committed the changes I made to switch from the loadPha.cc
function to the AceSwicsData.cc object.

7Jul2003:
---------
Runs are being generated in ~/ace/axlv2/runs/01Jul2003-br-for-tz/.

Addressing issue 3 above:  The boxrate peaks are contiuous.  I wrote
rp_axlv2_br.pro to plot these and demonstrate this fact.  See
axlv2_br_2002100.{ps,pdf}.

9Jul2003:
---------
I checked out the forward model alignment and it looks good, at least
for the big O, C and Fe ions.  See
~/ace/axfmtweak/runs/09Jul2003-boxrates-for-tz/fmcomp.axfmtweak_find_Si_coef_2001001_2001365.ps.

I also added a creation time/date tag and modification of xfmexp.cc
time/date tag to the axlv2_br file header.  And, I put the start ydoy
into the br filename.

To facilitate comparisons between these basic rates and the ones from
the last time I tried to check basic rates, I had rp_axlv2_br write
out a file (axlv2_br_2002100_total.dat) that contains sums of all the
basic rates over the day.  I compared the output with the table given
in 8Aug2002 entry.  They were not the same.  I realized this was
because I am using the experimental FM .  I switched to the
orig. (Hefti) FM and they became closer, but still not the same.  I
then realized that I had found a bug in basic rate weighting on
25Aug2002, making the baseline data hard to use.

Here are a few comparisions:
	   8Aug2002	09Jul2003(orig FM)	09Jul2003(exp. FM)
	   --------	------------------	------------------
20Ne8+	   7.46e+00	8.01E+00		3.25E-01
O6+	   1.15e+01	2.33E+01		2.75E+00
O7+	   2.36e+01	1.57E+01		1.07E+00
C5+	   1.15e+01	1.18E+01		5.97E-01
N5+	   2.80e+00	5.11E+00		8.47E-01
N6+	   1.63e+01	1.27E+01		8.69E-01

I think the alignment between 8Aug and now w/orig. FM are
encouraging -- I think everything is basically on track.  I am a
little distured by the values with the exp. FM.  They have *all* gone
down, which seems suspicious, though not conclusive because this is a
small subset of the values.

10Jul2003:
----------
I moved this code, axlv2_boxrates.cc, to the main branch of
development by renaming it to axlv2.cc (after renaming that to
axlv2_daily.cc).  This makes sense since the boxrate processor is the
model for the full up LV2 processor.  In fact, it doesn't need much in
the way of changes, except for finishing outputLV2.cc -- and making the
swindal code efficient enough to run each hour for a whole day in less
then a day!

I also added outputPR.cc to try to get a look at the prob. rates.

===> 12Jun2003 optimization/improvement suggestions

Thomas and I talked at length about axlv2 and came up with several
ideas for optimization and improvement.  The optimations, at least,
should be applied before testing the method at 1-2hr time resolution.

 o After the boxrates calculation, zero all MA elements outside of
   2-3sigma from a center.  Ideally, do not loop over these entries in
   the rest of the code (Spill, Prob, Distribution)

 o Reduce the total number of MA slices iterated over by selecting
   ranges of interesting steps.  Drop uninteresting steps from further
   consideration.

===> 30May2003 pav improvement for FM revisited

I had a chance to revist the improved pav for the FM (see 25Sep2002
entry) and realized that I had used a bad value.  The following are
the correct values:

date range       a3gxpavdpu      optimal for FM
---------------  ----------      --------------
1998037-2000118	 21.3		 22.8
2000138-present	 24.8682	 25.8682  (26.1 is close)

(Days 119-136 were ramp-up days.

Remember, the optimal for FM values were determined empirically using
axlv2.  See 25Sep2002-long-1998/fmcomp.1998001_1998365.ps for
alignment in 1998.  There are lots of examples of alignment after the
pav change (post 2000138) in axfmtweak/runs.

To get the optimal for FM pav from the a3gxpavdpu value, use the
simple formula below:

fm_pav = 0.85(a3gxpavdpu value) + 4.695

===> 22Oct2002 Fitting new FM coeficients

I developed fmfit.pro to be used in conjunction with axfmtweak.cc and
xfmexp.cc.

Procedure for fitting coeficients:
----------------------------------
--later--

--------------------------
-- Work highlights/tips --
--------------------------

22Oct2002:
----------
The hard-coded, fixed z-scaling in fmcomp.pro must be removed to properly
evaluate the FM alignment.  With the current hard-coded z-scaling Fe
ions are so poorly represented in the plots that it is impossible to
judge the goodness of FM fit.

23Oct2002:
----------
I was confused for some time today because the oxygen esd position
worsened after running fmfit for finding C coef.  I realized my
mistake later:  In wanting to output esd_dpu settings for C fitting,
also outputted them for every other ion (incl. O).  This was because I
set 

ESD    = exp(y) * E2C

Clearly, I should have simply set 

H3[12] = 0.0;
H4[12] = 1.0;

But, now there is another problem:  The C esd alignment actually got worse
with the new coeficients!

31Oct2002:
----------

I confirmed that C esd alignment got worse with the new
coef.  See ins_O_coef_again/ versus ins_C_coef_again.

4Nov2002:
---------
I re-fit Si with Si8+, yielding better results -- go figure.  I also
replotted the 1998 data (better zrange) to see how well Si fit in the
original FM.  

I was confused at first because the orginal FM did not
fit well.  I realized that I was not really putting in the right pav
(22.8), rather the read-out pav.  I put the pav-correction function
into axfmtweak -- for xfm only -- and got better results.

5Nov2002:
---------
Nope, I messed that up.  It was using the corrected pav in xfmexp this
way.

8Nov2002:
---------
I found that, for Fe, I needed to use multiple ions simultaneously and wrote
this into fmfit.  This helped considerably, though I still didn't get
a fit quite as good as Hefti's for states which were not included
(14+, 16+).

I also went back to try to fit Si a little better.  After
experimenting with different ions and step ranges, I tried tweaking
the coef. myself.

11Nov2002:
----------
I repeated the same with Si and got some improvement, though I did not
optimize.

13Nov2002:
----------
Working on N.  The fit seems great, but N6+ overlaps heavily, centers within
1.1 sigma.  I'm going to leave it for now.

Ne:  Looks tough.  Let's just see what comes out.


How do I deal with imperfect fits?  Options:

1)  More precisely pick oxidation states and step ranges (possibly for
each oxidation state)
2)  Pick a likely set of states -- the reasonably abundant ones -- and
pick a decent step range -- so there are one set of points with small
enough spread to be represented by a line
3)  Manipulate H3/H4 by hand after (1) or (2).
4)  Plot new line (3) versus data as in fmfit
5)  I think part of the problems might be due to picking the exact tof
center.  Sometimes the esd vector at the center tof is not really the
max.  I should probably average over a few tof channels around the center.

I tried option (5) above.  This averaging in tof worked great in some
ways.  The esd peaks are much better gaussians (see
find_Si_3/fmfit.axfmtweak_find_Si_3.ps).  That said, the final centers
do not seem to be much improved (see ins_Si_3/fmcomp.axfmtweak_ins_Si_3.ps).
This likely due in part because Si7+ and Si8+ do not fit the same
line.  They are actually two quite distinct lines.  I do not know how
to handle this.

===> 14Oct2002 simple pav improvement

[30May2003:  ***NOTE*** I went back and examine the values used to get
this function and found that one, the 20.5 value was wrong.  It should
have been 21.3.  As such, I calculated another linear equation.  See
entry 30May2003.]

I empirically determined that a different pav than returned by
a3gxpavdpu was needed for optimal FM aligment for two separate nominal
pav settings for levels 127 and 171 (entry on 23Sep2002).  These
values could be determined from the output of a3gxpavdpu via a simple
linear function

fm_pav = (0.7056)*pavdpu + 8.3212	[30May2003:  WRONG!]

Replacing pavdpu with fm_pav in xfm (actually in xfmexp right now)
resulted in optimal alignment for the 1998 data (year accumulated in
MA) and very good aligment in tof for 2001 and 2002.  

I did not try to test this on other pav levels.

===> 8Oct2002 inserting an alternate FM

For axfmtweak:  just put it in axfmtweak.c.

For running axlv2:  In AceSwicsData, make getFM call different FM
functions based on PAPS?  No, that is probably not what we want
anyway.  But, the new FM should probably be in this class.


===> 30Sep2002 questions for Rudy von Steiger

 - What is a good tracer of method performance?  We have be using
O7+/O6+, velocities and C/O.

 - What FM do you use and how were the parameters fit?

 - How are species with spill rates < 0 dealt with?

 - Probablistic rates -- which method?

8Oct2002:  See hardcopy notes.

===> 25Sep2002 PAV effect on FM

I finally got enough information to understand the PAV effect on the
FM.  Here are the salient points:

1)  PAVs of interest

date    read            optimal
----    ----            -------
1998	20.5201		22.8
2001	24.8682		25.8682  (26.1 is close)

[30May2003:  I do not know where I got the 20.5 value above.  I looked
into 25Sep2002-long-1998/run-axlv2.out and see the output that the
read value (from a3gxpavdpu) was 21.3 kV.  When I looked back into
~/ace/paps-history/xpapshist2.1999001-2001365.txt I also see the 21.3
value and not the 20.5 value. As such, I've recalculated the linear
eq. for getting optimal FM pav from a3gxpavdpu value.  See entry 30May2003.
]

The 'read' values are from libhef::a3gxpavdpu().

The optimal value was found by trial and error.

Remember: The voltage 22.8 is close to the average monitored voltage
during the period.  The 26.1 voltage is similar.

2)  The FM is much closer in 1998 than it is now, for whatever PAV,
which I believe indicates that the FM either does not perform well
when the PAV is changed (from that for which the parametes were
optimized) **or** must be optimized to handle changing PAV.

3)  The read out values (20.5201 and 24.8682) both result in *VERY
POOR* FM alignment, making results from these calculations essentially
worthless.

===> 23Sep2002 FM improvement plan

 x verify that FM centers are same as sxd (how could they not be?!)
   23Sep:  Wrote dumpfm.pl.  Spot checked edb's 41 and 46 -- all
   centers same.  Done.

 x determine method for better scaling in fmcomp.pro to more easily
   facilitate peak peaking.
   23Sep: Deferred.  A more suitable range for EDB 45 was chosen
   empirically.

 x run 1998 and 2001
   16Sep2002 - 20Sep2002 -- these runs made

 x choose a step on which to focus (O7+ max step, EDB 40 something)
   23Sep:  Chose EDB 42 (plot 52).  This is near the max of O7+ but
   also has decent peaks for C6+ and C5+ (or N6+).  Of course O6+ is
   still huge.
 
 x assign peaks which have significant counts in step to particular
   ions
   22Oct: Doing this on the fly.

 x re-visit old runs with different pav
   Actually re-ran old different pav runs.  See 25Sep2002 entry.
   Found optimal pav for current FM at each pav level.

 x add code to allow using saved MA (save Eqtab, FM -- maybe fromAnalysisData)
   7-11Oct2002 -- 

 x decipher FM formula
   7-11Oct2002 -- 

 o devise Hefti-like goodness of fit tests
   18-22Oct:  Wrote fmfit.pro which finds experimental esd centers based
   on xfm tof center then fits esd_dpu to these and spits out
   constants, H3 and H4.  See ~/ace/axfmtweak/runs/18Oct2002-writing-fmfit

 o tweak FM parms for step to best fit
   22Oct:  Improved oxygen FM alignment using this procedure.  See
   ~/ace/axfmtweak/runs/18Oct2002-writing-fmfit/inserted_coef

 - repeat (hopefully automated) for remaining steps

 - put new constants into libhef, possibly as different function or option

===> 5Sep2002 re-work of libhef containing code

I found that my recent modifications to AnalysisData, so that it loads
the FM for each day, caused runaway expansion of the ion lists, since
they were initialized along with the FM.  To fix this, I split the eqtab/FM
init from the ion tables and only ran the eqtab/FM for each day.
Unfortunately, the new configuration caused core dumps.  

Rather than fix that situation, I decided to fix the whole problem of
reading from nc files in more than one class.  So on 1Sep, I removed all
libhef calls from AnalysisData and made a new object, AceSwicsData to
collect the Pha vector (formerly in axlv2), Eqtab, pav, and forward
model.  These values are stored into class variables as they are being
read from the nc file (except fm) and accessed via get functions by
AnalysisData.

At first, this seemed to result in the same sort of core dump (which
gdb put at MeasurementArray.cc:70, the beginning of a loop which seems
odd).  Then, after changing what looked to be un-related code, the
core dumps disappeared.  But, the results are now not the same as they
were, e.g., O7+/O6+ = 2.9 )(box for 2002100) and the prob rates are
'nan'.

Summary of info so far:
(Compared 08Aug2002-nobrw with 03Sep2002[...]/box-nobrw.)

- total number of pha is the same, 828136 with 0 thrown out
- number of pha after each read is the same (I diff'd greps of the out
files)
- the same is true with the accumulation time: same at each stage and
total (86020)
- npha used to fill the array in each MeasurementArray::load is the
same, i.e. grep'ing for 'fill' on the out files, which gives nph used
to fill array, ma_sum and time interval info, yields no differences
(I had thought the ma_sums were different.
- a3readcycle returns 134553853 periodically now (from load) whereas
it was 60 in the same instances before
- SW speed is different.  I worked inconsistently before (some times
non-physical, in the 1000's) but now it gives 'nan' after the first one

Later:
------

Fixed.  Numbers of PHA loaded at each step are the same as they were
now.  Everything else is back to normal.  The problem was that the
Eqtab[] was indexed wrong in AceSwicsData::load which caused the FM to
fail and a bunch of other problems.

===> 28Aug2002 long runs for FM centers

There seems to be a new bug which only shows up when multiple days are
run.  In both a 30 day and 100 day run, the boxrates print out
repeatedly then the program crashes or hangs.

Later:

Fixed.  It was because AnalysisData::init() was being run for each day
(to get day's FM values).  This prompted a big switch around where all
instrument-specific code was moved to AceSwicsData in addition to the
existing stuff in Pha.  The function loadPha was moved into the new
class and AnalysisData was given several init/load functions for it's
own ion table, FM and Eqtab.  These call AceSwicsData functions which
either read out of tables stored during AceSwicsData::load() or wrap
libhef functions.

===> 19Aug2002 found BRW bug and maybe another

The basic rate weighting (brw) averages were way to high, on the order
of 10^2.  There turned out to be a couple of problems/issues:

1)  Libhef returns br as a long; Pha and loadPha were treating it as
an int.
2)  loadPha was not init. br[nedb][ibr] *and* was storing it on a per
PHA word basis, instead of once per edb.

I corrected these and the averages dropped down to a much more
reasonable level, around 2.87.  (I wrote
axlv2/devel/tools/sum_avewgt.pl) to calculate this.

21Aug2002:
----------
Now, I'd like to verify these rates against sxd.  

25Aug2002:
----------
Done, though I actually verified them against brwcomp.pl.  Both give the
following

2000158		1.2
2002100		1.3

The two numbers actually differed slightly in the 3rd sig. fig., but I
attribute that to C++/perl differences and am not concerned.

===> 8Aug2002 adding basic rate weighting (BRW)

This changed my state variables slightly, e.g. vO6+ was 301.3 and went
to 298.5.  It changed some ratios a lot, esp. O7+.  Others changed
very little, i.e, (run for 2002100)

NO BRW:

      	     Box	   Spill	    Prob	 Density
----------------------------------------------------------------
   C5+	7.17e-02	1.35e-01	4.70e-01	1.07e-02
   N5+	3.17e-02	3.73e-02	1.47e-01	3.02e-03
   N6+	6.63e-02	9.41e-03	2.03e-02	4.32e-04
   O6+	1.28e-01	1.60e-02	1.49e-01	2.68e-03
   O7+	8.21e-02	2.45e-01	3.50e-01	6.12e-03
20Ne8+	5.34e-02	1.29e-03	2.26e-02	3.02e-04
----------------------------------------------------------------
 ^sum^	4.34e-01	4.44e-01	1.16e+00	2.32e-02
----------------------------------------------------------------
 O7/O6	6.39e-01	1.53e+01	2.35e+00	2.28e+00
----------------------------------------------------------------
   C/O	4.35e-01	6.57e-01	1.21e+00	1.57e+00
----------------------------------------------------------------

BRW:


      	     Box	   Spill	    Prob	 Density
----------------------------------------------------------------
   C5+	1.15e+01	2.03e+01	4.54e+01	1.07e+00
   N5+	2.80e+00	3.37e+00	7.58e+00	1.57e-01
   N6+	1.63e+01	4.06e+00	6.42e+00	1.32e-01
   O6+	1.15e+01	1.55e+01	2.91e+01	5.30e-01
   O7+	2.36e+01	3.02e+01	1.03e+02	1.83e+00
20Ne8+	7.46e+00	8.76e-01	3.63e+00	4.95e-02
----------------------------------------------------------------
 ^sum^	7.32e+01	7.43e+01	1.95e+02	3.76e+00
----------------------------------------------------------------
 O7/O6	2.05e+00	1.95e+00	3.54e+00	3.45e+00
----------------------------------------------------------------
   C/O	4.41e-01	6.22e-01	5.82e-01	7.67e-01
----------------------------------------------------------------


===> 8Aug2002 importing into CVS

-----
Aside: Some useful commands:

cvs tag swindal-0-9-93 (after commit or with -c option for safety)
cvs release dir
cvs diff acetools/a3dif.c
-----

I separated swindal and axlv2, made new make files and got it to
compile.  Then, for each dir., ~/ace/swindal and ~/ace/axlv2, I moved
the misc. stuff into devel/, moved that out, and did

$ cvs -d $ACECVS import -m "Imported sources" swindal shrg start

I also added a few idl save files later, with the '-kb' option to
protect the binary files.

The revisions were automatically tagged as start, but I intend to tag
the set every time I get a good working version.

===> 6Aug2002 status report to kick off working with Thomas

Comparison with sxd: (See 4Jan2002 entry for detailed notes)
--------------------
[Comparisons are O7+/O6+ unless otherwise stated.]

1.  At the level of box rates, *daily-averaged* sxd and axlv2 compare
very well, .60 verus .599.  The difference could easily be
sig. figs.  The term 'daily-averaged sxd' means that the O7+ and O6+
counts are each summed over a day then the ratio is taken. 

2.  With native sxd averaging, the box rates of axlv2 do not compare
well, .72 versus .599.  This makes me wonder a lot about which
averaging is proper.

3.  The densities to not compare well, .84 versus .42.  But axlv2
densities are certainly off, apparently by several orders of
magnitude.

Performance
-----------

1.  FM alignment seems bad; In many cases the FM box is more than 50%
misaligned relative to the peak.

2.  Densities are orders of magnitude off

3.  Velocity comparision, day 2000158

For both sxd and xl2, I read the data into IDL and used 'moment' to
come up with the following averages.  I took the axlv2 values out of
the end of run dump.

2000158:
	   	vO6	vthO6	
sxd		377.9	8.0
xl2		449.3	17.0
axlv2		449.0	24.9	(PAPS 24.87kV)
axlv2		443.6	19.8	(PAPS 25.65kV)
axlv2		443.4	22.0	(PAPS 26.1 kV)

2002100:
sxd		322.0	16.2
xl2		304.4	7.67
axlv2		301.2	15.8	(PAPS 25.65)


===> 5Aug2002 back to checking forward model alignment

In the intervening period, I learned a lot about the different methods
for getting PAPS.  This is explained in detail in
ace-s3-analysis-notes.txt, but here is a summary:  

1) There is a commanded level (returned by a3gxpavlev) which is
converted to voltage using

pav = pavlev*.0811 + 11.0		*COMMANDED*

This is confirmed by the ACE S3DPU Command and Data Document.

2) There is a voltage sensor which measures the actual voltage and
returns a digital value into an HK item, SSCGRPAPS.  This is converted
using

pav = SSCGRPAPS * .15kV			*MEASURED*

Of note is that the measured value varys a fair amount and is not
always all that close to the commanded value.  The commanded value
usually reads about 1.5 kV below the measured one.  The measured value
typically hovers pretty close to the supposed set point of 22.8kV or
26.1kv (depending on which time period).

Now, the only confusion that remains is which one to use.  Simon used
a hard coded value of 22.8 in sxd (before the ramp up) which is
closest to using the *measured* value.  Now, sxd also grabs fm centers
every cycle, but this shouldn't matter since it only takes E/q, pav,
mass and charge as input.  As long as the E/q and pav don't change, I
cannot see how the fm center will change for a particular mass and charge.

===> 5Aug2002 back in the saddle

Ok, I've gotten the computer hacking and operations problems under
control and I'm ready to start working on this in earnest again.

My first work will be to change AnalysisData so that it does not load
the FM from one particular day.  I identified this as one of FM
problems back in May and I vaguely remember this being a problem
earlier this year.

I did this by taking all the code out of the constructor and putting
it in init().  This way, the init() can be called from within the loop
over days.  This allows it to use gCurLv1File for reading FM, pav and
eqtab.  And, since the instance of AnalysisData (ad) is declared
outside of the block in axlv2, it persists after the read-in phase.

Back to checking the forward model alignment.

===> 21May2002 checking forward model alignment

There is no question that the forward model (FM) is not aligned
perfectly with peaks -- even big ones like O7+ in step 41 -- much of
the time.  I would say it is aligned well 30% of the time.

The questions are, then, is this correct?  Is good enough?  Is this as
good as practical?  I aim to find that out this week.

Using libhef::a3gxpavdpu():
---------------------------

This definitely moves the centers:

PAV  Source	      E/q	  Ion		tof,esd
26.1 PDB conv	      2.39	  O7+		344.22, 51.55
24.9 a3gxpavdpu	      2.39	  O7+		352.48, 48.98

though the peak is at about (347, 49) so neither hits it really well,
and the shift shifts right over it.  Interestingly, though, the the
24.9 case, the method shifted the center closer to the peak center.
In fact, in the 26.1 case, the center actually shifts away from the
(visually estimated) peak center.

===> 17May2002 finding good days to run

I loaded all of /home/acedata/xmqh/gif/2002 into xv and just paged
through the year looking for good days to run.  Here are some days of note:

2002019	    heavy and noisy
2002027	    moderate, big non-iron overlap
2002063	    heavy (smear), less noise, big H/He cross talk
2002107     moderate but noisy, esp. big He
2002108	    light


===> 14May2002 Understanding method and how to tweek it

I modified axlv2.cc to produce a axlv2_ions_xx.dat file for each step
so that I could compare box, spill and prob rates to see if they
appear to be doing the right thing.

**On paper**, I did some conceptual calculations to see what the
method does under conditions of overlap.  See the 'Overall Method and
General' section in the binder.  This idea may need to be extended to
actual IDL code that shows the prob. rates calc. effect on gaussians
with various degrees of overlap.

I found that small overlaps (< ~1.5 sigma) tend to make the peak
narrower but not move the center.  Overlaps that cross the FWHH
(full-width half height) line tend to decrease the intensity of the
peak significantly and shift it away from the overlap.  This effect is
weighted by spillover-corrected rates such that gaussians with larger
rates effect a particular peak more.

Analysis of method performance:
-------------------------------

Day 2000158
-----------
[Statements of shifts like (-.5sigma, .7sigma) mean that center is
shifted left by about .5 sigma in tof and right by about .7 sigma in
esd.  Sigma refer to the half width of the original boxes in their
respective dimensions.]

step 31 --
---------- 
 Species   BoxRate SpillRate  ProbRate
   Fe11+  7.72e-04  4.58e-05  2.92e-03
   Fe12+  1.12e-03 -2.08e-08  2.41e-03
   Fe13+  1.57e-04  4.67e-09  8.47e-04

In the plot, all of these centers move substantially.  Fe11+ seems to
move to a slightly higher rate region, as shown in the prob. rate.
Fe12+ behaves strangely, and, I think, wrongly:  It shifts about
(-.5sigma,-1.2sigma) but *INTO A LOWER COUNT REGION*.  This last point
is what makes me think this is the wrong behavior.  And, the boxrate,
1.12e-03, is not particularly low.  But, the spillrate comes out
negative, which is why, I think, this large shift takes place.  The
questions is, why does the spillrate come out negative.

step 39 --
----------
      Species   BoxRate SpillRate  ProbRate
          N5+  1.33e-02  2.12e-02  1.87e-02
          O6+  3.26e-02  6.70e-02  6.31e-02
          O7+  1.57e-03  2.68e-03  2.15e-03
       20Ne7+  7.96e-03  1.49e-05  2.45e-05

O6+ seems to behave exactly as I expect:  The center moves from the
edge of the peak diagonally toward the middle (although it lands only
about halfway to the middle).  The counts double.

N5+ moves diagonally toward what I believe is teh O6+ peak as well,
though only slightly.  I don't know if this is the correct behavior.
There doesn't seem to be a peak at N5+ in this step, so this might be
what is expected, though I'd guess it is not ideal.  I probably should
find the step with the N5+ peak to see how it behaves there.

20Ne7+ moves away from the believed O6+ peak.  That seems good.

step 40 --
----------

      Species   BoxRate SpillRate  ProbRate
          C5+  7.61e-03  1.27e-02  1.35e-02
          N5+  3.66e-03  5.48e-03  4.90e-03
          O6+  9.56e-03  1.95e-02  2.13e-02
          O7+  6.78e-03  9.47e-03  9.43e-03
       20Ne8+  1.25e-02  2.22e-02  3.11e-02
        Si11+  8.82e-03  1.24e-03  5.48e-03

C5+ does seem to move closer to a peak -- I don't know if it is the
C5+ peak.  

O6+ moves closer to it's presumed peak.

N5+ moves laterally in this step, not diagonally toward the O6+ peak
as in step 39.  This puts it in a slightly higher rate region, which,
I guess, is expected.

20Ne8+ starts centered on a low rate box and moves up to a much higher
rate one, by about .4sigma in esd.  This seems to make sense.  *But*,
in doing so it moves further away from another apparent peak at
(360,52).  There is no center on this peak, which makes me suspicious
of the forward model in this case.

Si11+ also moved up substantially, 1sig esd, and seemingly away from
the higher count regions.

step 41 --
----------

      Species   BoxRate SpillRate  ProbRate
          C5+  9.40e-03  1.02e-02  1.20e-02
          N6+  1.99e-02  2.78e-02  2.68e-02
          O7+  1.74e-02  2.11e-02  2.25e-02
       20Ne8+  7.12e-03  2.90e-10  4.93e-09
        Si11+  4.51e-03  2.54e-04  7.01e-03

C5+ is exhibiting correct behavior by moving away from overlap and
toward a small peak.

N6+ and O7+ are tough to analyze.  They start with big overlap, right
at 1sigma in both dimensions.  The data shows a similar overlap,
**though the fm centers for both is shifted by about
(-.5sigma,.5sigma)**, which makes me think something is wrong with the
forward model.  The counts go up for both ions, which is in accordance
with the plot.

--------------------

Big Question:  The way I'm plotting this, the sigmas do not change
through the calculation.  I don't think this is true in the actual
calculation.  Even though the prob. rates calc. uses the original
gaussians with the fm sigmas, the formula behaves to change the peak
widths in the prob. rates without direct dependence on sigmas.  (This
seems like a good think to me.  The peak widths have to be flexible
for minimizing overlap while maximizing counts.)  But, my plot is
certainly deceptive in this way.

At this point, I think the procedure is basically working.  But, I'm
increasingly wondering about the line up of the forward model.  I will
now look for other days which may line up better.

16May2002:
----------
[I spent the AM reading Menke, _Geophysical Data Analysis:  Inversion
Methods_.]

Another Big Question: Does MA enter P(tof,esd) calc. anywhere except
boxcounts?  If not, it must be really sensitive to FM *and* centers
will only tend to move away from overlap of peaks that had big
boxcounts.  If the FM missed the peak, I don't think there is anything
that will cause the center to move back onto it, or at least not strongly.

===> 23Apr2002 REALLY CATCHING UP again: building fm comparision plots

[I cannot believe that I have worked on this only a few days in the
last three months.  No wonder it is not done.]

Problems:
1) The forward model centers are hard to read.

2) The intensities are too low in the tof/esd plots

Working on (1) first.

2May2002:
---------

I fixed these by zooming the plots in two separate regions.  The first
covers on the non-iron fm centers and the second covers the irons.  I
also messed around with color plot a lot to get things looking good,
especially black lettering but with a white background.  For this I
had to learn how to set specific values in a color table (see
~/notes/idl_usage_notes.txt) and added an option, zmin_color, to color
plot so I could set the color of the minimum z manually at run time.

I hacked a fix to the intensities by simply specifying a zrange, from
a by-eye estimate.  This may need to be fixed more robustly.

Now the big problem is that the fm centers do not match the
axlv2_fmcomp.dat file in steps.  For example, O6+ has the following
data in axlv2_fmcomp.dat:

eoq  ion tof    tofsig
3.19 O6+ 366.37 4.54
2.97 O6+ 367.85 4.56
2.76 O6+ 369.25 4.58

*but*, fmcomp reports 369.25 for eoq 3.19 (step 39).  I fixed this
partly by getting the eoq in the stdout report from the fmcstep
structure.  This makes fmcomp stdout report the same as
axlv2_fmcomp.dat.  But, I still think the tof/esd data is not matched
up properly, step-wise, with the fmcomp data.

14May2002:
----------
This has been fixed.  The plotting is now usable, though not perfect.
I plot three plots for each step:  full tof/esd, a region covering the
non-iron ions and a region covering the irons.  These regions adjust
themselves automatically.  Most of the counts are in EDBs 28 - 49 so I
usually just plot that range.  The z-scaling is fixed at an eye-balled
value.  That will need to be changed later.

===> 19Feb2002 catching up again and more careful analysis of Ulysses overlap

For comparison, I ran this over a period that overlaps with analysis
of Ulysses data in von Steiger et. al. 1999.  this was
1998037-1998120.  Ulysses was finishing it's first/starting it's second
solar orbit during that period and was far away from the Sun, in the
vicinity of Jupiter's orbit (though I believe Jupiter was not there).

I looked particularly at the C/O ratio von Steiger 1999, the C/O ratio
should not change much from his reported value of 0.670 =/- 0.086.  In
von Steiger 1997 the authors assert that this value should not change
much in fast versus slow streams.

Nathan's suggestions:

1) Run a similar timeseries for sxd (ellipses, ell+br, full) and plot

2) Put in BR correction

3) Build graphical tools to examine E-T matrix slices to find location
of fwm and spill correction.  Run for slices (charge steps) where a
lot of ions are coming in.

- wondering: is spillover correction too aggressive?  

===> 18Jan2002 100 day time series

28Jan2002:
----------

I wrote run-axlv2.pl to run axlv2 over a series of days then used this
to runn from day 2000102 to 2000200.  (Day 2000101 had problems during
the 100 day average run (14/15Jan2002).)

The run seemed successful, in that there are a whole bunch of files in
the 18Jan2002.  I need to come up with a meaningful way to plot the
results.
 

===> 14Jan2002 100 day (average) run

28Jan2002:
----------

The run was successful.  Surprisingly, the o7+/O6+ ratio actually went
down to 0.18.


===> 4Jan2002 comparing with modified sxd results
[note: this entry was originally in sxdcomp2/notes.txt]

Filename:		   mean,var	Description
x2000158_flag0.sxd	   .84,.20	sxd with xelletm flag set to 0
x2000158_f0v0.sxd	   -NaN,-NaN	_flag0 with vH set to 0.0
x2000158_try3.sxd	   .7232,.1612	flag 0,calc vH, no brw, no dj, no eff,
x2000158_try4.sxd	   .7219,.1579	try3 plus $v_i = 1.0, $den=$nmom
x2000158_try5.sxd	   same		try4 but zero vH, call
					xelletm, restore vH
x2000158_try6.sxd	   .7218,.1579	try5 plus ?? (crap, I got
					interrupted and didn't write
					it down!)
*** switched to svsbare2.pl to better control and use native daily averages ***

x2000158_hefell.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
359.58  214.92  27.83   17.67   24.83   
O7+/O6+ = 0.60

x2000158_ell.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
359.58  214.92  27.83   17.67   24.83   
O7+/O6+ = 0.60

x2000158_box1.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
155.08  89.08   13.17   6.75    9.33    
O7+/O6+ = 0.57

x2000158_box2.svsbare2:  changed boxes to be center +/- sigma like swindal
-------------------------------------------------------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
494.33  294.42  42.33   23.00   34.75   
O7+/O6+ = 0.60

Wow, that really lines up with the others.  So, to recap., with
stripped sxd (sxdcomp) I get O7+/O6+ = .72.  With the same function in
svsbare2 I get .60.  Now, the only difference should be the cycles
averaged at the end (sxdcomp) versus native daily averages in
svsbare.  This needs to be double checked.  Wait, there is one more
difference:  sxdcomp uses counts while svsbare uses rates (divided by
12s).  This should be equivalent in ratios.

To test this, I put 'elipsecount', from svsbare2, into sxdcomp and
used that for $tcnt.  Result:  O7+/O6+ = .7218 with var .1579 -- this
is IDENTICAL to the last sxdcomp run with Simon's a3xelletm.  All
right!

Let's try some more variations of sxd to nail down what each of those
steps does to the ratio:

elipses (below) are from a3xelletm() unless otherwise noted.  Also,
O8+ correction for half elipse is left in place unless otherwise noted.

Filename:	   mean,var	Description
x2000158_try7.sxd  .7218,.1579	elipses only
x2000158_try8.sxd  .7218,.1579	elipses plus vel. filter
-----
This vel. filter did not change O7+ and O6+ at all but did get rid of
one count of Fe5+ in a number of cycles.  There could have been other
small changes as well, but my cursory glance only found Fe5+.
-----
x2000158_try9.sxd  .7218,.1579	elipses plus br weighting
x2000158_try10.sxd .7218,.1579	elipses plus dj
x2000158_try11.sxd .6852,.1422	elipses plus O and Fe eff
x2000158_try12.sxd .7150,.1508	elipses plus phase-space density f
x2000158_try13.sxd .7232,.1612	elipses plus velocity weighting in sum (v_i)
x2000158_try14.sxd .7208,.1564	elipses plus density
x2000158_try15.sxd .8365,.1989	all on (like sxd)
-----
This last result, with everything on like in sxd, exactly matched the
ratio calculated from the archived sxd file.  Good!
-----
x2000158_try16.sxd .8365,.1989	sxd minus vel. filter
x2000158_try17.sxd .6798,.1361	previous - brw
x2000158_try18.sxd .6798,.1361	previous - dj
x2000158_try19.sxd .7161,.1503	previous - eff for Fe and O
x2000158_try20.sxd .7232,.1612	previous - phase space density conv
x2000158_try21.sxd .7208,.1534	previous - vel. weighting (v_i)
x2000158_try22.sxd .7218,.1579	previous - density conv

Ok.  Let's take a different tack.  I'm going to switch to native daily
averages and do this again.  The program is
sxdcompda.pl.  All I did is accumulate (by ion) the $den which sxd
normally prints out ion every in every cycle.

Output files are names x2000158_tryDD.sxdcompda, where DD is the try
number below.

Other columns:
IDL-ave		IDL-averaged O7+/O6+ ratio; per cycle values read from
		file then summed and divided
natda		native daily average; dividing accumulated rates
		(summed from sxd $den variable)

Try #  IDL-ave	    natda explanation
-----  -------      ----- -----------
1      .7218,.1579  .60	  try22 with new program (native daily ave)
2      .8365,.1989  .67	  all of sxd
-----
Note: the IDL-ave results are identical to sxd.
-----
3	.8365,.1989  .67  sxd - vel filter
4	.6798,.1361  .56  prev - brw
5	.6798,.1361  .56  prev - dj
6	.7161,.1503  .59  prev - eff for Fe and O
7	.7232,.1612  .59  prev - phase-space density conv
8	.7208,.1564  .59  prev - vel weighting
9	.7218,.1579  .60  prev - density conv
10	.8923,.2307  .71  prev + brw
11	.7218,.1579  .60  try9 + dens conv
12	.7150,.1508  .60  prev + phase-space dens conv
-----
Now we are right back where we started, with sxdcompda set up like
swindal.

I finally figured out why the native and IDL averaging are different.
In IDL, I took the ratio of each pair, O7+ and O6+, for each cycle,
then averaged the ratios.  I swindal, and now in sxdcompda, I sum up
the values of each for all the cycles, then take the ratio.  Clearly,
these will be different numbers, as one is the sum of the ratios and
the other is the ratio of the sums.

So here is a summary of the comparison of swindal derived results to
sxd (or sxd like) derived results:

Two types of averaging:
ave1)  calc O7+/O6+ ratio for each cycle --> ave. ratios
ave2)  sum 07+ and O6+ counts (separately) over cycles --> take ratio
       of sums

sxd varient		ave1	ave2
-----------		----	----
straight sxd		.84	.67
sxd - brw		.68	.56
sxd - vel filter	.84	.67	affects some iron slightly
swindal-like		.72	.60
swindal-like + brw	.89	.71
swindal-like + dens	.72	.60

Swindal results from runs/18Dec2001/run2, ***all ave2 style***:

             Box           Spill            Prob         Density
----------------------------------------------------------------
   C5+  2.51e-02        3.19e-02        3.55e-02        5.74e-04
   N5+  3.01e-02        4.75e-02        4.40e-02        6.24e-04
   N6+  4.54e-02        5.49e-02        5.35e-02        7.54e-04
   O6+  7.04e-02        1.38e-01        1.36e-01        1.67e-03
   O7+  4.22e-02        5.83e-02        5.82e-02        7.11e-04
20Ne8+  3.10e-02        2.74e-02        4.01e-02        3.91e-04
----------------------------------------------------------------
 ^sum^  2.44e-01        3.58e-01        3.67e-01        4.72e-03
----------------------------------------------------------------
 O7/O6  5.99e-01        4.23e-01        4.28e-01        4.27e-01
----------------------------------------------------------------
   C/O  3.26e-01        2.65e-01        3.32e-01        4.43e-01
----------------------------------------------------------------

Nathan's opinion of this is that I should run a longer time series,
say 100 days.

I'll set to doing that.

===> 17Dec2001 Catching up with Nathan

Nathan feels we need to 

a) have data in a good presentation format (ions table)

b) know exactly what is going on with sxd

I'm working on the former right now.

===> 14Dec2001 Brainstorming the data simulator

 - input to axlv2 via ma.dat file
 - 

===> 5Dec2001 Adding probabilistic centers

I tried several methods from several sets of equations I had written
along the way.  I finally settled on calculating them hand in hand
with Nprob, since all the same elements were needed.

Right now, they are all identically zero so I'm debugging around the
oxygen box in edb 41:  340 <= tofch <= 348 , 46 <= esdch <= 57 .

10Dec2001:
----------

Ok, now they seem to work and I have the report table looking like I
want.  Unfortunately, there are a lot of these that are 'nan' from 0.0
prob. rates.  And, there are a lot that are > 1 sigma different from
the forward model.  I think the only way to understand this now is to
write the data simulator.

===> 3Dec2001 adding banner and version
[I haven't been able to work on this in 2.5 weeks.  Ugh!]

I did this just like libsms -- use the compiler strings __DATE__ and
__TIME__ and pass in the version with -DSWINDAL_VERSION= .  This stuff
is set up in initSwindal.cc.  I wanted to get rid of that routine, but
I'm beginning to think I'll just have to make it called
automatically.  That would be close enough.

===> 15Nov2001 cleaning it up

I added a number of flags to turn on the various rates calculations
and such. I also wrote MeasurementArray filter stuff and put that in
place.  (I tested the passRange function first of course.)

I made an input section which reads all the flags and settings from
the terminal so I could pass these things in via an input file.

It seemed to be working on all but the directory name, but now it is
not. I figured out it does not like the comment strings I left in the
file.  

Ok, now it works.  I added 'getline' commands after each input to read
through the comment.

===> 5Nov2001 catching up with Nathan.

I showed Nathan the following output:

axlv2 -I- writing comparison tables:

             Box           Spill            Prob         Density
----------------------------------------------------------------
   C5+  2.51e-02        3.19e-02        3.55e-02        5.74e-04
   N6+  4.54e-02        5.49e-02        5.35e-02        7.54e-04
   O6+  7.04e-02        1.38e-01        1.36e-01        1.67e-03
   O7+  4.22e-02        5.83e-02        5.82e-02        7.11e-04
----------------------------------------------------------------
  sum:  1.83e-01        2.83e-01        2.83e-01        3.71e-03
----------------------------------------------------------------
 O7/O6  5.99e-01        4.23e-01        4.28e-01        4.27e-01
----------------------------------------------------------------
   C/O  3.26e-01        2.65e-01        3.32e-01        4.43e-01
----------------------------------------------------------------

He had a few comments:

1.  He realized the the Spillover correction is not normalized, so all
the numbers will increase.

2. The fact that Spill and Prob are so close *is an excellent*
indication that things are working reasonably, since they are
completely different methods.

3. 

Nathan's suggestions for further work were:

1.  Clean up the package so it is ready to be run a lot.  That means
making neat output files, including the probabilistic centers.

2.  Make a data simulator using the prob. function and a few ions (one
charge step).  Include a calculation of relevant ratios and such. 

3.  Pump the simulated data through the method via a loaded MA and
compare.

4.  See if I can output that simulated data in a form that can be
pumped through sxd for comparison.

===> 24Oct2001 deleting species from S

After noticing that all the counts seemed to increase from box to
spill, Nathan and I came to the conclusion that species with a
negative Nspill (at a particular edb) should be removed from S.  The
new, smaller S should be re-inverted and the rest of the
calc. finished.  Note:  Nathan recommended *against* taking out
species in a second round thinking that the matrix would become
unstable.

I misunderstood and removed the species from all S (at all edbs) which
resulted in *all* species being removed from the analysis.  (Oops!)
This work is saved in

save -I- saving...SpillRates.cc as save.d/SpillRates.24Oct2001-12:45:26-1.cc 
save -I- saving...SpillRates.h as save.d/SpillRates.24Oct2001-08:41:44-1.h 

Now, I will try to re-write this again, this time writing a routine to
do all calc for a particular EDB with a wrapper routine that loops over
the edbs, takes out species and re-runs it.

5Nov2001:
---------
Done (between many other intervening tasks).  It turns out I needed to
write an assignment operator to be able to copy ions to a new group.

And, I found out that for vectors

vector<float> v;
v.reserve(MAXEDB);
size =  v.size();
cap  =  v.capacity();

In the preceeding code, size  = 0 and cap = MAXEDB.  In order for size
= MAXEDB, I needed to either load elements via v.push_back() (and omit
the v.reserve()) or v.assign(MAXEDB, 0.0) then assign them
individually with v[s] = value.

This was a problem because I used size() in the Ion::operator=
overloading but it wasn't set since I had used reserve() followed by
individual assignment to fill the forward model vectors.  I added
reserve() and assign(MAXEDB,0.0) to Ion::Ion so this wouldn't be a problem.

===> 22Oct2001 catching up with Nathan

 - looks like axlv2 differs from sxd by about 200,000 K, 1.7 vs. 1.9
 MK.  This is only about 20% and not all that bad

 -  He suggests finding the t,e centers of the prob. rates, like we
 talked about 17Aug2001.  I wrote this out in hardcopy notes (same
date and title as this entry).

 - The v problem (from DistFunc results) is likely just unit
conversion.  I checked and I am not doing any.  Nathan's ballpark
figure is 437 ;m/s * sqrt(E/q).  I should double check that.

===> 22Oct2001 Thomas' thoughts about comparison with SXD

 - .82 is likely a CME ratio, 0.3 - 0.5 is more 'normal'
   - CMEs mean more energetic particles which means more background
   - Simon's MMQ (emailed) does have some filters so these energetic
   particles may not show up there.
   - Since my MMQ (no filters) don't seem to have lots of spurious
   counts, background is likely not a problem

 - sxd's phase space correction corrects for the fact that more counts
 are received as the velocity goes up, even with the same populations
 of particles.  (If you literally sped up a particular time period,
 the instrument would count more.)  This is for primarily two reasons:

 1.  delta E/q (acceptance width) increases with E/q.  Faster
 particles would enter at higher E/q levels.

 2.  since the collection (integration) time at any E/q is fixed, more
 counts are received when particles are moving faster.  (More
 particles enter per second since they are moving faster.)


===> 16Oct2001 new MeasurementArray::fill method - no MAcounts

I changed fill so that it would do the following, thus avoiding having
a 70Mb MAcounts lying around just for accumulating counts:

1. change MA from rates to counts
2. loop over PHAs, accumulating with + 1.0 into corresponding MA
   location
3. get time for list of PHAs (analysis interval worth)
4. add that time to global accumulation time (gAccumTime)
5. change MA back from counts to rates by dividing all by gAccumTime

Now, compared to a save MA before this transformation, the new one is
4 bytes longer.  (See runs/17Oct2001 for both.)  When the sums over
slices were loaded into IDL, I found that the zeros are all in
*exactly* the same places.  They appear the same when plotted except
that the new one is a little lower intensity.  I took the difference
between the two and found that a) it was in the .0001 - .001 range and
b) not exactly uniform, i.e. the whole matrices are not related by a
constant factor.

I compared running fill as described above (MA as rates) to running it
without steps 1 and 5 (MA as counts).

Everything seems to be consistent in the new version.  When I run fill
with MA as counts and divide the ma sum by gAccumTime by hand
after execution, I get the same sum as when done with rates.  The same
holds true for the box rates.  If I take a box rate and multiply by
the last reported gAccumTime, I get the same box count as when I run
fill with MA as counts.  I got the same behavior when run over an
entire day.

I think this is behaving correctly.  I never compared MA as rates with
svsbare.pl so I'm not sure it was exactly right before anyway.  I'm
going to move on and consider this working fine for now.

===> 15Oct2001 new ProbRates methods

Ok, now that I have gotten rid of the MQ and MM arrays for conversion,
there should be plenty of memory to run ProbRates with MA still in
memory.

First, I'll get rid of the block in axlv2 that destroys MA after
accumulation.  Done, works fine.

Note: I stored two copies of MA (all ranges and range 1 only) in
runs/15Oct2001/{all_ranges, range1_only}.  I should be able to use
these without accumulating again..  I think I will do the same with a
15 day run.

[Recap.]  Here's the plan.  The new ProbRates scheme will have upper
and lower bounds, based on two different methods (but both
incorporating MA back into it):

1)  upper bound -- For each location in MA, give counts to the species
with the highest prob. at that location.

2)  lower bound -- For each location in MA, the each species a fraction
of the counts equal to it's probability at that location.

(2) is easy, but (1) requires a search.  I could just do this simply
mindedly and loop.  I'll decide after I count the number of possible
searches.

16Oct2001:
==========
According to Numerical Recipes, p. 344, the fastest way to do the search
involved with (1) is to just loop over all elements.  

RAM Problems:
-------------

In writing code for (2), I notice that axlv2 now uses 312Mb RAM when it
hits ProbRates.  Geez!  It makes sense, because though I'm saving
about 140Mb from MM/MQ, I'm now keeping MA even after ProbRates is
declared.  I think I'll have to get rid of MAcounts and go to the
method which Nathan suggested of multiplying by accum. time, adding
counts, then dividing.

ProbRates seems to use a fair amount of memory.  P and G themselves
are 30Mb each.

It turns out I had taken out all references to MM and MQ -- EXCEPT
in the header and constructor!  Geez.  That's why it was using so much
memory.  Now, is is using 189Mb when it hits ProbRates, which is much
more manageable.  I still might get rid of MAcounts, but at least it
isn't an emergency now.
-----

Prob. rates are too small, the O7+/O6+ ratio is 0.07.  Also, when I
run for just one edb (39 or 41) they are all 'nan'.  Since the O6+
boxrates peak in edb 39 and those of O7+ in edb 41, this cannot be
right.  In all of the ProbRates stuff, there is no crosstalk between
EDBs so the ones that held the max boxrates should also hold the
highest probability.

I'm wondering if the nan results are genuine numerical precision
problems.  Looking into

/usr/lib/gcc-lib/i386-redhat-linux/2.96/include/float.h

I can see that floats can be btween 1.17549435e-38 and 3.40282347e+38
. Since some of the spill rates are 10^-14, I'm wondering if this
might really be the problem.

I think if you add a nan to a number you get a nan.  That would
explain these problems.  When I put a threshold (1.0e-5) on P to
qualify for adding to Nprob, the prob rates came back.

18Oct2001:
==========



===> 9Oct2001 looking into MMQ conversion by funtions again.

This does not currently work.  It seems that if I could get it to work
then I could reduce the memory load significantly before sending MA
into ProbRates for the changes to that method.

1) I took out the range 1 filter so all PHAs would be included and the
MMQ plot would look normal.

2) I split up outputMats into outputMA and outputMMQ to save
processing time.  I also made it output the summed mmq using all three
conversion methods, the matrices, eqte2mmq and mmqbyhand.

The mmq plot from the matrices (MM and MQ) looks fine but the others
are zero.  I'm going to try putting the edb 41 O7+ box through the
converters to see if they are even close.

EDB 41 O7+ box:  340 <= tofch <= 348 and 46 <= esdch <= 57

These are completely off.  All masses are in the 10^30 range and all
m/q in the 10^13 range. *OBVIOUSLY* this is wrong since they should
all be around 16.0 and 2.3, resp.

Previously, I checked all the constants in mmqbyhand and eqte2mmq to
be sure that they the same as those actual arrays (via the
getparams.pl program).

I double checked the equations -- they are the same.

Just for fun, I removed the exponentiation, ie,

mm = lnm
mq = lnmq

That brought the numbers into a much closer ballpark.  For example,
for ntof=345, nesd=50, mm=69.88 and mq=31.46.  The MM and MQ matrices
have values of 14.39 and 2.246, resp.

I tried multiplying the pav by powers of ten -- it increased.
Dividing it by 10.0 made it go negative.  I guess I could find a
factor by trial and error but I don't see what that would prove.

I tried putting in the pav from a3gxpavdpu() -- only about a 3% change
down, so this is not the problem.

I tried decreasing the E/q by using a lower numbered EDB, this made
the mw value increase while the mm value stayed mostly the same.
Besides, I double checked E/q values while doing the sxd/svsbare
validation and they were identical to values from libhef/perl.

11Oct2001:
----------

I fixed a problem copying in the constant arrays in eqte2mmq and now
it gives the same results as mmqbyhand, which is nice.  Of course,
they are still the wrong results.

So, that means my constants are all ok in mmqbyhand.

I tried changing the ntof to try to match the proper mass -- ntof=135
is close.  This makes no sense.

I tried changing the E/q -- no E/q (with a huge mass) would get the
m/q into the ballpark.

15Oct2001:
---------

Ok, I finally figured it out and it works.  There were two major
problems:  1) the results from libhef's dswxlnmq and dswxlnm were in
CHANNELS!  Geez, I should have known that.  2) the constants arrays in
eqte2mmq were not static but only loaded on the first run and, thus,
only produced a valid result the first time. 

Anyway, I ran it for a whole day and it compared well with the
matrix.  Note that individual values were not identical, but that
could easily because of digitization error (I think).  The values were
allways within a few tenths (for O7+) and the whole plot does not
appear to be shifted.

I'm removing the HUGE matrix stuff.

===> 9Oct2001 end-to-end testing for reasonable results (not validation)

1) I went through and put a lot of the output statements used for the
sxd validation behind if(DbgLvl >=... statements.

2) I added 12s to the timeInterval to account for the missing time of
the last cycle (since cycle times are for the beginning of the cycle)

3) I made MA rates again (from counts for the validation with sxd)

DistFunc still dies so I turned it off for now.

Here is a summary of results for axlv2 right now.  I saved the
axlv2_ma_2000158_2000158.dat file in runs/9Oct2001:

O7+ 0.453501
O8+ 0.044666
axlv2 -I- BoxRates O7+/O6+ = 0.584714

O6+ 1.58706
O7+ 0.612553
axlv2 -I- SpillRates O7+/O6+ = 0.385967

O6+ 1125
O7+ 783
axlv2 -I- ProbRates O7+/O6+ = 0.696

This calculation took 26 minutes.  Analysis: The BoxRates ratio looks
consistent with all the svsbare validation.  (The sxd ratio is .82
when hourly averaged counts are averaged.)  The SpillRates ratio seems
low.  I would think since O7+ is less abudent that it's counts would
go up while O6+ would stay about the same, making for a ratio that is
higher than the BoxRates.  (This is consistent with Nathan's
thinking.)  On the other hand, the ProbRates ratio looks better, so
things may be working somewhat.

===> 24Sep2001 validating with SXD

Nathan feels we should nail this down first.  So, he suggests 1)
figure out exactly what sxd is doing and 2) run libhef boxes and/or
swindal elipses to be sure we are comparing apples to apples.

Here are some runs with different counting schemes:
---------------------------------------------------
shorthands: accidental coincidence filtering	ACF
	    basic rate weighting		BRW
	    geometric factor			GF

sxd.pl uses elipses, ACF, BRW, GF
factor, 
0.836477     0.198850

svs using boxcounts with BRW (no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_box.svs', data=svs
read_sxd: reading data...
     0.793559  6.38987e-10    -0.373765    -0.487334

svs using boxcounts (no BRW, no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_wgtbox_1sigma.svs', data=svs
read_sxd: reading data...
     0.793566  5.79648e-09     0.605941      19.5108

svs using boxcounts and 10sigma (no BRW, no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_box_10sigma.svs', data=svs10
read_sxd: reading data...
     0.793559  6.38987e-10    -0.373765    -0.487334

[I'm confused that these values are changing so little so I'm going to
try to intentionally distort them and see what happens.]

svs using boxcounts with tof/esd centers mult. by 5.0

... Ok, I had a logic problem in the box counter which I only
discovered after stripping the routine down to nothing (svsbare.pl).
Here are some more real results:

name: ./x2000158_wgt_halfsigma.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
779.77  557.18  295.91  169.78  450.83  787.44  
O7+/O6+ = 0.71


name: ./x2000158.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
726.08  540.00  310.17  188.08  452.92  753.33  
O7+/O6+ = 0.74

name: ./x2000158_wgt_2sigma.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
1510.37 1090.85 610.10  368.34  903.84  1512.89 
O7+/O6+ = 0.72

Combining these with my old results from swindal:
		     (1/2)s	    s		2s
boxrates(libhef)     .71	    .74		.72
Boxrates	     .51	    .57		.59
Spillrates	     .49	    .55		.81

-----
25Sep2001:

Working with Nathan, we decided to compare with svsbare.pl very
carefully.  We found a few problems
 - Eqtab was being pushed and was thus offset from forward model and
   everything else -- FIXED
 - loadPha was running over one more cycle than AnalysisInterval
 - BoxRates::calc could have exceeded MA bounds 


-----
26Sep2001:

For 5 cycles, axlv2 reads 52987 PHAs.  MeasurementArray::fill examines
this same number of PHAs and the elements of MA sum to this same
number (even in getSlice later).  I think MA is ok.

svsbare.pl reads exactly this same number of PHAs as well.

Also, for the first cycle in the file, 

 rec 73576, Mon 05-Jun-2000 (157) 23:59:47

There are exactly the same number of PHAs in each EDB in both
programs.

-----
28Sep2001:  Further comparision with svsbare and tracking of PHAs
through swindal to explain difference in boxrates (with svsbare)

Summary of tracking/comparing PHAs for first cycle of 2000158.nc:

 - loadPHA reads 10587 PHAs
 - those PHAs all make into MA
 - number of PHAs in each slice (of MA, from outputMA run after
   Boxrates::calc) is exactly the same as the number for the
   corresponding edb when read in *and* exactly the same in svsbare
 - BOTTOM LINE:  The correct number of PHAs in the correct slice are
   definitely making it into Boxrates::calc.

Examining Boxrates::calc:

 - the boxes look like the right sizes (approx.) -- they were not exactly!
 - in edb 41, swindal gets 21 counts compared to 37 in svsbare
   Since the same number of counts at edb 41 went *in* to both box
   counters (526), the box counting procedure has to be the culprit.
 - while it is possible that svsbare is wrong, the results, across
   edbs, looks more reasonable: There is a notable peak (prev/subsequent
   edbs having counts which lead up to the max) and there are a few
   counts away from the peak.

3Oct2001:
---------

I finally got svsbare.pl and swindal to produce the same boxcounts for
the first cycle of 2000158.nc.

I put in "by hand" box counters in BoxRates::calc and in
svsbare::boxcount.  These had boxes of 338 <= tofch <= 350 and 44 <= esdch
<= 59.  In swindal this box (called o7box) found 36 counts; in
svsbare::boxcount it found 37 (round off difference, ok).  But, I
realized that these boxes *WERE NOT* the exact sizes that would come
from the foward model (when rounded to integers).  These boxes would
be 340 <= tofch <= 348 and 46 <= esdch <= 57.  (I calculated these by
hand from the forward model When I set both "by
hand" box counters to these ranges, they came up with exactly the same
results: 21 counts.  

It turns out, there was a problem with my counting logic in
svsbare::boxcount which was counting PHAs which fell into the esdch
box only.  (Geez!)

When I increased the number of cycles to 5, they two programs still
gave the same answers. They were also the same for 10 cycles (one
analysis interval).

When I run the two for a whole day, the results are close but not
identical.  This is because axlv2 reads more PHAs (993396) than
svsbare (979356).  This may be related to running axlv2 in multiple
analysis intervals.  I'm looking at the totals as they increment with
each cycle to compare.  (I've tabled this examination for now.)

I wrote a simple elipse counter (elipsecount).  It came up with less
counts but the same ratio (.6) as the box counter.  (See svsbare_wd_el.out
and x2000158_wd_el.svsbare.)

I changed the sigmas to sqrt(sigma).  It came up with less counts
again but a similar ratio to the others (.57).

Nathan and I resolved that I should push my method through to
completion now and deal with the sxd differences later.

===> 20Sep2001 found error in SpillRates, got ProbRates working

SpillRates:  Added - sign in numerator of Btil calc. (result of my
algebra mistake)

ProbRates:  Nathan made two suggestions after which I got actual
numbers,

1)  if |tof - fmtof| > 3stof (or likewise for esd), set G = 0.0

2)  Instead of summing the prob. into Nprob, add integer counts
whenever the prob. > PCutOff.  I set this cutoff to 0.7 and got actual
values.  Now, there aren't rates but O7+/O6+ = .77  .

He also suggested that I might be interpreting the foward model sigmas
wrong, either as twice or half what they should be.

We also came up with a better method for doing the prob. rates, which
actually amounts to two methods which will provide a high and low
bound.  These also integrate the measurements *back* into the rates directly.

1)  upper bound -- Foreach location in MA, give counts to the species
with the highest prob. at that location.

2)  lower bound -- Foreach location in MA, the each species a fraction
of the counts equal to it's probability at that location.

===> 12Sep2001 referencing to sxd

I fired up read_sxd.pro from sxd/devel/read_sxd.pro and read in the
data from 2000158.  I then calculated the ave. O7+/O6+ ratio for the day. 

read_sxd, file='/home/acedata/sxd/2000/x2000158.sxd', data=sxd
o7o6 = sxd.no7/sxd.no6
m = moment(o7o6, /nan)   ; /nan causes nan values to be discarded
print, m

I stuck these commands into a macro called o7o6.

The result was (ave, std. dev, ...)

    0.825485     0.212449      2.04040      7.80170

axlv2 results were pretty far off:

BoxRates      .5732
SpillRates    .9031

I also added functions for calculating these ratios withing axlv2.

===> 11Sep2001 mmq has grid of zeros

[I was distracted by the terrorist attack on the World Trade Center
and Pentagon today and ended up going home to be with Stacey while she
waited to hear from Tommy.

He had been on Wall St., just 2 small blocks from the WTC. While
watching the smoke from the first strike, he saw the second crash and
had to take cover.  He was kept in his building by police until after
the 1st tower collapse.  His building was hit with debris causing
shaking, plaster falling and screams from the occupants.

He fled the building and was a few blocks away when the second tower
collapsed, having to run from the ensuing smoke cloud.  We didn't hear
from him until about 1.5 hrs after the second collapse.

He ended up shaken up but ok.  
]


Briefly, after long run there is a regular grid of zeros in the mmq
plot but not in the ma plots.

I ran it again over the bastille day event, 200019?, and stored it in
runs/11Sep2001.  The MA look similar but with higher count levels and
much higher background noise.  The mmq plot was compared similarly --
including the zero grid.

12Sep2001:

I realized that the mmq plot was transposed relative to the axis
labels/scale. This resulted in a large spot at mass 6, which was the
tip-off that something was wrong.  (I did not believe Li was hugely
prevalent in the solar wind that day.)

This turned out to be a mistake of mine in two places: 1) The order of
indices for mmq in MeasurementArray::EtSlice2Mmq.  2) I had
inadvertently made the labels inconsistent with the order of indices
so that *the labels/scale* looked ok.  I switch the indices  and the
mmq plots look like they should now.

===> 10Sep2001 He rates 0

After running over all of 2000158, the He rates were still zero.  This
*has* to be a problem.  Other rates seemed reasonable:

axlv2 -I- dumping boxrates...
He1+ 0
He2+ 0
C4+ 0.0221252
C5+ 0.151242
C6+ 0.0735497
N5+ 0.0831191
N6+ 0.190478
O5+ 0.0119596
O6+ 0.499359
O7+ 0.286246
O8+ 0.0343599
20Ne6+ 0.00850973
20Ne7+ 0.0408466
20Ne8+ 0.1362
Si7+ 0.0189513
Si8+ 0.0386851
Si9+ 0.0443424
Si10+ 0.0452623
Si11+ 0.0925483
Si12+ 0.0764949
Fe6+ 0.000459982
Fe7+ 0.00354191
Fe8+ 0.0119595
Fe9+ 0.0184454
Fe10+ 0.0216654
Fe11+ 0.0215735
Fe12+ 0.0183536
Fe13+ 0.0137074
Fe14+ 0.00929175
Fe15+ 0.0128332
Fe16+ 0.0281963

Just to check, I dumped He rates per edb, they were still all exactly
zero:

He1+ 0( 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
He2+ 0( 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)

This cannot be right.

My plan is to examine the relevant region (by eye) of the tof-esd
matrix and see if there are counts there.  According to AnalysisData
(with DbgLvl=4):

He2+	tof		esd
	178 +/- 2.21	66.0 +/- 2.52
	339 +/- 4.20	17.8 +/- 1.82

I used the the following idl commands to produce a nice postscript
plot of MA (all slices summed):

.run smrp.pro
@loadct
psplt, file='ma.ps', /color
smrp, file='axlv2_ma.dat', data=ma
hardcopy, file='ma.ps', label='swindal testing', printer='file'

loadct contains the following two lines

;this idl macro is used in the r* macros in this dir
loadct, 33

I realized that I am only selecting region 1, which, I believe, drops
He.

[I updated smrp.pro and smmq.pro to include postscript plotting to a
file of the same name as the .dat matrix file but with a .ps
extension.]

===> 10Sep2001 (E/q, tof, esd) to (mass, m/q) conversion using functions

The functions dswxlnm and dswxlnmq calculate mass and m/q (resp., from
acetools/asxdpusim.c) from (E/q, tof, esd) for a given PHA word.  The
only problem with using them, once I get the calling sequence straight
(!), is that they also require a pointer to the adcnq array.  I'll
have to think about how to use this array from libhef directly.

After some searching, I discovered that I had worked on this already
when trying to get conversion info. for Nathan.  In

/usr/home/jraines/ace/nal2/parameters-for-nathan/

See

constants.txt

and 

getparams.pl

9Oct2001:  See ~/ace/libhef/notes.txt

===> 6Sep2001 ProbRates are too small

[The results are from a run over 2000158 - 2000165.  I had similar
results from just 2000158.]

Main problem:  The Gs are very small because 

Other things that don't look right:

     BR                SR
He1+ 0                 -4.38139e-08
He2+ 0	               2.03353e-09

Fe6+ 7.84662e-30       -2.37105e-09
Fe7+ 5.29323e-08       1.038e-07

Fe15+ 1.21767e-11      -3.10869e-05
Fe17+ 0                -2.63465e-05
	
Ok, I figured out one of the problems:  I was doing the normalization
(to counts/sec) wrong in MeasurementArray::fill.  I was simply
accumulating into MA locations then dividing those values by the
accumulation time during each analysis interval.  

This is fine for one analysis interval, but for the next one the
process causes counts to be added to rates, then the sum is divided by
accumulation time -- resulting in *very* small numbers indeed.

I fixed it by making a separate matrix for each, MA for the rates and
MAcounts for the counts.  The latter is then only used for
accumulating counts through one analysis interval. 

===> 24Aug2001 work log -- debugging SpillRates

First the elements of S were all zero.  I was using integer division
(1/2) which returned zero.  Then, (I swear) the spill rates were
giving reasonable values, but some were negative.  Finally , after
accidentally deleting some code, they are large (70 - 100).

Wait a minute.  I commented out code that tests the matrix inversion
and the spill rates came back down.

It turns out that declaring an ofstream 

ofstream fout;

is the culprit.  I don't know why.  If I use 

cout << P; 

to avoid the ofstream, everything is fine.

===> 22Aug2001 work log -- passing big objects

 - I figured out how to overload << properly; declare as friend (found
 in Deitel and Deitel).

 - realized that functions with pointer arguments (e.g. the .calc functions)
 should receive constant references, e.g.

   const vector<Ion>& grp

 That way, they are not copied in (no overhead) but a) they cannot be
changed in the function and b) they are called looking as though they
are called by value.  The latter makes sense since they are not
changed, contrary to the implication of passing them in as &var.  See
Stroustrup section 5.5 and 7.2 for more info.

 I tried to change the MA arg. in BoxRates.calc, but it won't allow
any sort of constant:

const MeasurementArray& MA		// recommended first choice
const MeasurementArray* MA		

all cause an error:

BoxRates.cc: In method `int BoxRates::calc (const MeasurementArray *, 
const vector<Ion, allocator<Ion> > &)':
BoxRates.cc:95: passing `const MeasurementArray' as `this' argument of 
`TNT::Matrix<float> MeasurementArray::getSlice (int)' discards 
qualifiers

Declaring a const. pointer, i.e. a pointer which holds a constant
address, works but does not accomplish the same thing:

MeasurementArray *const MA

I tried to fix SpillRates.calc and got the same results for
BoxRates::getBoxrates.  Since it works with the STL-vector group, I'm
guessing it is something wrong with the way my functions are
declared.  

For now, I will just pass the rates into the subsequent functions, the
arrays are only about 10kb and only get copied in once for each type
of rate (minimal overhead IMO). At least this doesn't make it look
like they are modified.  I changed all of the 'group' passing to
const. references.

16Oct2001:
----------

Will adding use of MA to ProbRates::calc, I tried the same thing,
i.e., passing MA as a constant reference.  It compiled.  Note:  I had
not yet modified the code to actually *use* MA in anyway.

As soon as I tried to use MA, e.g.,

MA.getElement(...)

I got the same error as above.

Deitel and Deitel p. 454 explained this and offered a solution.
Apparently, you can not call member functions of objects passed as
const (reference or not) unless the member functions themselves are
declared constant.  

I changed the member to constant:

float MeasurementArray::getElement(int nedb, int tofch, int esdch) const;

float MeasurementArray::getElement(int nedb, int tofch, int esdch) const {
...
}

And the error went away.  Deitel and Deitel to the rescue again.
It goes on to recommend that member functions should be declared const
if they don't need to modify the object -- meaning that many of the
ones in swindal certainly could be.

Stroustrup p229-30 also explains this somewhat.  It also explains that
passing by const reference is the preferred method for passing large
objects to functions which will not modify them. 

===> 15Aug2001 work log -- model tof/esd out of range

He2+:
tof 170.0 +/- 2.108 -> 347.9 +/- 4.314
esd 72.73 +/- 2.582 -> 16.90 +/- 1.800

O6+:
tof 193.7 +/- 2.402 -> 400.0 +/- 4.960
esd 184.0 +/- 8.197 -> 37.33 +/- 4.990

O7+:
tof 179.3 +/- 2.224 -> 370.4 +/- 4.592
esd 218.6 +/- 8.624 -> 44.06 +/- 5.214

Fe9+:
tof 293.1 +/- 3.634 -> 601.7 +/- 7.461
esd 182.0 +/- 12.98 -> 28.46 +/- 4.875

I looked carefully at AnalysisData::AnalysisData and realized that I
was using a3gxpavdpu to get the post-accel. voltage (PAV).  When I was
working with sxd.pl a few months ago, I discovered that I did not
understand the output of that function -- it returned voltages which
were different than the commanded ones, e.g., (pav is command voltage)

a3gxpavdpu returned 24.868179
pavlev=171 pav=26.100000

a3gxpavdpu returned 21.299759
pavlev=127 pav=22.800000

Now, it could be that those lower voltages take into account the
losses to the foil, but the offset doesn't seem to be quite constant
(could it be a linear function?)

Whatever a3gxpavdpu really returns, I am not using it (for now).  I
set up a look up table of known levels to commanded voltages

  map<int,float> pavlev2kv;
  pavlev2kv[127] = 22.8;
  pavlev2kv[171] = 26.1;

and use a3xpavlev to get the level

  PapsVolt = pavlev2kv[a3xpavlev()];




===> 14Aug2001 work log -- inversion

Got NR svdcmp and svbksb to work and solve a 2x2 linear system.
(Yeah!)  It worked write out of the book except I printed the solution
vector values (floats) as %d which gave confusing results until I
discovered the error.

Got inversion working for a 2 x 2 matrix.

Figured out dim() and size() in TNT:

A.dim(1) gives size of 1st dimension
A.dim(2) gives size of 2nd dimension
A.size() gives A.dim(1)*A.dim(2) 

See exp/matdim.cc for example code.

Got Invert working for test case.

===> 13Aug2001 work log -- spillover aborts

BoxRates.calc() causes abort because the tof2tofch returns out of
bounds toflo.  This could be fixed but it doesn't solve the problem.
I wrote exp/phaconv to explore the range of the function and found
that it easily exceeds the size of MeasurementArray. 

So, it looks like I'll have to implement the variable resolution that
we talked about earlier and a different scheme for mass and m/q
conversion.

===> 27Jul2001 work log -- mmq plotting

I'm trying to find that good color table that I used before.  So far
the following are decent:

27 Eos B	  great but white axis
33 Blue-Red 2	  great but blue background and axis
13 Rainbow	  fine but black background

===> 23Jul2001 work log -- m-m/q matrix
[Updated 24Jul2001.]

Plan:
----

 - make log-scaled binning matrix
   - 

 - cycle through each grid point in E-T slice
   - convert (esd,tch) -> (m,m/q)
     - Which?
       - equations
       - axdpusim
       - store from pha read in (keeps instrument specific stuff in
       that module) into a look up table
         - that would be another 70Mb cube!!  Bad idea.
         - It looks like I will go with the look up matrix for now to
	 keep things moving.

   - increment value at (m,m/q) by one




===> 19Jul2001 work log -- making a long run

I created longrun.cc which is xboxrates.cc with looping over data
files from start to finish.  This was not all that difficult, except
for some fooling around with char arrays vs. strings.  I ended up
using char array then copying it to gCurLv1File (string).

I created a function for incrementing the day properly (incrementYdoy)
and made sure the rest of the functions were skipped if loadPha
returned an error.  loadPha was already set up to handle multiple
files and that (actually!) worked as it was.

The only other problem was mysterious core dumping.  Apparently I had
not done 'make clean' on libhef since the upgrade and this was the
cause of the problem.

I had to add an include in hefnum/hefnum.i because DBL_MAX was not
defined.  I found it (with find/grep) in
/usr/include/kpathsea/c-minmax.h.  Apparently, Kpathsea is a directory
searching package included with TeX stuff, such as Xdvik.  I have a
hard time believing that Simon would use this arbitrary package, I'm
guessing this used to be included so I will continue searching on pooh
to find what (must have) been the original reference.  If worse comes
to worse, I'll hardcoded it myself in libhef.h.

I ran from 2000150 - 2000165.  It didn't crash!

plotting:
---------

I moved smrp to the swindal directory but can't get the thing to plot
in color again.  I knew I should have figured it out better back in June.

27Jul2001:
I was using color table 33 (Blue-red 2) and writing to a PS file with

psplt,/color, file'tmp.ps'

I gave up trying to use the screen graphics.

===> 15Jun2001 work log -- why does tof vs. esd plot look random

 - I checked that the arrays are being referrenced the same between
 C++/TNT and IDL, this includes checking that the array is read into
 IDL right.

 **********WRONG*********See IDL item below

 - MeasurementArray::fill seems to be adding to the correct MA
element:

MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=
MeasurementArray::fill-D- before add MA[41,324,0]=0
MeasurementArray::fill-D- after add MA[41,324,0]=1
...
MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=0
MeasurementArray::fill-D- before add MA[41,324,0]=1
MeasurementArray::fill-D- after add MA[41,324,0]=2
...
 MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=0
MeasurementArray::fill-D- before add MA[41,324,0]=2
MeasurementArray::fill-D- after add MA[41,324,0]=3
...

 - this seems to progress normally: (grepped for the add lines only)
...
MeasurementArray::fill-D- before add MA[41,324,0]=3
MeasurementArray::fill-D- after add MA[41,324,0]=4
MeasurementArray::fill-D- before add MA[41,324,0]=4
MeasurementArray::fill-D- after add MA[41,324,0]=5
MeasurementArray::fill-D- before add MA[41,324,0]=5
MeasurementArray::fill-D- after add MA[41,324,0]=6
MeasurementArray::fill-D- before add MA[41,324,0]=6
MeasurementArray::fill-D- after add MA[41,324,0]=7
MeasurementArray::fill-D- before add MA[41,324,0]=7
MeasurementArray::fill-D- after add MA[41,324,0]=8
MeasurementArray::fill-D- before add MA[41,324,0]=8
MeasurementArray::fill-D- after add MA[41,324,0]=9
MeasurementArray::fill-D- before add MA[41,324,0]=9
MeasurementArray::fill-D- after add MA[41,324,0]=10
...
MeasurementArray::fill-D- before add MA[41,324,0]=57
MeasurementArray::fill-D- after add MA[41,324,0]=58
MeasurementArray::fill-D- before norm MA[41,324,0]=58
MeasurementArray::fill-D- after norm MA[41,324,0]=58
(normalization off)

 - for fun I grabbed slice 41 and printed element (324,0) in
xboxrates:

  a = MA.getSlice(41);
  cout << thisprog << " -D- MA[41,324,0]=" << a[324][0] << endl;

output:

xboxrates -D- MA[41,324,0]=58

I figure it out and it was one big stupid mistake:  I tested IDL with
symmetric matrices.  So, when I transposed them I got the same
elements as in C++.  But, my region of interest was not symmetric, so
when I read it in transposed it cycled the elements around.  To fix
it, I changed my IDL code to 

a) read in a matrix with the dimensions flipped

mat = fltarr(coldim,rowdim)

readf,lun, mat

b) transpose the array

mat = transpose(mat)

Before the transpose, accessing element (324,0) does not give 58 (as
it is known to be from C++).  After the transpose, several elements
matched perfectly.  (I tried (324,0) and (328,19).  There should be no
way for them to match by coincidence.)


===> 3Jun2001 work log -- using TNT and LAPACK++

I decided to use TNT (Template Numerical Toolkit) for most of the
numerical matrix stuff.  It is organized in a way very similar to the
STL so it's use is easy to learn.  

There are two caveats:  

1) It is a beta version.  This seems ok since the swindal code will be
under development for the next several months anyway so changes to TNT
could be incorporated relatively easily.

2) It does not (yet I presume) include SVD.  I plan to use it's
precursor, LAPACK++ for that.  I considered using just the latter, but
it seems to make more sense to use TNT since it is current and seems
cleaner to use.  I can isolate the LAPACK++ stuff to a few routines,
most likely.  Or, I could use Numerical Recipes code for SVD.  Either
way, I think it is better to use the up and coming package in a new
project, rather than the one that is being replaced.

===> 21May2001 work log -- making boxrates arrays
===> 8May2001 work log -- running out of memory

loadPha runs out of memory after loading 530612 PHA words (about
half way through day 2000158).  I checked with top, it reported
xboxrates using 66M before it ran out of memory and exited.

Checking memory usage:  Each Pha should use

10  doubles	4 bytes ea.	 40 bytes
2   floats	2 bytes ea.	  4 bytes
7   int		4 bytes ea.	 28 bytes
1   string      ? bytes ea.       0 bytes
-----------------------------------------
				 72 bytes / Pha

530612 * 72 = 38 204 064 =~ 38M

This sounds possible.

Nathan and I decided to analyze PHAs in 5 cycle blocks per now (with 5
being a parameter).

===> 2May2001 work log -- member iterators?

Should iterators be declared in the class with the data object or
locally?  For example, in AnalysisData, if an iterator to the Ions
map<string,Ion> is declared in the local code where it is being used,
then the type of Ions is hardcoded in to that local code.  This
doesn't seem too encapsulated.  Come to think of it, since Ions is
public, the type of this is hardcoded anyway whenever it is used (by
syntax, since a key subscript is used for access).

I guess the encapsulation is pretty much gone unless I make the data
private and access it all with public functions.  Is this extra work
worth it in this context?

I'm saying no.  However, I will do the little bit of extra
encapsulation that is provided by declaring the iterator in the class.

===> 29Apr2001 work log -- no at() member function for vector

I could not resolve this issue:  Both Stroustrup and Josuttis (The C++
Standard Template Library) refer to an at member function for STL
vectors.  This function provides range-checked random access.
However, the library on hobbes (libstdc++-2.96-54) does not seem to
contain this function.  I searched all headers in /usr/include/g++-3
and found no mention of it.  And, I manually looked through all of the
public member functions in stl_vector.h.  This library version (2.96)
is more recent than the last on mentioned in the libstdc++ section on
gnu.org, so I don't think it is out of date.  And, the SGI STL site
does not include at in vector's member functions.  Also, compiling
with g++ on login.engin.umich.edu yields the same result as hobbes,
that there is no at member function.

Go figure.  I'm moving on.

===> 29Mar2001 Names

Michigan Solar Wind Plasma Data Analysis Library
M S W P D A L

Michigan Solar Wind Data Analysis Library
MiSoWiDAL
MSoWiDAL
