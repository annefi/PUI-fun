# File:  ionstream:~jraines/ace/axlv2/devel/AXLV2_Devel_Notes.txt
#
# Description:
#   Notes pertaining to development of ACE/SWICS Level 2 data
#   processor (version 2), SWINDAL and improvements to the ACE/SWICS
#   forward model.
#   
#   This file used to be 
#
#     ionstream:~jraines/ace/swindal/devel/Swindal_Notes.txt
#
# Author(s):  Jim Raines (jmr)
#
# Method:
#   All entries begin with a date/subject line preceeded by '===>'.
#   See entries for format
#
# Revision Controlled by CVS:
# $Id: AXLV2_Devel_Notes.txt,v 1.28 2005/06/09 04:07:09 jraines Exp $

===> Keep-at-Top Runs to be made
C5+/C6+ issue
o 2000156-9 at 1hr res.

===> Keep-at-Top To Do/Known Issues

Science:
--------
o Fe issues
  x implement physics FM
  o constrain motions of ion centers in prob. space
    o try prob. width and spill-over gaussian width on a per ion basis
x distribution function (DF) problems
  x output DF is not in proper units
x n problems
  o n is about O(1) too large.  It is not off by a constant factor
    from Ipavich, though.  It could be a linear function.
o v problems -- it is mostly quite good(!)
  o at high speed periods Fe9+ and Fe11+ are slow, could be from
    cutoff of very high E/q
x vth problems
  x slow overall compared to Ipavich
  x some heavies are quite jagged, with many 0 dropouts and some
    drop-downs.
x gauss-fitter
  x fit DF output to gaussian off-line with IDL
  o consider building
? build in knowledge of when charge state dist. are not decected well due
  to too large an E/q cutting off the distribution
  e.g. Low states of Fe could have been cutoff at very high solar wind
  velocities
o More careful check out C5+/C6+ for Ian Richardson
o verify O8+/O7+ for Collier
x add error calculation and assoc. data quality flagging
x determine best output method for delivered data products
x exploit assigned counts tracking for method verification

Code problems:
--------------

o 02Mar2005 -- change correctFeFM_phys.cc to correct all Fe ions, not
  start at Fe6+ and loop through

Code improvements:
------------------
o clean up method of returning single rates (by name maybe?)

o Change functions with pointer arguments (e.g. the .calc functions)
   so they receive constant references, e.g.

   const vector<Ion>& grp

   [This means the functions have to be const if they are to be
   called.]

o 19Aug2003 - calculate basic rate weighting per sector then average
   for more correct weighted counts, maybe use libhef::a3xbrwgt (but
   fix it first)?

o 09Oct2003 -- fix up AnalysisData encapsulation.
  o move loadFM to AceSwicsData
  o load constants (GeomFactor, EqAcceptFac, etc.) from AceSwicsData

- reading over multiple analysis intervals causes number of pha's not
   to align with svsbare.pl anymore.  The difference between
   svsbare.pl and axlv2 is +/- for different intervals
   -- is this fixed now? 19Feb2002 --

- 10Nov2003 -- SpillRates craps out with a Numerical Recipes error
  when run on 1998031 - 1998365.  Presumably, this will be a problem
  with other pre-paps ramp-up periods as well, maybe all.  (This is the
  first time I've tried to run a pre-paps ramp up day since I made
  SpillRates properly iterative.

- 30Sep2004 -- loading an MA does not load proper accum. time; right
  now, this only causes the 'counts' columns in the rate files to be
  wrong.  To fix, save accum. time along with MA and restore.

===> 06Jun2005 switching from count-based to time-based loader

Andrew and Thomas thought a loader which read all the cycles in a
given time interval would result in data which is easier to use for
the general community and easier to integrate with the ASC merged data
set.

This has not been a particularly switch since it required adding copy
constructors to AceSwicsData and Pha, as well as changing the loading
logic which has always been tricky.

I didn't initially realize that accum. times like 60 min. would cause
the next timestep's start cycle to often be included.  I switched to
59 min. and this problem went away.

I wrote some simple tools to help:

devel/tools/extract_load_fill_seq.pl
devel/tools/extract_accum_diff.pl

caveat:
The ladder picks up an interval irregularity at the start of most
files since the first doyfr is actually from doy-1 and not necessarily
spaced like the running accumulation.  This is ok and cannot be helped
without more extensive changes.

Right now, year 2004 is running (BR only).  The intervals are regular
except the above note so far.

===> 06Jun2005 merging OSX version with RH Linux version

I ported axlv2/swindal/libhef to OSX for running on the parallel
machines of Gombosi's group.  (See axlv2_porting_notes.txt for
details. )  Before changing the Linux version, I thought it best to
merge the two back together to keep things straight.

This worked with minimal effort.

===> 05Apr2005 revising deliv. output file formats

After closer inspection of the products that we promised, I realized
that we needed to make some changes in the deliverables:

1)  Densities other than He2+ should be removed from the deliv_nvt.  We did
not promise these.

2)  We should deliver the deliv_nvt data (kinetics mostly) at both 12
min (promised) and 2 hr (better).

3)  The Fe/O ratio was promised at the 12 min. resolution, so it
should be stuck in the deliv_nvt file (and probably left in the
deliv_elem file as well, for convenience).

4)  The S/O and N/O ratios should be removed from the deliv_elem file
and placed in a separate (maybe deliv_elem2 or deliv_elem_la (la - low
abundance))

Summary of changes:

- deliv_nvt -- remove all density ratios, added Fe/O
- deliv_elem -- re-ordered labels to match data, removed N/O and S/O
- deliv_elem_la -- (added) put in 3He/4He, N/O and S/O
- deliv_qratio -- changed C5+/C6+ label to C6+/C5+ to match data
- deliv_*_qdist -- no change

===> 22Mar2005 separating error levels by ion

Sue and I decided that we should have separate error levels for
different ions.  This came about largely because many good (by DF) Fe
states are thrown out with an n error cutoff of 50%.  Thomas argued
that this follows from the fact that Fe counts are spread over many
states (2-24+) so that the error *should* be higher.  Our anecdotal
evidence supports this as well.

We decided to set the following cutoffs:

Fe, Si, S:  .5 (yellow), 1.0 (red)
Rest:   .35 (yellow), .5 (red)

For now, the vth error cutoffs will be the same for all ions.

I decided the most sensible way to implement this was by putting these
cutoffs into the ion structure.

===> 02Mar2005 May 2000 anomaly

We are seeing strange results when comparing data before and after the
May 2000 rampup of the pav from 22.8 to 26.1 (TM voltage monitor).

Here are the facts:

- This whole issue started because Sue sent Fred data from 199950-70.
He made a comparison plot (comp4.gif) which showed axlv2 data very high (almost
O(1)) in comparison.  (Plots and email located in 
shrgproc:~jraines/ace/axlv2/devel/May2000-anomaly/umd-comp-18Feb2005 .)

- We looked back at a plot Sue had made of 2005 data compared to the
UMD weekly data (http://umtof.umd.edu/swics) where the agreement,
though done through comparing separate plots by eye, appeared very
good.

- Sue made a number of runs spanning the ramp-up.  
(shrproc:~slepri/runs/22Feb2005-SwepCompandFred/1-transitionto2000)

These plots show the density dropping hugely after the rampup.  She
plotted the box and prob. rates and saw the same effect.

- I made a longer run (8-full-pl), this time with the perl program
(run-axlv2-many.pl) and found that the rates no longer drop off.  I
realized that Sue made here incriminating run from one instance of
axlv2 (with the .sh program and differing start and stop).  I had
recently made a change that caused the forward model to only be loaded
on the very first load of the run.  This made sense to save time, but,
of course, doesn't take into account that one might run it across the
rampup period, where it wouldn't work. (I changed this to load at the
beginning of each accumulation, the most sensible, though not the
fastest, interval.)

I plotted the axlv2_br*.dat file and found that THE RATES NO LONGER
DROPPED across the rampup!  However, I had some trouble with the
plotting program and got side-tracked

-----

Pat's ideas:  

- Try including doubles in our box counts (couldn't get them
beyond there) to see if we see UMD-like behavior.

- Check our other data products to see if they see a change

  The version 1 LV2 data only contains ratios which are not sensitive
  to this problem.

  I have the sxd data plotted day by day so it isn't that easy to see
  trends, but it appears that nH, nC5 and nO6 go up after the ramp.  I
  guess this makes more sense if the increased pav increases efficiencies.

  I wrote a mini script to compare these, mom.pro, and wrote them to
  an output file, pre_post_moments.txt.  There was no

Thomas' ideas:

- check for double-counted PHAs in pre. period

-----

04Mar2005:
----------

I plotted the re-run 110-150 period, run with the Perl program, and
put the data in 9-110-150-pl.  I plotted a time series of the data and
confirmed that the box rates no longer drop off like they did before.
The probabilistic rates also do not drop off.  See plots

axlv2_br_2000110_2000150.ps
axlv2_pr_2000110_2000150.ps

Thinking that the efficiencies may now be the problem, I hardcoded
them to 1.0 and set off that series for a run over the weekend.

07Mar2005:
----------
Unfortunately, I forgot the run-axlv2-many.pl does not run my personal
version of axlv2, so the weekend run was wasted.  I changed this and
restarted the run.

I also realized that the current version of the deliv. files aren't as
good as we need, in case we will actually keep this 2004 run
(04Mar2005-year-2004/).  Sue and I decided to stop it, fix the issues
and restart in a different directory.

08Mar2005:
----------

The eff=1.0 run (c-110-150-eff-1) has gone far enough to be of use, so
I am plotting it up.

===> 22Feb2005 critical path as of today

Jim:

- Nail down UMD comparison
  - add He2+ to AssignedCount output
  - release latest axlv2

- re-run varied 10 day x 6
- run varied 3d x 6 at 12 min.
- Add ions/restrict deliv. ions
- get into amlv2 problem

Sue:

- Nail down UMD comparison
  - re-run 1999055-060 -- turn on E-T slices and MMQ output
  - plot correlations of n and fit_n, etc.

- make SWIMS eff. plots
- look at 12 min. kinetic properties
- work on data description document (D^3)

===> 14Jan2005 clustering of prob. centers in low count conditions

Dir: axlv2/runs/10Jan2005-m12_14-lowQ-enhance/6-1h-mmq-all-low-cno/ 
Data:  step 29 on doy 2005 9.40 (axlv2_et_slices_009_40.dat and
axlv2_pccomp_009_40.dat)
Note:  This day/time had unusually high counts for low charge CNO.

With PR cutoffs at 3.0 and 1.0 for non-Fe and Fe (resp.), Sue and I
noticed many ions overlapping in the following box:

TOF: 420-450
ESD: 20-35

We identified these ions as C3+ O4+ Mg6+ Si7+ Fe13+ with the following
commands:  (C3+ and O4+ were not piled on top of each other with the rest)

i = where(pcc.step eq 29 and (pcc.tof ge 420. and pcc.tof le 450.) and $
          (pcc.esd ge 20. and pcc.esd le 35.))
print, pcc[i].ion

We looked at the distribution functions (DF) and found that there
seemed to be small peaks around 520 in most of them.  We attributed
this peak to step 29.  Fortunately, the DF were all pretty small and
had large (> .20) F0 errors.

There seemed to be counts in only one TOF,ESD box.  Also, there were
no counts near any of the FM centers for ions in question.  I
speculated causes this oddity:  The prob. are zero every where else in
the neighborhood (0 counts outside the one box).  As such, even with the
mathematical penalty in probabilty for overlapping, this TOF,ESD
coordinate is still the highest prob. for those ions.  

The net effect of this issue does not seem significant, since it only
occurs for ions that don't have any counts anyway and only when the
counts are concentrated into one box.  I don't think any real peaks
fit this description.

===> 06Jan2005 validation of delivered products

Brainstorming:
--------------
- start with those 5 periods w/10 day windows
- compare SWEPAM vHe and nHe
- pick group of major ion v/vth and plot together to see
coherence (timeseries and correlation (scatter))
- Q contour plots
- elemental abundances relative to oxygen
- plot quality flag

===> 10Dec2004 moments of gaussian

After switching around DistFunc::calc to push the gaussian (fit_A)
through the moments calculation, weird behavior was observed.

Timeline:

---

dir: 18Nov2004-1st-cut-quality/2-varied-intervals
tag: feeling_out_err_level
days/res: from varied-1d.txt, 1d res.
No calcMoments function; fit_v/fit_vth from center/width of gaussian;
2000178:  C4+ v0 = 443.

---

dir: 01Dec2004-fit_A-moments/1
tag: n/a
days/res.: 2003302, 1h
This was an hourly run to test the calcMoments function.  I figured
out later that I might be shadowing class variables with the the n, v,
vth arguments to calcMoments (intended to be local)

---

dir: 01Dec2004-fit_A-moments/2-made-nvt-local/
tag: n/a
days/res.: 2003302, 1h
This was an hourly run to test the calcMoments function after changing
n, v and vth in the calcMoments arguments to _local.
C4+ v0 is cursorly the same as 1/

---

dir: 18Nov2004-1st-cut-quality/3-varied-fit_A-mom
tag: feeling_out_err_level
days/res: from varied-1d.txt, 1d
1st multi. day run with calcMoments 
C4+ v0 = 460.
 
---

dir: 01Dec2004-fit_A-moments/3-no-change/
tag: 3-no-change
C4+ v0 = 460.

---

dir: 01Dec2004-fit_A-moments/4-released-ver/ 
tag: 4-release-ver
days/res.: 2000178, 1d
C4+ v0 = 460.
This is the 23Nov dated axlv2 from /shrg1/ace/sw/bin.

---

dir: 01Dec2004-fit_A-moments/5-gauss-v-vth/ 
tag: 5-gauss-v-vth
days/res.: 2000178, 1d
C4+ v0 = 460.

---

dir: 01Dec2004-fit_A-moments/7-302-1h-released-ver
tag: n/a
days/res.: 2003302, 1h
C4+ v0 is cursorly same as 1/

---

Sue and I came up with the idea that this might be related to the 1d
versus 3d moment issue.  I discovered that, while a 1D Maxwellian
distribution has a Gaussian shape, a 3D Maxwellian does not!  I had
never noticed this in all my years of thinking about these.  Compare:

1D Maxwellian (Gaussian shape):

f(v) = (m/(2 pi k T))^3/2 exp(-v^2/vth^2)

3D Maxwellian (Gaussian-like shape but with extended high-v tail):

g(v) = 4 pi n (m/(2 pi k T))^3/2 v^2 exp(-v^2/vth^2)

Note the extra independent variable (v^2) outside the exp().

Rather than checking out the various past versions of the code, I'm
going to try to move ahead by making a 1D moments function and a 3D
Maxwellian function for fitting.  Hopefully different combinations of
these will help me figure out what is going on.

Also, I think I will create a showVersions() routine which contains
all the RCS IDs of all the code.  (Some don't have that string in the
source, but I can be sure to include it in any routines I'm actively
developing.)  

---

Wait a minute, I just realized that the 3D dist. functions were likely
implemented on 22Nov2004 -- after the quizzical
18Nov2004-1st-cut-quality/2-varied-intervals (C4+ v0 = 443).  I had
checked when I ran the new 3D dist. func. code and thought that the
nvt's were the same, but I may have been mistaken.  Hopefully trying
combinations of the 1D and 3D DF will clear it up.

===> 18Nov2004 quality flag

I need to figure out the best information to use for a quality flag
and the number of separate flags within it.

brainstorming possible flags:
-----------------------------
basic:
- low statistics (fractional error cutoff?)
  Make this DF parameter and push it out the the UI.
  Average fractional error >= 20% (10%?) -- use IDL code to get a feel

  plan: colorize IDL error bars by fractions (<10, 10-20, >20) to
  determine fractions

- non-thermal distribution (vth comparision?)
  options: vth only -- Sue thinks this is the necessary measure
           vth and v
	   (stay away from n!)
  Thomas suggested the following comparision:
  - plotting corresponding parameters against each other
  - fit to a line?
  - find std. dev. around line
  - compare empirical cutoff

  plan: 
  - try Thomas' suggestion in IDL
  - add vth to existing IDL (maybe using Thomas' method for compare)

- multiple peaks (ouch! maybe)
  This might be fairly simple if it just looks for multiple (genuine)
  maxima with separation above some cutoff.  I might need to calc. a
  smoothed (boxcar or exp) A first, but that is straight-forward.

  plan:
  - Sue's suggestion: A large difference (fractional or absolute) in
  the moment vth and the fit vth with a reasonable error may indicate
  multiple peaks


more:
- individual comparision for nvt to gauss fit, compare to some cutoff
- non-thermal, major component nvt reported (that could be tricky)
  something like: 
  - smooth
  - locate peaks
  - find peak areas
  - compare against some cutoff
  - if there is no clear major/minor, report fill values or caveat emptor?
 
  Let's see how the basic non-thermal flag goes

for all flags:
- keep stats on number of nvt items marked with a flag

19Nov2004:
----------

I got some good days to run for varied conditions from Sue:

2000178	   CME, high <QFe>, high O7/O6, low Np

2000315	   coronal hole, low O7/O6, moderate <QFe>

1998057	   looks quite, slowstream, positive polarity

2002327	   positive polarity, relatively quiet

(I added)

2003302	   crazy high CME day

I also decided it would be worth my time to add the ability to run
multiple discrete intervals to run-axlv2-many.pl.

29Nov2004:
----------

I fixed up run-axlv2-many.pl as stated above making a pretty handy
tool for running code.  Now, I need to decide the best way to set up
the running of axlv2_distfunc.pro to analyze the data.  It has to be
automated -- doesn't everything?! :-0 -- but I'm still trying to
decide on the best way.

Three quality checks have been implemented in axlv2_distfunc.pro as
coloring of the error zero line (ave. error), the peak centers
(v/fit_v) and the fit function (vth/fit_vth).  The task now is to
find a set of parameters that adequately characterizes the
quality.  If no set can be found, additional quality logic will have
to be added.

More planning for quality evaluation:

- two parts: 
1) Evaluate performance on test cases which are particular
combinations of the individual qualities and the peak shape.
Hopefully this set can be extracted from the 1-day resolution files
run in 2-varied-intervals/.

2) Evaluate performance on 1-hr resolution runs of the same period
focusing on one ion at a time

[Part 3, evaluating the performance on more data is strictly part of
validation.]

- when a decent set of parameters has been determined, the logic
should be implemented in the axlv2 code.  At that time, the current
axlv2_distfunc.pro should be moved to axlv2_distfunc

30Nov2004:
----------

I found a condition where the error and v comparisons were green and
the fit looked almost perfect -- about as good as it will get.  Still,
vth/fit_vth = .2803, a fairly large number.  

Sue and I decided that we should be considering the resolutions
(analyzer and integration step) when making the vth comparision, since
they are so close.  As such, we decided to change the vth comparison
to the following:

abs(vth - fit_vth) < n delta_v

with 

esa_delta_v = esa_delta v / 2 = 0.064 v / 2  (derived from deltaE/q / E/q)

the multiple (n) will be determined empirically, but will likely have
to be at least 2.

(I could also calculate the integration step delta_v.  It is likely that
the vth comparison cannot get better than the largest of these two.
I'll do this if this esa_delta_v method doesn't prove satisfactory.)

I solved this for n and set up some quality levels of n, [10., 3., 2.]
and ran the axlv2_distfunc.pro.  The results look a lot more
promising:

analysis log:
+-+-+-+-+-+-+

(Ran timeperiods at start of entry; all at 1d resolution.)

2002327 -- vth comparison looks good, v comp. looks good., errors seem
ok at first look

specifics: 

He2+ is all green which seems appropriate

Fe8+ shows green vth despite small shoulder, seems appropriate

---

1998057 -- more small shoulders w/green vth, looks good; vth looks
good overall; errors look bad, many red -- which will likely be fill
values in final product

---

2000178 -- same story as 1998057, error values seem *high*

--- 

2000315 -- He2+ green vth despite big shoulder, this comp. may not
work for detection of overlaped multiple peaks.  comp_vth = .483 so it
is not a limit problem.

C4+ -- bigger shoulder, vth yellow, comp_vth = 2.13
C5+ -- peak is genuinely bifurcated (err and v green), vth red at
3.04.  Maybe this will work for mult. peaks.  N5+ is similar but with
yellow err.
O6+ -- bifurcated and picked up by comp_vth = 3.25 (red).  Good!

I see more of the same on this day, lots of bifurcation.  There must
be two populations.  It may be that yellow/green err. + y/g v + r vth
(> 3.0) might indicate multple peaks.

S8+ -- gauss fit grabs one side of bifurcated peak and vth is still
yellow.  But, comp_v is red, so that still flags it.

F11+ -- big shoulder missed by gauss fit but comp_v is (again) red

F22+ -- gauss fit goes bad and is one big hump, comp_vth = 16.3 (red)
with red err.

---

2002327 -- pretty good

---

2003302 -- H+ green err but terrible DF, v and vth (10.7) red

--- 

2003303:

He2+ -- green err and vth; yellow v reflects shift in gauss fit peak
by exclusion of a high v tail in the actual DF.  This may be useful as
a measure of shouldered (but not multi) peaks.  (Multi. peaks would
likely have red vth (though I think not always.)

C4+ -- spikey/bumpy DF but green v and vth (red err.); this may be a
complication if error thresholds are relaxed some.

20Ne6+ -- similar to C4+

Fe6+ -- green v and vth but huge red error

---

2003304 -- lots of bifurcated peaks

---

2003305 -- Fe6-14+ red errors!

--------------

Sue and I discussed the possibility that the average of the ErrA[nedb]
might not be the right measure.  She suggested sqrt(sum(A))/sum(A).
This value is in fact different, but it seems to be even worse for the
bad ones, which does not help.  I tried some off-hand computations ofr
sqrt(sum(P))/sum(P) from the 'axlv2_pr_yyyydoy_total.dat' file (after
adding '/counts' option to axlv2_rates).  This gave yet another, much
smaller error.  These errors tend to be much differnt, though the
first two are often similar when small.  For example:

doy 2000178
sums are over steps (i) and for a particular species

ion     <sqrt(Pi)/Pi>	sqrt(sum(Ai))/sum(Ai)	sqrt(sum(Pi))/sum(Pi)
C4+	.15		.16			.024
N6+	.23		.49			.037
O6+	.16		.09			.008
Mg7+	.58		.82			.1
Fe8+	.27		.24			.03

The big question:  Which is the right way?

===> 11Nov2004 added ion-specific PR Gauss Cutoff (written 18Nov)

I had little trouble making this work.  The bigger question is whether
it is doing anything.  One a single day run, even 2003 302, there are
so few counts that it is impossible to make any judgements about peak
alignments.  My thought now is to make those judgements on longer
accumulations and try my best to confirm, at least position-wise, that
they are the same for short timesteps.

===> 09Nov2004 adding gauss fitter

Parent directory: axlv2/runs/04Nov2004-adding-gauss-fit

I started work on this last week and made fairly quick progress,
considering that I switched to NR C++ and TNT 1.3 (for NR C++).  All
this proceeded with little trouble.

However, on Friday (5Nov) I started having problems with the fitting
routine, mrqmin.  It keeps dying saying that the matrix is singular.
Add to that, I started getting really strange behavior later on in the
day on Friday:  GaussFitA wasn't being run even though it should have
been.  I'm not sure the cause, but I have it calmed down now.

As a double check of the gaussian, I implemented the fit_A (gaussian)
output and added that plotting to axlv2_distfunc.pro.  After I changed
the gaussian parameters to Amax, v0 and vth (in that order), I got
very reasonable gaussians (as a function of v not edb).

b-alamda/:
----------

This run died (singular matrix) but the axlv2_distfunc*.ps plot shows
good agreement with the actual dist. function!

10Nov2004:
---------

I finally got the gauss fitter working after having several problems.
On notable change was to switch NR in C++ over to using TNT and then
use *exclusively* TNT variables (not mixing in NR C++ style
matrices/vectors).  In all, despite the difficulties, it was much
easier to get the gauss fitter to work using NR C++ than it has been
in the past with NR C.

===> 19Oct2004 adding statistical quality check

Parent dir:  axlv2/runs/19Oct2004-add-stat-qual-check/

I decided to put the statistical quality check in the distribution
function file, since it has a free column; it's format mirrors rate files.

===> 19Oct2004 updated FM added to libhef

I copied the lines recently added to xfmexp.cc into libhef::xfm.  I
then recompiled all my code and the released versions ($ACESW/src).

===> 18Oct2004 adding 3He to pav=22.8 FM
(See 'Procedure' section at the end of the 01Oct2004 entry for a
general procedure used.)

I first looked in 2001 (see H entry, same day) and had trouble finding
it.  Going back and forth in the E-T slices (steps 49-54) showed that
the another peak, roughly between H+ and 4He2+, seemed to appear in
steps 50-53, right before a constant-esdch streak set in.  I took it
as 3He2+, even though that is not where the 2001 plots show it.  There
were no other likely candidates so that was the best that could be
done.

steps:  50-53
fmpick -I- dumping H parameters:
+ H[1]=  1452412.00
+ H[3]=      -34.77
+ H[4]=        3.07

I put these coef. into xfmexp.pro and xfmexp.cc.

===> 18Oct2004 adding H to pav=22.8 FM

(See 'Procedure' section at the end of the 01Oct2004 entry for a
general procedure used.)

Protons don't usually show up in this analysis because they are
directed into the aux. channel during normal solar wind speed
conditions.  However, enough high speed solar wind is encountered
during a year to make them show up in yearly accumulations.

To find H+, I first looked in 2001
(24Sep2004-reworking-loader/d-load-2001/axlv2_fmcomp.ps) and found a
substantial H+ peak in all steps from 21 to the 40's.  (I didn't look
for the end.)

In 1999, I found something similar, with a good range for picking
begin 36-42.  Then I defined and picked those peaks interactively with
fmpick.  I had to modify ion_parse because it returned state=0 for H+
(no number in name) and the E-T slice comparison section so that the
He centers did not match 'H'.  (It was using strpos; I switched it to
the more precies stregex and added a logical pattern to avoid these
mis-matches.)

After that, I got some reasonable results:

steps: 36-42
fmpick -I- dumping H parameters:
+ H[1]=  1466554.25
+ H[3]=       28.05
+ H[4]=       -1.64

The center looked good enough for now.

I added these values to xfmexp.pro and xfmexp.cc.

===> 12Oct2004 adding sulfur to pav=22.8 FM

(See 'Procedure' section at the end of the 01Oct2004 entry for a
general procedure used.)

Sulfur:

Analyzed states: 8-11+.
Rel. abundance: .45 9+ and .35 8+ @ 1.2MK
Parent run directory:  axlv2/runs/01Oct2004-adding-xfm-ions

I looked back at 2001 in 24Sep2004-reworking-loader/d-load-2001 to see
where S is located in 2001.  

14Oct2004:

Sulfur is really(!) hard to find.  This is because the major ions,
S8/9+ overlap strongly with Si7/8+.  The overlap in the E/q dimension
is found by inverting KE to find v(E/q,m/q) then setting those equal
and solving.  The result is:

(E/q)1/(E/q)2  = (m/q)1/(m/q)2

(Which I guess is pretty obvious from KE.  The numbers are subscripts.)

ion	     m/q
Si8+	     3.5
S9+	     3.55
Si7+	     4
S8+	     4

For Si8+/S9+, the RHS is 

3.5/3.55 = .986

That is better than a 98% overlap!  This means they both peak at the
same E/q step.

Now, within a step (in one E-T slice), the TOF overlap is slightly
different.  It is governed by the amount of energy gained in
post-acceleration:

V q1 = KE = 1/2 m1 v1^2 = 1/2 m2 v2^2

Solving for V and setting equal gives

(m/q)1 / (m/q)2 = v2^2 / v1^2

or 

v2/v1 = sqrt((m/q)1/(m/q)2)

That gives a 99.3% overlap in TOF space.  Geez.  

The overlap in ESD is governed by the same m/q ratio as E/q.

I tried to find S by taking ESDCH slices at particular TOFCH values
(fmpick_slices.{pro,.ps}), but I still couldn't separate the S9+ peak from the
Si8+.  Note, I did get confused by another peak (step,tofch,esdch) =
(37,443,60), but it was clear by looking at the E-T slice that this
was something else, probably Fe.

In fmpick_slices.pro, I used the following definitions in the config section:

steps = [34,35,36,37,38,39]
lines = [66,63,63,60,47,43]
tofs = indgen(25) + 440
esd_range = [1.,100.]
yrange = [1.e-7,1.e-5]          ; ~= zrange from fmcomp
npp = 9

In the end, I decided that I could not really find S and just picked
some peaks that put it approx. where I see it in 2001 and not
overlapping Fe or Si much.  We'll have to worry about this later.

I used steps 36-8 and picked S9/10+ in those slices.  I picked them in
the faint region overlapping the upper right corner of the S8/9+ peak
(resp.).

The program produced:

fmpick -I- dumping H parameters:
+ H[1]=  1382425.50
+ H[3]=       21.23
+ H[4]=        0.58

I saved the files as

fmpick_peakdata_S9_10_final.save
fmpick_fit_S9_10_final.ps

The slice plots in fmpick_fit_10_final.ps so reasonable S centers, as
compared to 2001 data. I'm moving on now.

===> 01Oct2004 adding Mg to pav=22.8 FM, procedure and troubleshooting

Need:  1H, 3He, 24Mg, 32S
Parent run directory:  axlv2/runs/01Oct2004-adding-xfm-ions

It took me several hours to figure out how I was doing this last and
get it working again.  I also made some improvements in the code to
allow easier entering of step/ion pairs.  And, I had left the ESD
fitting ill-conceived and broken (see below), which I fixed.

Method overview:

- enter step/ion pairs (e.g. step 29, Mg10+)
- click one by one on these ions on an E-T display with current FM
centers and previously clicked centers plotted.
- optionally adjust TOF constant, H1, manually
- TOF fit:  The set of picked points, (tof, charge) are fit using
xfmtof_fit (which calls an IDL xfm to produce the four FM values).
These are then plotted into fmpick_fit.ps.
- ESD fit: The set of picked charges are used to create a set of
Etilde's, the un-parameterized energies.  These are then paired with
picked ESDs and fit to 

ESD    = H3[im] + H4[im] * Etilde

These fits are also plotted.
- finally, the selected E-T slices are re-plotted with the original FM
centers and the new ones (from just-fit H params) for comparision

05Oct2004:
----------

I was using saved items from the loader re-work runs
(24Sep2004-reworking-loader/).  Since the 1999 (b-1999-1ts) run did
not produce FM output (why?), I was stupidly using axlv2_fmcomp.dat
from the 2001 run (c-2001-1ts).  Of course, these would be completely
different FM's!!  (Geez.)

I was able to make a run loading that saved MA (b-1999-1ts) and output
a new axlv2_fmcomp.dat but it doesn't seem quite right: The existing
centers seem pretty high (Si seems good) on the ESDCH axis, though the
TOFCH axis looks fine..  At first I thought these steps were just bad
examples for O6,7+, C5,6+, but the same steps in the 2001 accumulation
(c-2001-1ts) look great.

Here are some questions:
Q:  Why aren't the Mg ions coming back with 0.0 FM values since they
are not in the model?
A:  Sulfur also has non-zero values.  It is because inside
libhef::swxtools::xfm, adfm is a static array which is *not*
initialized.  The ions H+ and 3He1,2+ don't work because they are the
first ones.  Magnesium is just given the values from Ne and S those
from Si.  This should be fixed but doesn't really matter right now.

Q:  Why have some centers (C and O notably) *apparently* shifted higher
in energy since Nov. 2002?

Q: Are the output of the fmpick (center over slices plots) and fmcomp
the same?
A: From step 31, they appear exactly the same on the fmcomp plot.  I
diff'ed the two files, .  I am assuming this to

$ diff 2-highFM-libhef-only/axlv2_fmcomp.dat \
       3-highFM-libhef+phys/axlv2_fmcomp.dat

This yielded only Fe ions, so only those differ.

Q: Is xfmexp_phys correction (suposedly of Fe only) some how affecting
others?
A: No.  I ran it with only libhef::xfm (2-highFM-libhef-only) and the results
were the same.

Q: What is different in the method (up to E-T slice and FM output) 
since Nov. 2002?
A:  A lot!
- new loader
- new machine (shrgproc)
- new compiler version

08Oct2004:

Synopsis:  I don't know why, but the 1999 FM definitely seems to be
high in E, compared to the way it was before.  I'm going to sit on
this for now and deal with it later.

Back to finding Mg...

11Oct2004:

It turns out the Etilde/ESD fit for Mg was failing because I had left
it broken (Nov. 2003) and ill-conceived.  I was sending (etil,p.esd)
points to xfmfit_esd, but it was expecting p.crg as the independent
variable.  Since they were the same size, there was no explicit
complaint.  Now, that could be fixed by passing the charges in via the
common block **BUT** why even use xfm for the H3/4 fitting?  It is
just a linear fit between Etilde and (picked) ESD.  So, switched to a
linear fit and wrote a big warning in the comment of xfmfit_esd to
this effect.

The Mg center looks pretty good in the diagnostic slices that fmpick
plots, so I put the H's into xfmexp.pro and xfmexp.cc.

12Oct2004:

Mg fits pretty well now.  Here are the values:

(ion,step) pairs: All Mg10+ (couldn't find others) steps 29-32.
H1 = 1392056.0
H3 = -2.17
H4 = 1.00


**********************************************************************
Procedure:

- Run axlv2 for long accumulation (e.g. 1 year) with 1 timestep,
answer 1 to save MA query.  The program will complain about the ion of
interest being missing, but will proceed anyway.  **But, do not run
any rate calculations; axlv2 will crash.**  Note that missing ion(s)
may not have empty FM values because libhef::xfm does not initialize
that array.  Therefore, missing ions have FM values of the last good
ion.  This may be fixed in the future.

Alternatively, set FM type to '-1' (none) to avoid complaints of
missing ions.  The problem with this is that axlv2_fmcomp.dat will be
empty, which means axlv2_fmcomp.ps will not have any FM centers.  It
is typically hard to find an unknown ion this way.

- Examine axlv2_fmcomp.ps from the run.  Try to find the ion of
interest in the E-T slices.  Tips:

  - watch for peaks to build and decay as you page through energy
  steps.  They will often do this somewhat independently of near
  (maybe overlapping) neighbors, which makes finding the peak and the
  center easier

  - Carefully adjusting the zrange in fmcomp can help bring peaks out.

  fmcomp, fmcfile='axlv2_fmcomp.dat', etsfile='axlv2_et_slices_365_99.dat',$
  tag='1999001-365, finding Mg,',fmcdata=fmc, $
  steps=[27,28,29,30,31,32,33,34,35,36,37,38], $
  zrange=[1.e-9,7.e-5], etsdata=ets

  add '/noread' after first time through

- determine set of (ion,step) pairs to pick from E-T slices.  If well
defined peaks for multiple charge states can be found, the coef. will
result in a better fit across the whole charge range.  Of course, the
more points used in the fit, the better.

- insert dummy H1-6 coef. for desired ion into xfmexp.pro::xfm.
  Copying them from a nearby (in mass) ion works well enough.

- run fmpick as follows to define, and pick the peaks.  Use the same
zrange that proved useful with fmcomp.

  fmpick, fmcfile='axlv2_fmcomp.dat',etsfile='axlv2_et_slices_0-1_-99.dat',$
  fmcdata=fmc, etsdata=ets, element='Mg', mass=24.0, pav=21.2998,$
  subsec=2, $
  peakdata=pd, zrange=[1.e-9,7.e-5], $

  after running this once, add the '/noread' arg. to speed it up

- once you have a good set of picked peaks, add the '/nopick' and
'/nodef' keywords to speed up the process

  fmpick, fmcfile='axlv2_fmcomp.dat',etsfile='axlv2_et_slices_0-1_-99.dat',$
  fmcdata=fmc, etsdata=ets, element='Mg', mass=24.0, pav=21.2998,$
  subsec=2, $
  peakdata=pd, zrange=[1.e-9,7.e-5], $
  /noread, /noadj, /nodef, /nopick

- look at fmpick_fit.ps to examine the fits and the resulting FM
centers.  When you are satisfied with the results, put those H values
into xfmexp.cc and make some runs.  (Put them in xfmexp.pro  as well,
so they will work when doing the next ion.)  Eventually, put them in
libhef::xfm for all to use.

- if you can't find the peak in the slices, try fmpick_slices.pro.
Values for steps, tof range, esd range, etc. are hardcoded in along
with a location at which to draw a line (for testing peak center
guesses).  This works pretty well, but beware of peaking peaks in the
completely wrong ESDCH neighborhood.  (Refer the the E-T slice plot
often.)  These can be entered into fmpick.pro by specifying '/nopick'
but *not* '/nodef'.

**********************************************************************
===> 24Sep2004 re-working loading loop in axlv2
Parent run directory: axlv2/runs/24Sep2004-reworking-loader/

I re-worked the loading loop so that the number of cycles to load at a
time and the number of cycles per timestep are specified separately.
This was mainly motivated by the need to add more ions to the pav=22.8
FM, which required axlv2_daily.cc.  This code was out of date wrt
axlv2 and the library (and the latest improvements) so I decided to
integrate the long accumulation ability into axlv2 once and for all.

I used a mini-axlv2, axlv2_devel.cc, to test the load loop.  The
result was simpler than amlv2, luckily because that one didn't come
out very clear.  (I left the building of axlv2_devel in the main
makefile but not automatically.)

I came up with one scope issue:  Does re-initializing variables,
i.e. MA, guarantee that they are fresh, new copies?  To test this, I
bracketed the MA stuff to make a block and compared.  The output files
(5-2hr/run-axlv2.out and 6-explicit-MA-scope/run-axlv2.out) were the
same except for irrelevant dates/times.

I should check on this to know for sure.

30Sep2004:

I got the new loader working correctly for different accumulation
times across days.  (I did not test different numbers of cycles in
memory.)  A year long accumulation into one timestep works and produces
all output files.  The rate plots look reasonable, as do the E-T slices.

I got the 'save MA' feature working again and added FM options, most
notably a 'no FM' option for working with ions not in the FM.  (Of
course, none of the method can be run without the FM.)

I tested loading an MA then performing the calculations for
accumulation (one timestep only, obviously).  These results were the
same as without the save/load interruption except that the 'counts'
column in the rate files are wrong after the save/load because the
accum. time is not saved.  This does not cause any problems right now
and was noted in the 'code improvements' section.  I diff'ed the nvt
files -- they were identical except the timestamp.  (Whoohoo!)

It looks like I am ready to start adding ions to the pav=22.8 FM.

08Oct2004:

It looks like I broke the timestamp with my loader changes.  I had
made the ad.loadInstParam run only on the first time step.  This is
where the doyfr for the output file timestamp column was being loaded;
clearly, that will not work.  I moved it back outside the
'FirstTimestep' block, which seems to have fixed the problem.

===> 20Sep2004 meeting with Thomas

density:

- convert delE/q / E/q as a calculus change of variables
- try to figure this out; we did not promise densities but they are a
useful sanity check
- no one except Ruedi with USWX has produced absolute densities

error:
- sqrt(counts) only
- use 1-2 semi-minor species (e.g. C and Fe) as tracers for rest; have
one error flag
- don't get too complicated

FM:
- build coef. for hefti model before May 2000
- don't switch to all physics FM

Fe:
- go ahead with plan to make parameters specific to ion

Validation:
- look at lots of things together; look for self consistency
- get going on this as soon as possible

Sue and I will meet to define a critical path in the project tomorrow,
based on the result of this meeting.

===> 16Sep2004 developing a working FM for pav=22.8

The Problem:  Hefti did not determine parameters for all the ions
axlv2 currently analyzes.  These are H, 3He, 24Mg, and 32S.

The Options:
    1)  Develop empirical xfm parameters for missing ions.
    2)  Implement physics forward model for all ions, not just Fe.
    3)  Remove missing ions from analysis (for now).

Option 2 is clearly the best option and a pretty easy one to try.  The
problem will be testing it.

20Sep2004:

Module AxPhysicsFM has been written as a wrapper to xfmexp_phys.c
which loads all FM values into the AnalysisData::Ions array of
structures.  I implemented a FM-type input to describe which FM to
use.

Unfortunately, it turns out that xfmexp_phys didn't have the following
masses in the foil loss table:  3, 24, 28, 32.

===> 16Aug2004 debugging AceSwicsDutyCycle.cc

The interpolation worked most of the time, but every few values it
would come up with a significantly different duty cycle.  For example,
on 2004010, it would get a few .24's then throw in a 3.8, when the
aspect angle either had not changed (using default value due to out of
date ACE_ANCIL) or had changed in the thousandths.  (Aside:  many of
these outliers failed the 2% accuracy criterion, but not all, so some
were actually being used!)

I dug into the routine and found that I had not consistently handled
the unit-offset arrays for NR interpolation properly, having added
some spurious +- 1's in various places and switching back and forth
between treating them as unit and zero offset.  To be honest, I'm
surprised it worked as often as it did!

The clear evidence of this problem was that the 'locate' routines were
returning clearly wrong values (index 8 instead of 1 for del_psi=0.1).

Another aside: After fixing it to where it seemed to work, I kept
getting segmentation faults when the SpillRates object was being
created and with a reference in the back trace to an empty line just
before the constructor for AceSwicsDutyCycle.  That was particularly
weird because the SpillRates calculations were off.  Come to think of
it, one of those times, it gave a similar error with BoxRates.
Anyway, the problem was a bad index into 'ya' in the duty
interpolation, but that was not obvious from the back trace messages.

I also added comparision of the interpolated duty to the min/max of
possible values read in from the table.  If the interpolated value is
outside those limits, -1.0 is returned with an error message.  (There
should be no way to get a properly interpolated value which is outside
those limits.  That would be a sort of *extrapolation*!)

===> 29Jun2004 debugging correctFeFM_phys/xfmexp_phys.c

This setup works fine the first two times through but on the third
time it gets a really bad interpolated CFOIL energy loss (~10^13) and
crashes.  (It crashes because that leads to ridiculous values for the
next interpolation, which could certainly be fixed -- but that's a
symptom.)  The problem shows up in the first ion (s=32, Fe6+) of the
third time through correctFeFM_phys, meaning that xfmexp_phys works
fine for all ions twice, but on the third time through it has problems
on the first ion (and presumably more).

I found that it gets these bad interpolated values because the
subarray has a non-sense (~10^13) value in it.  (Hey, now the bad
interpolated value makes sense!)  The question remaining, is why is
that bad value in the subarray.

I next realized that all of the 'sublb' (sub-array lower bound) stuff
was written as 'sublb-1', which is how it is in the NR book.  However,
I'm pretty sure that refers to 0 offest arrays (starting at 0)
whereas CFOIL_EON, CFOIL, DFECT_EON, and DFECT are all unit offset
(starting at 1).  So, I replaced 'sublb-1' with 'sublb'.  The problem
still occurs, though somewhat differently.

(4 point interpolation => 4 member subarray centered on eon_index)

eon	 eon_index    sublb    subarray
11.94	 7	      6		4.13,  4.59, 5.65, 7.4, (3.64090176e+15)

Ok.  I figured out the problem!  The 'subarray' variable was set up as
unit offset.  But(!), I copied the appropriate values (from CFOIL or
DFECT) into elements 0..3 (rather than 1..4).  When I fixed that, the
problems went away.

===> 17Jun2004 verifying per period E-T slice output

Because of an odditity we saw when delivering 2001152/3 data to
Micheal Collier (08+ too high)

===> 22Jun2004 troubleshooting duty cycle

Sue noticed that a bad accuracy for the duty cycle interpolation was
being reported for *every cycle*.  I had pretty haphazardly determined
the 'good' accuracy cutoffs and there were a number of accuracies
(e.g. -0.00285764) which resulted in reasonable-looking duty cycles
(0.253753), so I changed the cutoff to be on the absolute value of the
accuracy/duty and raised the cutoff from 0.001 to 0.01.  (I also made
this a member variable with set/get function.)



===> 22Jun2004 troubleshooting efficiencies

The simply wasn't reading the file correctly and with correct file
names.  It took about 2hr. to get it working.  I spent another
2hr. verifying it against Ulysses.  On the plots, the interpolated
values exactly matched the raw data.  See swindal/devel/uswx_eff for
plots and code.

===> 05Mar2004 adding MMQ plots of assigned counts

Brainstorming options:

Size of the output data is the limiting factor, so these are ranked
largest to smallest for array in RAM and file on disk, RAM (file).
The multiplication by timestep does not apply to RAM size, since
ProbRates knows nothing of different timesteps.

Per day:
--------
3.5Gb (42Gb) - Accumulate a full MA for each species at each time step
	       70Mb x 50 species x 12 timesteps = 42Gb

10Mb/species (120Mb/species) - Accumulate a full MA for each
			     *selected* species at each *selected* EDB
			     at each time step.
1Mb x n_selected_EDBs x m_selected_species x 12 timesteps
Typically, < 10 EDBs would be selected for each species, so
1Mb x 10 EDBs x m species x 12 timesteps

50Mb (600Mb) - Accumulate MMQ summed over EDB for each species and time step
	       1Mb/MMQ x 50 species x 12 timesteps = 600Mb

1Mb (1Mb)  - Accumulate sum over all EDBs/species/time steps
	     256 x 1024 x 4 bytes = 1Mb

I think I'll go with the 50Mb option for the paper and the
10Mb/species option for later.

16Mar2004:
----------

I got a rudimentary version of this working.  Pairs of ion/edb are
registered in axlv2.cc and these are dumped out in E-T and MMQ (summed
over steps, separate for each ion).  The question is where to go next.
This would work ok for diagnostics but doesn't really do it for the
publication.

Brainstorming:

* daily or timestep sum over all ions/steps
This would most closely approximate the existing MMQ and may look
like a cleaned up one.  This would not be that difficult. 

* registerACSliceSum("O6+", edb0, edb1,ad)
Sum from edb0 through edb1.

* registerACSliceSumMax("O6+", edb_window,ad)
Find Spillover rate maximum for ion.  Sum through edb_max +/-
edb_window.  This would be most practical for seeing things in E-T
space (diag).

* registerACSum(ion)
Add ion to ACSlice which will be summed over all EDBs and registered
ions.  This matches the existing axlv2_mmq_fmc.dat, so it minimizes
problems in that way.  (The per timestep per E/q axlv2_fmcomp.dat
matches a single ion/single edb style in E-T.)

Ok, I think I have it now.  Here's the plan:  There will be two
branches, one intended for E-T and one fro MMQ.  

ACSlicesET
* registered with ion/edb pairs
* intended for low-level diagnostics of method
* no summing available
* syntax
- registerACSlice(ion, edb, ad)			*existing*
Register ion/edb pair for storage of separate AC slice.  Cost is 1.2Mb/slice.
- outputACSlices(file)
Output all slices into specified file.

ACSliceMMQ
* registered as ions to be included in sum (over ions; sum over all
EDBs also done)
* cost is 1.2Mb total.
* syntax:  
- registerACIonMMQ(ion)
Add ion to MMQ summed over ions and EDBs for timestep
- outputAC_MMQ
Output this MMQ.

===> 13Jan2004 re-fitting Fe in FM

I made long runs of axlv2_daily in 2003 and from the resulting fmcomp
plots, it is pretty clear that the Fe FM does not fit even the low
charge state Fe as well as it could.  (It's not far off, but can be
improved.)  It is hoped that getting the low states as good as
possible will improve the high states significantly.

===> 12Jan2004 using CAEN grid computing

I checked into CAEN grid computing, software which allows users to
submit jobs that run on lab computers when idle.  It won't work for
this project right now, because CAEN linux machines have not yet been
added.  In the future, it may be pretty handy. 

One drawback is that the job suspends when a user logs into the
console, which may make execution pretty slow at times.

CAEN is supposed to add it's linux computers this winter.  See 

http://www.engin.umich.edu/caen/news/spotlight/grid.html

for details.  (I also copied that to
ionstream:~jraines/misc/CAEN_grid_computing.txt.)

===> 09Jan2003 Checking into C5+/C6+ (from Ian Richardson)

To Do
-----
? fix prob. slices
  Is this really going to help since it is daily only?
  ? fix DF units and look at that?
  ? make prob. slices work in shorter periods?
o run 2000 156-9


===> 01Jan2004 Plan for 1 month of good data by 1Dec2003
[Was:  Plan for 1 month of good data September 2003.] 
[Was:  Plan for 1 month of good data in January 2003.]
[File under current date when finished.]
Plan made:       8Nov2002
Plan completed:  we can only hope by 15Sep2003.

 x Finish outputLV2.cc so results can at least be viewed

 o Tune FM to reasonable fit for important species
   x tweak species
     x O
     x C
     x Fe
     x Si
     x He(2+ only 1+ is bad)
     x H (not in xfm)
     x Mg (not in xfm)
     x S (not in xfm) -- .45 9+ and .35 8+ @ 1.2MK
     x N -- .64 5+ and .33 6+ @ 1.2MK
     x Ne -- .96 8+ @ 1.2MK
   x add another subsection plot in fmcomp for H/He
   x (optional) re-write fmcomp to use the modular stuff from fmpick
     (read_ets.pro, plot_ets.pro)
   o fine tune a little more
     o Si(again)? -- states 7-9 -- it looks pretty good now though
     o C(again) -- .12 4+, .49 5+, .39 6+ 
   o add H, 3He, Mg and S to Hefti's coef.

 x Fix densities, they are off by a few orders of magnitude or so,
   probabably a unit conversion issue
     I think this was never true; rather it was the combination of
     duty cycle and eff. that kept them low.

 x Try out Ruedi's random assigment method in Prob. rates
   Called it dice roll (DR) method.  Results similar to counts
   distributed (CD) method.

 x Verify that Ruedi's SpillRates method is what we used
   3Sep2003: Made calculation iterative when negative ghost-count
   corrected spill rates are larger that specified fraction of rest.

 - Real LV2 data product
   x finish outputLV2.cc (more complex items)
     All done by isotopic ratios.  Are these supposed to be from
     SWIMS?  Yes.
   x build IDL reader for LV2
   - build IDL plotter for LV2
     Much of this will come from the work on the Oct-Nov2003 CMEs.
     Very little extra work would be needed.

 x update input options for axlv2, in vein of axlv2_daily

 o Determine and implement optimizations
   x turn on compiler optimizations
   ? reduced EDB range for all ions, possibly specified at runtime
     This seems risky, after seeing the shifting that goes on during
     extreme events.
   ? ion-specific EDB range (this would be tough)
   ? somehow save on G and P calculation
     I do not think this will be an improvement.
   - run on a faster machine -- that's an easy one!

 x Include duty cycle
   x find angle between spacecraft spin axis and GSE X
   x find angle between bore sight vector and ACE spin axis from eng. docs
   x include ACE orbit data into axlv2
   x calculate duty cycle

 - Check high-state iron performance (up to Fe24+)
   Try these CME days in 1998:  2May, 122.173-122.841, 187

 - Include ULYSSES/SWICS efficiencies
   - write interpolation routines
   o read interpolated values into Ion structure, like FM params
   o confirm that results are sensible
   
 - Validate results
   - track C6+/C5+ together with O7+/O6+
     O7+/O6+ freeze-in temp. is 1.15-1.2 * C6+/C5+ freeze-in temp.
   - try Thomas' suggestion of watching spill rates for isolated Fe
     species
   - rate spectra: each (box, spill, prob) should be smooth, continous
     and with peaks moving to lower energy with increasing charge state

 x Finish hourly axlv2
   Moved daily-oriented code to axlv2_daily.

 x Make a runner perl program for this data that smartly restarts
   after failures

 o Run code for one month of data -- approx. 2 days required (on hrdi!)

Items to do for final release:
------------------------------
 o Include calibration data -- efficiencies
    o assess current state of our knowledge 
    o find incident beam for all files
    o turn the crank and find efficiency curves for each ion
    o fit eff. curves to functions
    o incorporate this into DistFunc

 o Run code for all days

===> 25Nov2003 FM correction for Fe 

During the Oct-Nov2003 CME analysis, we noticed that the FM did not
fit high state Fe (17-24) well.  As a quick measure, I've decided to
make a correction function by fitting a line to a few points (by
eye).




===> 20Nov2003 re-work fmcomp again to fix and separate prob centers 

50% filter: axlv2/runs/20Nov2003-fixing-fmcomp/1-baseline
0% filter: axlv2/runs/05Nov2003-Fe-to-24/5-d-fmcomp-2003302

Done.  

The separation was not a big improvement, in fact, it was probably
more trouble than it was worth, except for getting a fast turn around
on FM comparision, since it now works like axfmtweak and prints out
the fmcomp data right after MA accumulation.

Fixing the prob. center comparison (pcc) *was* a big improvement.  It
turned out to be that I had not gotten the zeroing of the
prob. centers denominator right.  I had copied it from the CD method,
which has tofch/esdch as the inner loops.  By contrast, DR and CM
methods have species as the inner loops.  This zeroing was easy to fix.

===> 13Nov2003 added velocity filter and re-worked Pha class slightly

As part of my Oct-Nov2003 CME analyis, we found we needed a velocity
filter during CMEs to get rid of noise, masked as low charge iron
(6-8).

I added the filter by adding a speed and swspeed member to each Pha.
(I also removed the now-obsolete hspeed ans asp.)  Both are caclulated
by AceSwicsData::load.  The swspeed member is calculated by a
count-weighted average of significant steps (frac. of max) of the He2+
matrix rate.  The speed member is calculated using the kinetic energy
per charge equation, using the E/q and m/q of the PHA itself.

Of course, it did not work that simply!

Troubleshooting log:
[All subdir. of /home/jraines/ace/axlv2/runs/11Nov2003-vel-filter/.]

7-2002099:  quiet day yet still significant filtering.  This confirms
that it is not working properly.

- swspeed is good when printed out (per cycle) from the function
output. It is very similary to the He2+ speed which comes out of end
of axlv2.

8-no-filter: MA used about 91%, with a few 85% and a 50ish%.  The plot
axlv2_fmcomp.ps shows heavies *very* shifted into high energy.  Also,
several streaks in tofch at esdch = 0.  (Streak explanation TBD later.)

1-daily(previous overwritten): re-ran with static cycle counter to try
to get a handle on when the swspeed is 0.0 in PHAs since it is good
when printed to stdout.  Did not work.  Variable ncycle is used to
stay within AnalysisInterval.  Made a new variable, ncycle_file

I found a mistake in MeasurementArray::passVelFilter; only ions with
speeds (1.0+velFilterFrac)*swspeed would pass.  This *clearly* led to
some of the problems.

I got a decent look at the pha speeds and swspeeds after I put in the
static ncycle_file, since I could then plot them by cycle
(~/ace/axlv2/devel/tools/pha_dump.pro) to see evolution over time.
The swspeed looks great.  The speed is all over the place, but that
may be the way it is.  I tried to compare to 7-2002099, but I have to
re-run that with the new ncycle.

I re-ran the processing and put it into 9-daily-gt2lt.

Of note, when mqch is an overflow channel,> 124, the value stored is
mq=mqch*10.0.  This will result in abnormally low speeds (down by 32%
over mq=mqch).  This might account for some of the low speeds in the
plots.  I don't know if another way of handling this is any better,
but it definitely makes some sort of PHA filtering (range, vel.) a
necessity.  

Alternatively, I could a) just pass the channel through or
b) not store that PHA (i.e. filter in load()).  I'm not sure any of
these options are better.

Analsysi To Do:
-----
x re-run 2003302 (9-daily-gt2lt) to see change after > to < change.
  Fe states bar graph looks better.  
x re-run 7-2002099 to see if *less* speeds are crazy.
  Plotting pha_dump.txt is paradoxical.  There still seem to be a very
  large number of PHAs outside of the velocity filter.  Duty cycle
  seems much improved.
x re-run hourly (a-hourly-new-filter) -- we can only hope!

18Nov2003:
----------

I made a bunch of different runs and one change and finally am
convinced that I have a working velocity filter.  

The first  thing I
changed was to store -1.0 in pha.speed when 0 < mq <= 124.0 is not
true.  Values of mq outside this range are not physical m/q values so
do not correspond to a speed.  This helped get rid of some of the crap.

I also changed MeasurementArray::fill to output used
(mafill_used_pha.dat) and discarded (mafill_discarded_pha.dat) PHA
words.  I plotted them with ~/ace/axlv2/devel/tools/mafill_pha.pro for
a particular run,
axlv2/runs/17Nov2003-improving-Fe/1-gauss-cutoff-1.0.

The last plot convinced me that it is working since it shows 

a) used PHA speed exactly following the vsw +/- vsw*filter 
b) most discarded PHA are outside this vel. range.  I'm guessing the
exceptions are range 3 (which should be excluded) or with bad EDBs
(the other discard criteria).

Now, the issue seems to be making the whole code behave better using
this velocity filter.

===> 09Nov2003 merged xfmexp and xfm

I merged them and keyed the coef. off a3gxpavdpu(127) or (171)
levels.  I tested it by running all of 2001 in 

/home/jraines/ace/axlv2/runs/09Nov2003-xfmexp-merged-with-xfm/1-lv171

The forward model lined up as before.

10Nov2003:
----------
However, I found out that pre-paps ramp up days (before May 2000) did
not work correctly.  I realized that this was because I was keying it
off the wrong pav (20.5).  I had pulled the 1998 pav out of the old axlv2
1998001-1998365 run, but I didn't realize that the pav level was 120
(not 127!) on doy 1998001-2. 

I fixed this by changing the values which libhef::xfm uses to
determine which set of coef. to use and changing pav correction equation.

Of note, though, the program failed out at SpillRates due to a
Numerical Recipes error.  I'll need to fix that!

===> 07Nov2003 quickly adding Fe17+ -> Fe24+

For doy 302/3, I quickly added more Fe states.  axlv2_daily
seg. faulted at edb=9 on ion=3 during output of P slices.

I did the following to try to get it to work:

- increased the precision from 3 to 5

9Nov2003:
---------

I tried a bunch of tricks but none of them worked.  I finally realized
that outputPslices in ProbRates.cc was not being turned off properly.
I hardcoded it to false which kept it out of the the ouputSlice code
and fixed the seg. fault.

I'll need to fix this for real later.

===> 09Oct2003 building in efficiencies

For now, we will use ULYSSES/SWICS efficiencies from Ruedi von
Steiger, since the ACE/SWICS eff. are not yet done.

Brainstorming:

- the A array should be output before and after?
- will that be intelligible?
- three ideas for importing eff:
  1) interpolate points from file on the fly
  2) fit to function and pull eff. out of function
  3) interpolate offline to exact E/q

- eff array should be stored with each ion in an Ion object.
That means it can be loaded like the forward model, with the code
residing in an AceSwicsData object.

28Oct2003
---------

Now that the duty cycle is (finally!) nailed down, I'm ready to get
back to this.  Having just had a good experience with interpolation,
I'm going to try idea (1) first.  It should be easy to implement.

01Dec2003
---------

This got tabled for several months but I'm back at it now in
conjunction with the Oct-Nov2003 CME analysis.



===> 18Sep2003 building AceSwicsDutyCycle

After spending 2 mornings figuring out what to do with the duty cycle
formulas and one morning coding it, I've spent at least one trying to
get it to compile/link.  Current problems are:

 - qsimp does not exist in the Numerical Recipes library that I got
from CAEN.  What's up with that?!

   So, I need to pick another integrator.  The function qtrap works,
but the NR book makes it sound like a bad choice.

 - I cannot get qtrap to work with an integrand function which is a
member of AceSwicsDutyCycle.  It works with a test global function
just fine.  The problem is that qtrap expects a pointer to the
function.  In C++, a pointer to a member function has that class as
part of it's type.  The NR function qtrap does not like this and
casting it to (float (*) (float)) as the error message says, does not
work.  I could get around this by either buying NR for C++ or just
making the integrand a global function.

 - I need to write ancillary functions for the HSE coord. system.
This should not be a problem.

 - I cannot get proper linkage when I include the acetest::ancillary
code.  I probably need to copy all of the libraries to
/shrg1/local/lib to simplify things.  Once I add the proper HDF
directories and linkages, I get a conflict between netcdf functions and
HDF functions.  I wonder if there is a way to use namespaces to solve
this problem or possibly the order of linking?

19Sep2003:
----------

I solved the linking problem but not the overall problem.  I get the
same error that Jessica and I were getting when trying to build the
O/A stuff into libhef:

/shrg1/local/lib/libnetcdf.a(nc.o): In function `NC_check_id':
nc.o(.text+0x6c): multiple definition of `NC_check_id'
/shrg1/local/hdf/lib/libmfhdf.a(file.o)(.text+0x0): first defined here
/usr/bin/ld: Warning: size of symbol `NC_check_id' changed from 103 to 60 in nc.o
/shrg1/local/lib/libnetcdf.a(var.o): In function `NC_var_shape':
var.o(.text+0x4f8): multiple definition of `NC_var_shape'
/shrg1/local/hdf/lib/libmfhdf.a(var.o)(.text+0x24c): first defined here
/usr/bin/ld: Warning: size of symbol `NC_var_shape' changed from 797 to 267 in var.o
/shrg1/local/lib/libnetcdf.a(attr.o): In function `NC_findattr':
attr.o(.text+0x388): multiple definition of `NC_findattr'
/shrg1/local/hdf/lib/libmfhdf.a(attr.o)(.text+0x214): first defined here
/usr/bin/ld: Warning: size of symbol `NC_findattr' changed from 163 to 131 in attr.o
collect2: ld returned 1 exit status

I don't think there is any way around this, unless I were to modify
one of the codes, acetest/anciltest or HDF, which I'm not going to do.

Brainstorming Options:

1.  Keep looking for a work around with the existing set up.  This
will likely burn a lot of time (days) and is not that likely to find a
solution.  This is probably why Simon did HDF->netCDF conversion in
two steps(!), so it is unlikely that there is any easy solution.

2.  Build a stand-alone program which takes the cycle time and returns
either the aspect angle or the two angle cosines needed to find the
aspect angle.

a3gaspect cycle_time

3.  Build something similary to (2) but more general, a program which
would accept arguments either of the form

3a.
a3goa 'att_GSE_x' cycle_time
a3goa 'pos_HSEb_y' cycle_time
a3goa 'vel_GSE_z' cycle_time

or

3b.
a3goa 'att' 'GSE' 'x' cycle_time
a3goa 'pos' 'HSEb 'y' cycle_time
a3goa 'vel' 'GSE' 'z' cycle_time

The former seems better since the program would just have to assemble
the arguments in the latter case to form a full name. 

I decided on option (2) for now.  I may do (3a) later for more general
use.

22Sep2003:
----------

After finishing much of the duty cycle coding, I ran into a problem:
The integrand is always zero!  This is because terms one and two are
always equal (1.0) and thus subtract to 0.

Units!  I realized I was defining the angles in degrees rather than
radians.  After fixing that, the duty cycle changed to a non-zero, but
very small number.  For 

alpha = 3.06 deg.
del_psi = 0.01

duty cycle = 1.4E-3

It should be about 1.0E2 - 2.0E2 times that value.  I'm guessing that
some of the angles are wrong, though I have not tried changing many of
them.  Increasing gamma to the ULY/SWICS level of 57.0 deg. raises the
duty cycle to 2.1E-3, still about 10^2 low.

I've decided to clean up the coding problems associated with this and
finish the aspect angle calc. before coming back to this.

30Sep2003:
----------

I got some new ideas from Thomas (after being kept away from this work
for several days).  His suggestions:

1)  Add Pi to theta_c_0 and/or phi_c_0 on the grounds that they could be
off by a factor of due to the arc cosine and arc sine.

before messing with angles -- duty = 0.0106815
theta_c_0 + M_PI -- integrand all -0
phi_c_0 + M_PI -- little change, duty = 0.0109105 
theta_c_0 + M_PI and phi_c_0 + M_PI -- integrand all -0

Oops!  Those were actually for the ULYSSES gamma (57).  Here goes
again with ACE gamma (34.5):

before messing with angles -- duty = 0.00731251
theta_c_0 + M_PI -- integrand all -0
phi_c_0 + M_PI -- little change, duty = 0.0073843 
theta_c_0 + M_PI and phi_c_0 + M_PI -- integrand all -0

Well, these results are basically the same as before.

Ok, I will focus on reproducing the ULYSSES results and then switch to
ACE angles.

So, for gamma = 57.0 deg, del_phi_c = 69.0, del_theta_c = 4.0,
duty = 0.0100439.

(With ACE angles, gamma = 34.5 deg, del_phi_c = 70.0, del_theta_c = 4.2,
duty = 0.00731251.)

So why am I not getting Ruedi's results?  According to his plot, I
should be getting about .28 for alpha = 3.0 and del_psi = 0.1.

I tried the calculation by hand (see 11Sep2003 hardcopy notes) and
found that I got 0.0 when duty_exp.cc gave 0.00953778.  I'm guessing
the problem there is round-off error and it makes me wonder if that is
the bigger problem with the duty cycle, at least for intermediate
results.  So, I'm switching everything to double and trying again.

duty = 0.0100439 -- NO CHANGE!

I still think I might be on the right track with this.

26Sep2003: (written on 3Oct2003)
----------

I looked into Nathan's code, DUTY.f and the generator, DUT1.pro.
(They are in ~/ulysses/libnat/swx98 and the idl.dir sub. resp.)
NATHAN IS NOT USING THE SAME FUNCTION AS IN HIS PAPER.  This section
in Ruedi's paper is a verbatim copy of Nathan's (written by Ruedi
according to Thomas), so I sent mail to Ruedi asking if he used
exactly that equation.  He is out of town until 6Oct, so I'm pushing
on.

Items of note from Nathan's code:

 - the equation used may be a normalized version of the published one
(I have not yet decoded it so I'm not sure.)

 - the numerical integration is done once in the IDL code (DUT1.pro)
 then written to an ascii data file.  The fortran code reads in this
 file and interpolates for alpha.  I'm not sure why this was done
 since the integration goes really fast and only has to be done once
 per time period.  Nathan used long accumulations (i.e. few, long,
 time periods) so I don't know why this would be an issue.

 - I could probably get the fortran routine to work as a stand-alone
 program to keep pushing ahead on this with at least a reasonable --
 if cryptic -- duty cycle.

 - The NR routine QROMO is used for the numerical integration.  The NR
 book states that this is for poorly behaved functions, but the duty
 cycle does not seem to be this way.  I wonder why he used it then?
 Maybe he was just used to it.

3Oct2003:
---------

I got get_ace_aspect and libace_ancil working as a shared library
yesterday.  And, I figured out which angle to use, HSEb x attitude,
after Thomas reminded me that the angle of ACE z to the solar wind is
the critical angle.

6Oct2003:
---------
While waiting for Ruedi to let me know about the proper duty cycle
function (he returns today), I need to do the following:

 x Decide whether to make integrand a member function or not and
finish the coding either way.  Made it a static member function and
the problem was solved.  (After much annoyance and needing to change
MachAngle and AspectAngle to static as well.)

 x Commit libace_ancil to CVS even though the perl stuff does not
work.  (Add a README disclaimer.)

 x Build in a 'system()' call to get_ace_aspect -- Ha!  I didn't need to do
this!  I was able to call the libace_ancil routines that
get_ace_aspect calls directly from AceSwicsDutyCycle.cc by simply
including the .h file in a namespace.  Wow, and after all that work.

9Oct2003:
---------

Current loose ends:

 - duty cycle is not yet used in DistFunc and there is no clear way to
   get the cycle time from an AceSwicsData object to AceSwicsDutyCycle
   object

 - the correct duty cycle function is unclear

21Oct2003:
 
Ruedi confirmed that he did not use the duty cycle expression in his
JGR paper.  Rather, he did the triple integral from his
Habilitation Thesis (also in the form of a memo he sent me).  The
entire triple integral was done numerically.

I sent him p88-9 of the SWICS SIIS.  He confirmed that ACE/SWICS is
mounted exactly like ULYSSES/SWICS and that the duty cycle should be
directly applied.

I still have a few questions:

- The ACE angles are slightly different.  Does that matter?

- Why does the gamma angle matter at all?  Why not just set it to zero
and integrate in theta from 0 - del_theta_c?  Ok.  I looked at a
drawing of ULYSSES and can see SWICS on it.  That led me to find a
real picture (of ACE/SWICS but they are the same) and I think I see it
now:  gamma is a particular angle of rotation around the theat_c axis.
It matters because the collimator is not mounted perpindicular to the
deck, but rather at an angle of ... 57 deg. (gamma!).

- 

27Oct2003:
----------

I sent Ruedi a picture of SWICS on the ACE deck (ace_sc2.gif) just to
confirm.  He again assured me that SWICS is mounted the same on
ACE as it is on ULYSSES.  So, I'm taking his word for it and using his
tabulated duty cycle.  I may re-run his code later with the very
slightly different ACE/SWICS angles.  For now, though, I want to get
off this.

I made the 2D polynomial interpolation work (after a day of hoop
jumping) in an experiment, 2D_interpol.cc (was bilinear_interpol.cc).
I'll extract that code an put it directly into the duty cycle class.

28Oct2003:
----------
[Relevant dir. parent: ~/ace/axlv2/runs/10Sep2003-adding-duty-cycle]

The interpolation code has been put into AceSwicsDutyCycle and all
connections made.  And, it actually works!  I spot checked a few densities (in
axlv2_nvt...).  They have increased by about 4, which follows from a
duty cycle value of about .25 in the denominator.  The speeds and
thermal speeds are identical.  Also, all values reported in
axlv2_lv2... are identical.  So far, so good.

I also re-verified that running from saved prob. rates works.  I
compared output in 4-interpol dir. with that in 5-full-run.  These are
the same runs but the prob. rates are loaded in 4 and calc. in 5.  The
files were the same except for the creation date.  (Woohoo!)

Of note, I'm not sure I set up the way that duty cycle is passed to
DistFunc correctly.  Here's the flow:

---
AnalysisData calls AceSwicsData calls AceSwicsDutyCycle with current
CycleTime

resultant duty is loaded into AnalysisData.DutyCycle

AnalysisData is passed to DistFunc (already was for ions)

DistFunc uses AnalysisData::getDutyCycle() for value
---

This works fine but there are several problems:  1)  It is unclear how
to set options in AceSwicsDutyCycle.  Functions are provided by they
are not visible to the top level program.  2)  There are a lot of
dummy functions that just call other ones, making it confusing.

I think I need to improve this, but I'll think on it awhile first, and
make some other progress.

13Nov2003:
----------
As part of my Oct-Nov2003 CME analysis, I noticed that the duty cycle
was varying wildly sometimes, and with several different days.

I looked into Numerical Recipes and was reminded that interpolations
can sometimes be ill behaved and that I should take a look at the
error.

I found that (in the test day anyway), all the bad duty cycles were
paired with accuracies that were outside of 0.0 < accuracy <= 0.001.
So, I altered calcByInterpolation to return -1.0 when the accuracy was
outside this range.  I then altered AnalysisData::loadInstParam to
keep the current duty if the calc. returned <= 0.0.  As an extra
fail-safe, I made it set the duty to 1.0 in case the previous duty was
also bad.  And, I changed the init. of duty to 1.0, since that is the
safest value.

===> 09Sep2003 finishing outputLV2 and assoc. IDL

I'm not sure how to do the isotopic ratios.  I don't have those
isotopes in the analysis and I cannot image them being good if I put them
in.  My guess is that they were intended to be from SWIMS.  

21Oct2003:

The isotopic ratios are intended to be from SWIMS.  I do not know if
we can release this LV2 data without them.  But I hope we can.

===> 08Sep2003 brainstorming validation methods

Possible validation/improvement branches:

1) Experiment with more spill rates iterations by lowering the cutoff
very low.

2) Try running on different days (2002099) is the only day right now.

2a) Try running on larger accumulations.  An easy one would be the
2001 block used for FM tweaking.

3) Carefully examine P slices and, in particular, species which don't
seem to be working.

4) In a P slices, what does it mean when two species are both in the
red zones?  Does that mean they have a similarly high probability of
being selected?

 - Spillover correction to box rates does not really iterate out
overlap, only negative solutions.  Presumably, it could be made to
iterate out overlap if calculated from something like the last
iterations prob. rates.


Validation status:

 - box rate plots look good.  Spectra are mostly continous and fairly
smooth.  Peak E/q decreases with increasing charge and smoothly.

 - spill rate plots are much improved.  The spectra look pretty good,
but they are less so:  less continuous, less smooth and with some
species, S and Mg, looking pretty broken up.

 - prob rate plots are very similar to spill rate plots.

Ideas:

 - make number of sigmas in spill rates mass-dependent; tweak it down
for highly overlapped species

===> 04Sep2003 cleaning up fmcomp code

I updated fmcomp.pro to use the modular code developed for
fmpick.pro.  (This was split out from fmcomp originally.)  This way,
there is only one line of development for reading, plotting and limit
checking of E-T slices (fmcom).

Of note, plot_p_slices uses slightly adapted versions of these
subroutines to do it's work.

===> 28Aug2003 making spill rates more iterative

Directory:  axlv2/runs/28Aug2003-iterate-SR/
Running mostly on ydoy 2002099 (used to compare with SWEPAM data)

I built outputSR.cc to see the spillrates with axlv2_rates.pro.  This
worked great, but the rates look pretty bad:

1)  There is a high background everywhere, about 10^-4.  There is no
background in the other rates.

2)  The peaks are very rough, with black (0.0) squares interspersed
and intensity changing by several orders of magnitude (red -> green)
in adjacent squares then back again in the next square.

29Aug2003:
----------
I looked at the sr file more closely and found out a few things that
make more sense than yesterday:

1) The background is not present in the data file:  There are lots of
zeros instead.

2) There are some negative rates -- these may be messing up the
scaling, making it appear as though there is a non-zero background.

3) There are also many very small numbers, 10^-12 ocassionally 10^-17,
which leads to a large dynamic range.  This may also be causing
problems in the plots.

Calculation of Btil[s]:  I was worried that I had made a math error in
this calculation.  

  for (int s = 0; s < grp.size(); s++){
    for (int t = 0; t < grp.size(); t++){
      if (t != s){
	// derived from equation 7
	Btil[s] += - Sinv[s][t] * B[t]
	  / Sinv[s][s];
      }
    } // loop over species t

My concern was with Sinv[s][s] in the denominator since the equation
actually calls for the sum in the numerator to be divided by
Sinv[s][s].  But, since Sinv[s][s] does not depend on t, it factors
out of the above and results in the same equation.

So, the implemented calc. is fine.

Implementing Ruedi's iteration method:
--------------------------------------

It took me a long while to decipher the spill over rates calculation
and the ghost count removal because they are poorly documented both in
the code and in Reudi/Nathan's papers.

I finally realized that I was marking bad ions that had negative spill
rates, NspillC2, instead of negative ghost-corrected spill rates,
NspilltillC2.  This cannot be right because the mathematical method of
calculating Btil is supposed to take care of this problem.  I changed
SpillRates::calc so that *it* checked to see if spill rates
(ghost-corrected) were bad *and* more than 0.01 of the sum of the
positive rates.  If a species at a particular edb was bad by these
criteria, then it was ?.  I re-ran the code (2/);  The rates were
still not great but substantially improved.  

Next up is to make the method truly iterative so that the calculation
iterates until no ghost-corr. spill rates, , are more than 0.01 of the
sum.

08Sep2003:
----------

Done (last week) and it works fine.  The only kicker is that even with
the threshold low (0.005) I don't get more than one iteration.  I'm
sure I could force multiple iterations but I have not yet tried to do that.

===> 25Aug2003 troubleshooting prob rates

Even after fixing the forward model and switching to Ruedi's dice roll
prob. method, the prob. rates still don't look very good compared to
the boxrates.  Characteristics of good rates, from the perspective of
axlv2_rates.pro plot:

 - Smooth, continuous peaks for each ion; few isolated color blocks
 - Smooth shift of the max. of higher charge states to higher steps

Right now for the prob. rates (DR method) the Fe looks pretty good by
the above criteria, as does the 4He2+.  But, the peaks are much less
smooth, with lots of large color changes or white holes interspersed.
And, most of Si and S look *very* spotty, with very few charge states,
only Si9/10+ and S10+, looking much like peaks at all.

As a diagnostic method, I wrote out the P slice (tofch,esdch matrix)
for each species at the EDB with the max. counts from axlv2_rates.pro.



===> 20Aug2003 Mg boxrates erroniously 0

There are several symptoms of this problem when examining data from
axlv2_daily:  
1)  The Mg boxrates are identically 0.
2)  The forward model stof are -tof.
3)  The fmcomp plot has solid, horiz. lines across the entire plot

In looking into this, I found out (or was reminded):

1) I changed the *_fmcomp.dat format in axfmtweak so that there are FM
centers/sigmas for Hefti FM and the newly revised FM.  The new FM
stof's are -tof just as they are from axlv2_daily.  So, the problem is
not new.  The stof values were wrong even back when I fit Mg with
axfmtweak.

2) The plotter fmcomp uses one column of sigma tof's -- those that
were assoc. with the FM in the original axlv2 format.  That means that
during fitting the FM I wouldn't have noticed the problem with the new
stof's because the orig. FM stof's were being used with the new (good)
tof's.

3) When doing FM tweaking, I had turned off plotting of the orig. FM
in fmcomp.  Around line 299 I had replaced

        then doPlot = 1 else doPlot = 0

with

        then doPlot = 0 else doPlot = 0

which kept it from plotting orig. FM stuff.

So, the reason the fmcomp plots have changed is that the new FM
tof/stof are now in the place of the orig. FM tof/stof during
axfmtweak fitting.  These new FM stof's are bad and so the plots are bad.

So, these facts explain why the fmcomp plot changed.  But why are the
stof's bad?

Geez!  The problem went away, though I can only surmise what it was:  

I noticed a test target being run when run-axfmtweak.sh tried to do
the make.  And, a recent change to xfmexp.cc did not take, so I looked
at the axfmtweak makefile.  When I changed the axfmtweak makefile
around (to allow building from the $ACESW/src tree), I made a
different default target.  This broken the automated making.  Now, the
Mg stof's were likely bad because I forgot to add the H2[24] at some
point (though it was in
axfmtweak/runs/05Aug2003-Mg-6-insert/xfmexp.ps) and it was left at the
default value of -1.0.

The only curiosity is *exactly* why that change didn't take, but I
won't worry about it unless it happens again.


===> 12Aug2003 adding Ruedi's dice roll method of prob. assignment

I put the code for this assignment method into ProbRatesDevel.cc, so
that the other code could still be used for troubleshooting the
density.

As a test of the method, I put the same algorithm into
axlv2/devel/dice_roll_method/prob_range.cc.  I wrote a quick IDL
program to analyze the output, prob_range.pro, and stored the plots in
prob_range_data.ps.  

I ran the program for 10000 iterations with 3 species.  The
distribution of random numbers (from random()) seems pretty uniform.
And, the distribution of species chosen was exactly the same as the
probability for each species.  This was not an extensive test, but I
think it shows that the method works in principle.

That said, I should really build similar output into ProbRates::calc
so that I can track these distributions for the real probabilities.
And, I should also put in some switches that toggle the various
prob. assignment methods, since they are pretty costly computation-wise.

22Aug2003:
----------

After getting distracted by the Mg problem and various changes, I just
got to look at the DR method rate plots (from axlv2_rates.pro)
yesterday.  They were terrible!  There were very few counts -- the
array was almost blank -- and they were distributed strangely.  Come
to find out I had dropped the '+' when adding to Nprob[s][nedb], so
that the only counts in there for a given species were those from the
last (tof,esd) won.  Obviously that cannot be right.

I added the '+' and the rates came out *very* similar to the CD
method, though careful inspection reveals some differences.  I diff'ed
the data files.  The differences seem to be -- upon cursory inspection
-- in the overlapped regions, which would make sense.

===> 12Aug2003 checking on densities

A back of the envelope calculation shows that the natural units
plugged into the Ai term for density divide out to 9.65E5, when
conversion factors are applied to get all units to cancel out.  (This
term should be dimension-less, so that part works.)

I'll have to do this again more carefully to see if that factor should
really be applied or if part of it is already in there.

19Aug2003:
----------

Upon closer inspection, I notice a factor of 9.648e4 in the velocity
calculation in DistFunc.cc.  And, I double checked this factor with my
reference book.  It is very probably correct.

I compared with swepam data daily averages and found a different
problem:  I don't have H+ densities (don't know why) and they don't
have 4He densities.  So I used their 4He/H ratio to calc. approx. 4He
density and came out with the axlv2 density being 3 times *too high*
(instead of low as I thought):

For 2002099 (swepam does not have data on 2002100),

4He/H = 0.0494
density(H) = 5.462

--> 5.462*.0494 = .2628

axlv2 density for 4He = .8258

I wonder if they just don't know the 4He density that well?

===> 11Aug2003 improving Ne in XFM

.96 8+ @ 1.2MK

Ne8+ looks pretty good in steps 21-36, though only 19-24 were used for
pick.  Of note, I cannot see a peak for Ne7+, but it should only be 5%
of the Ne8+ counts.


===> 11Aug2003 improving N in XFM

.64 5+ and .33 6+ @ 1.2MK

Used N5+ picked over steps 30-35 (plots 85-100).  Final alignment good
in steps used but seemingly high (in ESD) at lower steps (2-insert).  

I then realized that higher steps showed the N5+ peak so I re-ran the
pick (3-insert) over steps 30-47 and got a better alignment throughout.


===> 11Aug2003 adding S to XFM

.45 9+ and .35 8+ @ 1.2MK

See /home/jraines/ace/axfmtweak/runs/11Aug2003-S for data and plots.

Suprisingly, H1 copied from () looks pretty good.  I used 9+ state
since it is 45% of the total.  The final alignment looks pretty good.
===> 05Aug2003 adding Mg to XFM

All directories relative to ~jraines/ace/axfmtweak/runs/05Aug2003-Mg/.

In order to add Mg, I need to put it into the ions list in axfmtweak.cc.

This worked without difficulty.

The difficulty was that Mg10+ strongly overlaps Si11/12+ and 20Ne8+
(at least in it's current FM position) making it
difficult to pick the peak.  After a large amount of trial and error,
I discovered that through a fairly narrow step range, 30-46, a peak at
the FM-indicated TOF channel for Mg10+ seems to increase while the
overlapping peaks decrease.  I used this knowledge to pick the peak
over that step range.  In order to actually pick this peak center, I
had to imagine an oval peak, only loosely defined in the plot, and pick
the center of that.

In the end, I got a decent alignment with what I think is the Mg10+
peak.  I plan to go with this as Mg for now with hopes of confirming
it through another method (rates/speeds/?) later.

I did realize that I removed a useful functionality in fmpick: the
ability to pick an assortment of different charge states and steps for
a particular element.  I may need to put that back, or maybe I should
put it back while I still remember how to do it.

===> 30Jul2003 adding TOF to fmpick and fitting H+

4Aug2003:
---------

Adding TOF to fmpick proved easy -- figuring out what to add was not
so easy.  I could not set H1 to 1.0 because of it's size, O(10^6); that
through off TOF so far as to break the tools. I ended up setting it to
1.0E6 and split H1 into two constants, the first being the 1.0E6 that
I used when picking and the second being the results of the pick.
(See hardcopy notes.)  It worked out so that 

TOFpick = TOFdpu*sqrt(1.0E6)*sqrt(H1pp)

then sqrt(H1pp) (H1 double prime) is determined from a fit to the
equation above.

To complicate matters further, I could not find an H1pp that really
fit the data without an offset term (see insert-H/).  Adding a term to
the FM is being avoided at this point.  But, the following did result
in a good alignment:

H1p^2 = 1000000.0 (H1 set to this in FM during fmpick)
H1pp = 1.44      (calc. from linear fit)
H1 = H1pp^2*H1p^2 = 1440000.0
offset = 4.7 channels

See insert-H-add-const/ for plots.

My only guess as to why this would be necessary is that the foil loss
term was wrong.  I know that it is wrong since I copied from He, so
this seemed logical.  

I talked to several people about how to find it and got a range of
answers:

1) Pat:  Try pumping a good TOF spectrum through SRIM and play with
the foil thickness to fit the data.

2) Me:  Use UMD's cfoil data, averaged over energy, to first back
calc. foil thickness from one of the other UL terms then forward
calc. H with this thickness.

3) Thomas/Pat: Look into Oetliker or Gonin's thesis.  Gonin had one
figure which described this for 1H only.  Thomas is pretty sure that
the foil is 2.0ug/cm^2.  In Gonin's thesis, paper 2, page 3, if figure
3 is extended out to 26kV, it implies that the loss should b

4) Thomas/Me: Set H1 to that of He and fit UL.


From the discussion with Thomas, we came to the following conclusions:
a) UL for H  should be less than for He
b) it should depend on incident energy somewhat -- this implies that
it could be a function of post-accel. voltage(!)

Don't forget, H1 at least represents distance and unit conversion and
possibly the combination of other uncertainties.

According to Funsten, et. al., "Low-energy neutral-atom imaging
techniques for remote observations of the magnetosphere", 1995, the
loss in the foil for proton should be < 5%.

--- aside: foils in ug/cm^2
Pat believes this to be a column integration, transverse to the area
of the foil.  As such, it represents both thickness and density,
though it is technically neither. 
---

Fitting to find UL:
-------------------

This turned out to be relatively easy.  I simply got all the energy
terms on one side

(m/q)*H1(m)   = (E/q) + Up - UL(m)
-----------
   TOF^2

and fit E/q (indep.) to 1/TOF^2 (dep)

     C        = m(E/q) + b
-----------
   TOF^2
    
then found UL from

UL(m) = Up - b

This worked pretty well, giving a UL(1) of about .5.  (It varied some
with my peak picks.)  Notably, the slope (m) which should come out to
be 1.0 if following the FM came out to be about .92.  When I put that
multiplier on E/q into xfmexp.cc, I got a little better TOF fit.
Thomas and I agreed not to change the character of the FM, though, so
I took it back out.

Overall, the H+ fit is pretty good.

===> 30Jul2003 updating He in FM

Directories:  axlv2/runs/29Jul2003-2001-all-ranges
	      axfmtweak/runs/20Jul2003-He

[I'm trying to figure out how I was doing this.]

I examined the output of fmfit.pro,
fmfit.axfmtweak_find_He_coef_2001001_2001365.ps, the fit was terrible
with the step range I gave, 6-48.

I found the following step ranges had good fits: 12-18, 21-31, 34-46.

I ran fmfit again with steps 34-46.  All the resulting fits were good
and the line looked great.

The fmcomp plots also showed pretty good alignment.  The FM position
seems to be shifted to a little higher tofch than the peak center, but
only by a small amount (maybe only a few channels).

I'm calling it good enough for now.

Note:  4He1+ looks pretty bad in energy.  There does not seem to be
anyway to correct this with the forward model we use.  The dependence
on charge (alone) is not parameterized.  Or, at least I don't see how
it is.

31Jul2003:
----------

Oops.  I just realized that I was supposed to set H3/4 to 1.0 for a
proper fit.  I guess I'll try that now.

The coef. changed
	  H3   H4
was	 0.40 0.830
now      2.53 0.71

BUT THE FIT IN FMCOMP SEEMS THE SAME.  Go figure.

Since this last result is curious and since the fit is a little low in
energy, I'll try fmpick.

===> 30Jul2003 updating Si in FM

Hefti used states 7-9+.  A careful look at Si alignment from
axfmtweak/runs/09Jul2003-boxrates-for-tz, shows that it aligns pretty
well.  The max. for Si7/8+ is at about step 27 (plot 76).  The FM
there fits very well for Si7+ and a is little high in E for Si8+.  The
fit where Si is small, step 13 (plot 33), is basically just as good.
As the size of the peaks come up, paging the fmcomp plot into
increasing step, the Si fits are still good.

I think this is good enough for now.

===> 29Jul2003 backup to updating FM, starting with an all range, 2001 run 

Ok, I started by looking at axlv2/runs/5Nov2002-long-2001-allrng but
the fmcomp results (in axfmtweak/runs/29Jul2003-2001-all-ranges) were
weird: Only two plots were made (of the 3*58 usual) and the FM peaks
were all (tof,esd)=(600-800, 0-50).  I immediately re-ran the long run
in axlv2/runs/29Jul2003-2001-all-ranges.

30Jul2003:
----------

I ran axfmtweak in the morning
(axfmtweak/runs/30Jul2003-2001-all-ranges-again).  The fmcomp results
*still* looked weird so troubleshooting runs with both axlv2_daily and
axfmtweak were made.  Eventually I discovered the problem to be the
specification of an outdated directory (/shrg1/acedata) for the LV1
file from which the eqtab is read.  I fixed this and the problems went
away.

On looking at the fmcomp plot, I was reminded that the range 0 ions
(H/He) really make it hard to resolve the peaks for the non-iron heavy
ions.  This is because fmcomp plots them on the same plot.  So, either
fmcomp must be modified to plot an additional H/He range *or* the FM
tweaking must be done without range 0 ions in the mix.

To try to make some progress on the FM, I'll do the latter for now and
modify fmcomp later.

31Jul2003:
----------
I tried to run fmpick.pro but had some plotting problems.

The background color was fine but the data colors seemed to be random
(not just wrong color table).  I looked back in the notes from the
previous entry and realized that the change of d.n_colors to
d.table_size in color_plot.pro had been lost.  (This probably happened
because I put that change in color_plot_jmr.pro, which I deleted when
creating the CVS-controlled idl_lib on the server.)

I replace all instances of !d.n_colors with !d.table_size and the
problem went away.

Now, I need to make some usability modifications to fmpick.pro

Done.  I changed it to accept the steps from which to pick peaks as an
array and only ask for the ion name/subsection once.  This technically
makes it less flexible, but not in anyway that would be used for
fitting a forward model peak!  (You would never choose multiple peaks
from the same step or mix and match ions.)

I tried it with He2+, picking from steps 34-46.  This yielded H3=6.98
and H4=0.50.

fmcomp plots to inspect: 52, 70, 85, 91, 100, 112, 121

Yow!  The steps that I did not pick from got worse.  The steps I did
use for picking were great, exactly where I picked them.  I guess I
need to spread out the peaks over a larger step range.

----------
Aside -- Method:
1) Set H3/4 to 1.0 in xfmexp.cc and re-make axfmtweak.
2) Check out fmfit.*.ps.  Look for bad Gaussian fits and lots of
outliers in the final linear plot.  Try taking out steps with bad fits via
the step_range argument and re-running fmfit.  If the fit looks good,
move on.
3) Put the fmfit coef. into xfmexp.cc as H3/4 and re-make axfmtweak.
4) Re-run axfmtweak.
5) Check out FM alignment in fmcomp.*.ps.
6) If necessary, run fmpick to hand select peak centers over a
discontinuous step range.  (The repeat steps 3-5.)
----------

I did fmpick with steps going from 15-45 in 3 step increments.  I got
H3=-0.07 and H4=0.84.  The resulting linear fit looks pretty good.
And, the FM alignment looked better than it has all day.  However,
when compared with the insert-He/fmcomp.*.ps plot, it really looked
the same.  This is interesting, since insert-He was made with fmfit
(auto peak picking).  The constants produced in insert-He were H3=0.40
and H4=0.83.  I guess these are pretty close since offsets (H3) get
rounded to the nearest channel.

Hey, I realized I was making a mistake:  To find the coef., H3
(intercept) should be set to zero.  I repeated the last find method
(using fmpick on steps 15-45 in 3 step increments) and got H3=0.67,
H4=0.84.  Upon putting those into xfmexp.cc and running again
(insert-He-again/), I found the alignment *much* improved.  (My bad!)

===> 22Jul2003 produced hourly prob. rates for whole day

See axlv2/runs/10Jul2003-hourly-pr/.

I looked through the rate plots in axlv2_pr_2002100.pdf and made the
following observations:

100.20 Fe9+,8+ nice peaks; Fe10+ - Fe14+ have nice stair-step of
       maxima
100.30 Same as .20 + nice stair step for C(4-6)+

When compared with the box rtes, the prob. rates are generally more
broken and about 10x more intense (10^-1 maxima rather than 10^-2).
The peaks are in the same locations, at least for update-FM ions (C,
O, Fe).  The intensity increase makes sense, since the prob. rates
include all of E-t space.  The broken nature of the prob. rates may be
a problem.  I'll try running it on a big day.

===> 10Jul2003 re-wrote axlv2.cc to process hourly

After successfully producing hourly boxrates (12Jun2003 entry), I
moved axlv2_boxrates.cc to axlv2.cc thus making it the main branch of
development.  I preserved the old, well-used and trust-worthy axlv2.cc
as axlv2_daily.cc.  I updated CVS with these changes.  

I also copied rp_axlv2_br.pro into axlv2/tools/axlv2_rates.pro and put
it into CVS, since it seemed useful.  When updating CVS, I noticed
that fmcomp.pro had been modified, a fair amount, but not commited.  I
went ahead and did the commit, because I remember doing the mod.s
early this year.  I think the new version still works fine since I ran
it to check the forward model fit for the boxrates (12Jun2003 entry).

===> 12Jun2003 producing hourly boxrates for Thomas to check FM

I looked at axlv2_2hr.cc to see if that was a useable staring point
but found that it was pretty out of date with respect to axlv2.cc,
especially with regard to AnalysisData changes.

I decided to start with axlv2.cc and set it up to produce hourly
boxrates.  Essentially that means running the boxrates calc. after
each analysis interval (set at 5 gives 5cyc*12min/cyc = 60min time res.).

In doing so, I took out some of the debugging features that
no longer made sense, .i.e., 

loading MA -- would take longer than accum. again for such a small
number of cycles

looping over days? -- should this be possible?  What is gained?  I
don't know but I left it in. 

I put the new version in axlv2_boxrates.cc (rather than a new version
of axlv2.cc) since it is not yet complete.

02Jul2003:
----------
The code (axlv2_boxrates) works now.  I had make some modifications to
swindal::AceSwicsData.cc, which made me nervous because I'm rusty.
Changed the time stored in each Pha structure (AceSwicsData::load) to
seconds since 1970 (like libhef) from seconds since 1950.  The latter
was for compatibility with Nathan's code, which has no bearing now.  I
alsow added a conversion function, ss1970_to_doyfr, to convert to day
of year fraction.  It is really just a wrapper for libhef::sec70_time.

I double checked that one analysis interval at a time was being output
into the boxrates file:  There are 29 ions and 58 steps (59 - 2 + 1).
As such, there should be 1682 lines in the file if *one* analysis
interval is represented.  There is one header line and 1683 lines in
the file, which is just right.

I also tried the code over a whole day -- it seems to work fine.  For
2002100, there are 33641 lines in the file:

33641 - 1 header = 33640

33640 / 1682 = 20 (exactly)

So, there should be 100 good cycles in the file, at 5 cycles per
analysis interval, which is appropriate for 1hr resolution
(5cyc*12min/cyc=60min).

Now I need to do the following:

1.  Verify that these boxrates are what I was getting before
2.  Verify the condition of the experimental FM for these ions
3.  Verify that these numbers seem reasonable

I committed the changes to CVS for swindal, though it took some work.
I had never committed the changes I made to switch from the loadPha.cc
function to the AceSwicsData.cc object.

7Jul2003:
---------
Runs are being generated in ~/ace/axlv2/runs/01Jul2003-br-for-tz/.

Addressing issue 3 above:  The boxrate peaks are contiuous.  I wrote
rp_axlv2_br.pro to plot these and demonstrate this fact.  See
axlv2_br_2002100.{ps,pdf}.

9Jul2003:
---------
I checked out the forward model alignment and it looks good, at least
for the big O, C and Fe ions.  See
~/ace/axfmtweak/runs/09Jul2003-boxrates-for-tz/fmcomp.axfmtweak_find_Si_coef_2001001_2001365.ps.

I also added a creation time/date tag and modification of xfmexp.cc
time/date tag to the axlv2_br file header.  And, I put the start ydoy
into the br filename.

To facilitate comparisons between these basic rates and the ones from
the last time I tried to check basic rates, I had rp_axlv2_br write
out a file (axlv2_br_2002100_total.dat) that contains sums of all the
basic rates over the day.  I compared the output with the table given
in 8Aug2002 entry.  They were not the same.  I realized this was
because I am using the experimental FM .  I switched to the
orig. (Hefti) FM and they became closer, but still not the same.  I
then realized that I had found a bug in basic rate weighting on
25Aug2002, making the baseline data hard to use.

Here are a few comparisions:
	   8Aug2002	09Jul2003(orig FM)	09Jul2003(exp. FM)
	   --------	------------------	------------------
20Ne8+	   7.46e+00	8.01E+00		3.25E-01
O6+	   1.15e+01	2.33E+01		2.75E+00
O7+	   2.36e+01	1.57E+01		1.07E+00
C5+	   1.15e+01	1.18E+01		5.97E-01
N5+	   2.80e+00	5.11E+00		8.47E-01
N6+	   1.63e+01	1.27E+01		8.69E-01

I think the alignment between 8Aug and now w/orig. FM are
encouraging -- I think everything is basically on track.  I am a
little distured by the values with the exp. FM.  They have *all* gone
down, which seems suspicious, though not conclusive because this is a
small subset of the values.

10Jul2003:
----------
I moved this code, axlv2_boxrates.cc, to the main branch of
development by renaming it to axlv2.cc (after renaming that to
axlv2_daily.cc).  This makes sense since the boxrate processor is the
model for the full up LV2 processor.  In fact, it doesn't need much in
the way of changes, except for finishing outputLV2.cc -- and making the
swindal code efficient enough to run each hour for a whole day in less
then a day!

I also added outputPR.cc to try to get a look at the prob. rates.

===> 12Jun2003 optimization/improvement suggestions

Thomas and I talked at length about axlv2 and came up with several
ideas for optimization and improvement.  The optimations, at least,
should be applied before testing the method at 1-2hr time resolution.

 o After the boxrates calculation, zero all MA elements outside of
   2-3sigma from a center.  Ideally, do not loop over these entries in
   the rest of the code (Spill, Prob, Distribution)

 o Reduce the total number of MA slices iterated over by selecting
   ranges of interesting steps.  Drop uninteresting steps from further
   consideration.

===> 30May2003 pav improvement for FM revisited

I had a chance to revist the improved pav for the FM (see 25Sep2002
entry) and realized that I had used a bad value.  The following are
the correct values:

date range       a3gxpavdpu      optimal for FM
---------------  ----------      --------------
1998037-2000118	 21.3		 22.8
2000138-present	 24.8682	 25.8682  (26.1 is close)

(Days 119-136 were ramp-up days.

Remember, the optimal for FM values were determined empirically using
axlv2.  See 25Sep2002-long-1998/fmcomp.1998001_1998365.ps for
alignment in 1998.  There are lots of examples of alignment after the
pav change (post 2000138) in axfmtweak/runs.

To get the optimal for FM pav from the a3gxpavdpu value, use the
simple formula below:

fm_pav = 0.85(a3gxpavdpu value) + 4.695

===> 22Oct2002 Fitting new FM coeficients

I developed fmfit.pro to be used in conjunction with axfmtweak.cc and
xfmexp.cc.

Procedure for fitting coeficients:
----------------------------------
--later--

--------------------------
-- Work highlights/tips --
--------------------------

22Oct2002:
----------
The hard-coded, fixed z-scaling in fmcomp.pro must be removed to properly
evaluate the FM alignment.  With the current hard-coded z-scaling Fe
ions are so poorly represented in the plots that it is impossible to
judge the goodness of FM fit.

23Oct2002:
----------
I was confused for some time today because the oxygen esd position
worsened after running fmfit for finding C coef.  I realized my
mistake later:  In wanting to output esd_dpu settings for C fitting,
also outputted them for every other ion (incl. O).  This was because I
set 

ESD    = exp(y) * E2C

Clearly, I should have simply set 

H3[12] = 0.0;
H4[12] = 1.0;

But, now there is another problem:  The C esd alignment actually got worse
with the new coeficients!

31Oct2002:
----------

I confirmed that C esd alignment got worse with the new
coef.  See ins_O_coef_again/ versus ins_C_coef_again.

4Nov2002:
---------
I re-fit Si with Si8+, yielding better results -- go figure.  I also
replotted the 1998 data (better zrange) to see how well Si fit in the
original FM.  

I was confused at first because the orginal FM did not
fit well.  I realized that I was not really putting in the right pav
(22.8), rather the read-out pav.  I put the pav-correction function
into axfmtweak -- for xfm only -- and got better results.

5Nov2002:
---------
Nope, I messed that up.  It was using the corrected pav in xfmexp this
way.

8Nov2002:
---------
I found that, for Fe, I needed to use multiple ions simultaneously and wrote
this into fmfit.  This helped considerably, though I still didn't get
a fit quite as good as Hefti's for states which were not included
(14+, 16+).

I also went back to try to fit Si a little better.  After
experimenting with different ions and step ranges, I tried tweaking
the coef. myself.

11Nov2002:
----------
I repeated the same with Si and got some improvement, though I did not
optimize.

13Nov2002:
----------
Working on N.  The fit seems great, but N6+ overlaps heavily, centers within
1.1 sigma.  I'm going to leave it for now.

Ne:  Looks tough.  Let's just see what comes out.


How do I deal with imperfect fits?  Options:

1)  More precisely pick oxidation states and step ranges (possibly for
each oxidation state)
2)  Pick a likely set of states -- the reasonably abundant ones -- and
pick a decent step range -- so there are one set of points with small
enough spread to be represented by a line
3)  Manipulate H3/H4 by hand after (1) or (2).
4)  Plot new line (3) versus data as in fmfit
5)  I think part of the problems might be due to picking the exact tof
center.  Sometimes the esd vector at the center tof is not really the
max.  I should probably average over a few tof channels around the center.

I tried option (5) above.  This averaging in tof worked great in some
ways.  The esd peaks are much better gaussians (see
find_Si_3/fmfit.axfmtweak_find_Si_3.ps).  That said, the final centers
do not seem to be much improved (see ins_Si_3/fmcomp.axfmtweak_ins_Si_3.ps).
This likely due in part because Si7+ and Si8+ do not fit the same
line.  They are actually two quite distinct lines.  I do not know how
to handle this.

===> 14Oct2002 simple pav improvement

[30May2003:  ***NOTE*** I went back and examine the values used to get
this function and found that one, the 20.5 value was wrong.  It should
have been 21.3.  As such, I calculated another linear equation.  See
entry 30May2003.]

I empirically determined that a different pav than returned by
a3gxpavdpu was needed for optimal FM aligment for two separate nominal
pav settings for levels 127 and 171 (entry on 23Sep2002).  These
values could be determined from the output of a3gxpavdpu via a simple
linear function

fm_pav = (0.7056)*pavdpu + 8.3212	[30May2003:  WRONG!]

Replacing pavdpu with fm_pav in xfm (actually in xfmexp right now)
resulted in optimal alignment for the 1998 data (year accumulated in
MA) and very good aligment in tof for 2001 and 2002.  

I did not try to test this on other pav levels.

===> 8Oct2002 inserting an alternate FM

For axfmtweak:  just put it in axfmtweak.c.

For running axlv2:  In AceSwicsData, make getFM call different FM
functions based on PAPS?  No, that is probably not what we want
anyway.  But, the new FM should probably be in this class.


===> 30Sep2002 questions for Rudy von Steiger

 - What is a good tracer of method performance?  We have be using
O7+/O6+, velocities and C/O.

 - What FM do you use and how were the parameters fit?

 - How are species with spill rates < 0 dealt with?

 - Probablistic rates -- which method?

8Oct2002:  See hardcopy notes.

===> 25Sep2002 PAV effect on FM

I finally got enough information to understand the PAV effect on the
FM.  Here are the salient points:

1)  PAVs of interest

date    read            optimal
----    ----            -------
1998	20.5201		22.8
2001	24.8682		25.8682  (26.1 is close)

[30May2003:  I do not know where I got the 20.5 value above.  I looked
into 25Sep2002-long-1998/run-axlv2.out and see the output that the
read value (from a3gxpavdpu) was 21.3 kV.  When I looked back into
~/ace/paps-history/xpapshist2.1999001-2001365.txt I also see the 21.3
value and not the 20.5 value. As such, I've recalculated the linear
eq. for getting optimal FM pav from a3gxpavdpu value.  See entry 30May2003.
]

The 'read' values are from libhef::a3gxpavdpu().

The optimal value was found by trial and error.

Remember: The voltage 22.8 is close to the average monitored voltage
during the period.  The 26.1 voltage is similar.

2)  The FM is much closer in 1998 than it is now, for whatever PAV,
which I believe indicates that the FM either does not perform well
when the PAV is changed (from that for which the parametes were
optimized) **or** must be optimized to handle changing PAV.

3)  The read out values (20.5201 and 24.8682) both result in *VERY
POOR* FM alignment, making results from these calculations essentially
worthless.

===> 23Sep2002 FM improvement plan

 x verify that FM centers are same as sxd (how could they not be?!)
   23Sep:  Wrote dumpfm.pl.  Spot checked edb's 41 and 46 -- all
   centers same.  Done.

 x determine method for better scaling in fmcomp.pro to more easily
   facilitate peak peaking.
   23Sep: Deferred.  A more suitable range for EDB 45 was chosen
   empirically.

 x run 1998 and 2001
   16Sep2002 - 20Sep2002 -- these runs made

 x choose a step on which to focus (O7+ max step, EDB 40 something)
   23Sep:  Chose EDB 42 (plot 52).  This is near the max of O7+ but
   also has decent peaks for C6+ and C5+ (or N6+).  Of course O6+ is
   still huge.
 
 x assign peaks which have significant counts in step to particular
   ions
   22Oct: Doing this on the fly.

 x re-visit old runs with different pav
   Actually re-ran old different pav runs.  See 25Sep2002 entry.
   Found optimal pav for current FM at each pav level.

 x add code to allow using saved MA (save Eqtab, FM -- maybe fromAnalysisData)
   7-11Oct2002 -- 

 x decipher FM formula
   7-11Oct2002 -- 

 o devise Hefti-like goodness of fit tests
   18-22Oct:  Wrote fmfit.pro which finds experimental esd centers based
   on xfm tof center then fits esd_dpu to these and spits out
   constants, H3 and H4.  See ~/ace/axfmtweak/runs/18Oct2002-writing-fmfit

 o tweak FM parms for step to best fit
   22Oct:  Improved oxygen FM alignment using this procedure.  See
   ~/ace/axfmtweak/runs/18Oct2002-writing-fmfit/inserted_coef

 - repeat (hopefully automated) for remaining steps

 - put new constants into libhef, possibly as different function or option

===> 5Sep2002 re-work of libhef containing code

I found that my recent modifications to AnalysisData, so that it loads
the FM for each day, caused runaway expansion of the ion lists, since
they were initialized along with the FM.  To fix this, I split the eqtab/FM
init from the ion tables and only ran the eqtab/FM for each day.
Unfortunately, the new configuration caused core dumps.  

Rather than fix that situation, I decided to fix the whole problem of
reading from nc files in more than one class.  So on 1Sep, I removed all
libhef calls from AnalysisData and made a new object, AceSwicsData to
collect the Pha vector (formerly in axlv2), Eqtab, pav, and forward
model.  These values are stored into class variables as they are being
read from the nc file (except fm) and accessed via get functions by
AnalysisData.

At first, this seemed to result in the same sort of core dump (which
gdb put at MeasurementArray.cc:70, the beginning of a loop which seems
odd).  Then, after changing what looked to be un-related code, the
core dumps disappeared.  But, the results are now not the same as they
were, e.g., O7+/O6+ = 2.9 )(box for 2002100) and the prob rates are
'nan'.

Summary of info so far:
(Compared 08Aug2002-nobrw with 03Sep2002[...]/box-nobrw.)

- total number of pha is the same, 828136 with 0 thrown out
- number of pha after each read is the same (I diff'd greps of the out
files)
- the same is true with the accumulation time: same at each stage and
total (86020)
- npha used to fill the array in each MeasurementArray::load is the
same, i.e. grep'ing for 'fill' on the out files, which gives nph used
to fill array, ma_sum and time interval info, yields no differences
(I had thought the ma_sums were different.
- a3readcycle returns 134553853 periodically now (from load) whereas
it was 60 in the same instances before
- SW speed is different.  I worked inconsistently before (some times
non-physical, in the 1000's) but now it gives 'nan' after the first one

Later:
------

Fixed.  Numbers of PHA loaded at each step are the same as they were
now.  Everything else is back to normal.  The problem was that the
Eqtab[] was indexed wrong in AceSwicsData::load which caused the FM to
fail and a bunch of other problems.

===> 28Aug2002 long runs for FM centers

There seems to be a new bug which only shows up when multiple days are
run.  In both a 30 day and 100 day run, the boxrates print out
repeatedly then the program crashes or hangs.

Later:

Fixed.  It was because AnalysisData::init() was being run for each day
(to get day's FM values).  This prompted a big switch around where all
instrument-specific code was moved to AceSwicsData in addition to the
existing stuff in Pha.  The function loadPha was moved into the new
class and AnalysisData was given several init/load functions for it's
own ion table, FM and Eqtab.  These call AceSwicsData functions which
either read out of tables stored during AceSwicsData::load() or wrap
libhef functions.

===> 19Aug2002 found BRW bug and maybe another

The basic rate weighting (brw) averages were way to high, on the order
of 10^2.  There turned out to be a couple of problems/issues:

1)  Libhef returns br as a long; Pha and loadPha were treating it as
an int.
2)  loadPha was not init. br[nedb][ibr] *and* was storing it on a per
PHA word basis, instead of once per edb.

I corrected these and the averages dropped down to a much more
reasonable level, around 2.87.  (I wrote
axlv2/devel/tools/sum_avewgt.pl) to calculate this.

21Aug2002:
----------
Now, I'd like to verify these rates against sxd.  

25Aug2002:
----------
Done, though I actually verified them against brwcomp.pl.  Both give the
following

2000158		1.2
2002100		1.3

The two numbers actually differed slightly in the 3rd sig. fig., but I
attribute that to C++/perl differences and am not concerned.

===> 8Aug2002 adding basic rate weighting (BRW)

This changed my state variables slightly, e.g. vO6+ was 301.3 and went
to 298.5.  It changed some ratios a lot, esp. O7+.  Others changed
very little, i.e, (run for 2002100)

NO BRW:

      	     Box	   Spill	    Prob	 Density
----------------------------------------------------------------
   C5+	7.17e-02	1.35e-01	4.70e-01	1.07e-02
   N5+	3.17e-02	3.73e-02	1.47e-01	3.02e-03
   N6+	6.63e-02	9.41e-03	2.03e-02	4.32e-04
   O6+	1.28e-01	1.60e-02	1.49e-01	2.68e-03
   O7+	8.21e-02	2.45e-01	3.50e-01	6.12e-03
20Ne8+	5.34e-02	1.29e-03	2.26e-02	3.02e-04
----------------------------------------------------------------
 ^sum^	4.34e-01	4.44e-01	1.16e+00	2.32e-02
----------------------------------------------------------------
 O7/O6	6.39e-01	1.53e+01	2.35e+00	2.28e+00
----------------------------------------------------------------
   C/O	4.35e-01	6.57e-01	1.21e+00	1.57e+00
----------------------------------------------------------------

BRW:


      	     Box	   Spill	    Prob	 Density
----------------------------------------------------------------
   C5+	1.15e+01	2.03e+01	4.54e+01	1.07e+00
   N5+	2.80e+00	3.37e+00	7.58e+00	1.57e-01
   N6+	1.63e+01	4.06e+00	6.42e+00	1.32e-01
   O6+	1.15e+01	1.55e+01	2.91e+01	5.30e-01
   O7+	2.36e+01	3.02e+01	1.03e+02	1.83e+00
20Ne8+	7.46e+00	8.76e-01	3.63e+00	4.95e-02
----------------------------------------------------------------
 ^sum^	7.32e+01	7.43e+01	1.95e+02	3.76e+00
----------------------------------------------------------------
 O7/O6	2.05e+00	1.95e+00	3.54e+00	3.45e+00
----------------------------------------------------------------
   C/O	4.41e-01	6.22e-01	5.82e-01	7.67e-01
----------------------------------------------------------------


===> 8Aug2002 importing into CVS

-----
Aside: Some useful commands:

cvs tag swindal-0-9-93 (after commit or with -c option for safety)
cvs release dir
cvs diff acetools/a3dif.c
-----

I separated swindal and axlv2, made new make files and got it to
compile.  Then, for each dir., ~/ace/swindal and ~/ace/axlv2, I moved
the misc. stuff into devel/, moved that out, and did

$ cvs -d $ACECVS import -m "Imported sources" swindal shrg start

I also added a few idl save files later, with the '-kb' option to
protect the binary files.

The revisions were automatically tagged as start, but I intend to tag
the set every time I get a good working version.

===> 6Aug2002 status report to kick off working with Thomas

Comparison with sxd: (See 4Jan2002 entry for detailed notes)
--------------------
[Comparisons are O7+/O6+ unless otherwise stated.]

1.  At the level of box rates, *daily-averaged* sxd and axlv2 compare
very well, .60 verus .599.  The difference could easily be
sig. figs.  The term 'daily-averaged sxd' means that the O7+ and O6+
counts are each summed over a day then the ratio is taken. 

2.  With native sxd averaging, the box rates of axlv2 do not compare
well, .72 versus .599.  This makes me wonder a lot about which
averaging is proper.

3.  The densities to not compare well, .84 versus .42.  But axlv2
densities are certainly off, apparently by several orders of
magnitude.

Performance
-----------

1.  FM alignment seems bad; In many cases the FM box is more than 50%
misaligned relative to the peak.

2.  Densities are orders of magnitude off

3.  Velocity comparision, day 2000158

For both sxd and xl2, I read the data into IDL and used 'moment' to
come up with the following averages.  I took the axlv2 values out of
the end of run dump.

2000158:
	   	vO6	vthO6	
sxd		377.9	8.0
xl2		449.3	17.0
axlv2		449.0	24.9	(PAPS 24.87kV)
axlv2		443.6	19.8	(PAPS 25.65kV)
axlv2		443.4	22.0	(PAPS 26.1 kV)

2002100:
sxd		322.0	16.2
xl2		304.4	7.67
axlv2		301.2	15.8	(PAPS 25.65)


===> 5Aug2002 back to checking forward model alignment

In the intervening period, I learned a lot about the different methods
for getting PAPS.  This is explained in detail in
ace-s3-analysis-notes.txt, but here is a summary:  

1) There is a commanded level (returned by a3gxpavlev) which is
converted to voltage using

pav = pavlev*.0811 + 11.0		*COMMANDED*

This is confirmed by the ACE S3DPU Command and Data Document.

2) There is a voltage sensor which measures the actual voltage and
returns a digital value into an HK item, SSCGRPAPS.  This is converted
using

pav = SSCGRPAPS * .15kV			*MEASURED*

Of note is that the measured value varys a fair amount and is not
always all that close to the commanded value.  The commanded value
usually reads about 1.5 kV below the measured one.  The measured value
typically hovers pretty close to the supposed set point of 22.8kV or
26.1kv (depending on which time period).

Now, the only confusion that remains is which one to use.  Simon used
a hard coded value of 22.8 in sxd (before the ramp up) which is
closest to using the *measured* value.  Now, sxd also grabs fm centers
every cycle, but this shouldn't matter since it only takes E/q, pav,
mass and charge as input.  As long as the E/q and pav don't change, I
cannot see how the fm center will change for a particular mass and charge.

===> 5Aug2002 back in the saddle

Ok, I've gotten the computer hacking and operations problems under
control and I'm ready to start working on this in earnest again.

My first work will be to change AnalysisData so that it does not load
the FM from one particular day.  I identified this as one of FM
problems back in May and I vaguely remember this being a problem
earlier this year.

I did this by taking all the code out of the constructor and putting
it in init().  This way, the init() can be called from within the loop
over days.  This allows it to use gCurLv1File for reading FM, pav and
eqtab.  And, since the instance of AnalysisData (ad) is declared
outside of the block in axlv2, it persists after the read-in phase.

Back to checking the forward model alignment.

===> 21May2002 checking forward model alignment

There is no question that the forward model (FM) is not aligned
perfectly with peaks -- even big ones like O7+ in step 41 -- much of
the time.  I would say it is aligned well 30% of the time.

The questions are, then, is this correct?  Is good enough?  Is this as
good as practical?  I aim to find that out this week.

Using libhef::a3gxpavdpu():
---------------------------

This definitely moves the centers:

PAV  Source	      E/q	  Ion		tof,esd
26.1 PDB conv	      2.39	  O7+		344.22, 51.55
24.9 a3gxpavdpu	      2.39	  O7+		352.48, 48.98

though the peak is at about (347, 49) so neither hits it really well,
and the shift shifts right over it.  Interestingly, though, the the
24.9 case, the method shifted the center closer to the peak center.
In fact, in the 26.1 case, the center actually shifts away from the
(visually estimated) peak center.

===> 17May2002 finding good days to run

I loaded all of /home/acedata/xmqh/gif/2002 into xv and just paged
through the year looking for good days to run.  Here are some days of note:

2002019	    heavy and noisy
2002027	    moderate, big non-iron overlap
2002063	    heavy (smear), less noise, big H/He cross talk
2002107     moderate but noisy, esp. big He
2002108	    light

2003049     heavy, not noisy, huge Fe16+ enrichment
            [According to Sue Lepri, this is a typical CME signature.
            The Fe16+ material is attributed to flair material.  An
            additional dist. of Fe9+ is often present, possibly
            signifying a mix of material from two regions.]

2003057     heavy and large noise streak
2003118     very light, esp. iron
2003189     very light, esp. iron, streak

[Updated 22Jul2003.]

===> 14May2002 Understanding method and how to tweek it

I modified axlv2.cc to produce a axlv2_ions_xx.dat file for each step
so that I could compare box, spill and prob rates to see if they
appear to be doing the right thing.

**On paper**, I did some conceptual calculations to see what the
method does under conditions of overlap.  See the 'Overall Method and
General' section in the binder.  This idea may need to be extended to
actual IDL code that shows the prob. rates calc. effect on gaussians
with various degrees of overlap.

I found that small overlaps (< ~1.5 sigma) tend to make the peak
narrower but not move the center.  Overlaps that cross the FWHH
(full-width half height) line tend to decrease the intensity of the
peak significantly and shift it away from the overlap.  This effect is
weighted by spillover-corrected rates such that gaussians with larger
rates effect a particular peak more.

Analysis of method performance:
-------------------------------

Day 2000158
-----------
[Statements of shifts like (-.5sigma, .7sigma) mean that center is
shifted left by about .5 sigma in tof and right by about .7 sigma in
esd.  Sigma refer to the half width of the original boxes in their
respective dimensions.]

step 31 --
---------- 
 Species   BoxRate SpillRate  ProbRate
   Fe11+  7.72e-04  4.58e-05  2.92e-03
   Fe12+  1.12e-03 -2.08e-08  2.41e-03
   Fe13+  1.57e-04  4.67e-09  8.47e-04

In the plot, all of these centers move substantially.  Fe11+ seems to
move to a slightly higher rate region, as shown in the prob. rate.
Fe12+ behaves strangely, and, I think, wrongly:  It shifts about
(-.5sigma,-1.2sigma) but *INTO A LOWER COUNT REGION*.  This last point
is what makes me think this is the wrong behavior.  And, the boxrate,
1.12e-03, is not particularly low.  But, the spillrate comes out
negative, which is why, I think, this large shift takes place.  The
questions is, why does the spillrate come out negative.

step 39 --
----------
      Species   BoxRate SpillRate  ProbRate
          N5+  1.33e-02  2.12e-02  1.87e-02
          O6+  3.26e-02  6.70e-02  6.31e-02
          O7+  1.57e-03  2.68e-03  2.15e-03
       20Ne7+  7.96e-03  1.49e-05  2.45e-05

O6+ seems to behave exactly as I expect:  The center moves from the
edge of the peak diagonally toward the middle (although it lands only
about halfway to the middle).  The counts double.

N5+ moves diagonally toward what I believe is teh O6+ peak as well,
though only slightly.  I don't know if this is the correct behavior.
There doesn't seem to be a peak at N5+ in this step, so this might be
what is expected, though I'd guess it is not ideal.  I probably should
find the step with the N5+ peak to see how it behaves there.

20Ne7+ moves away from the believed O6+ peak.  That seems good.

step 40 --
----------

      Species   BoxRate SpillRate  ProbRate
          C5+  7.61e-03  1.27e-02  1.35e-02
          N5+  3.66e-03  5.48e-03  4.90e-03
          O6+  9.56e-03  1.95e-02  2.13e-02
          O7+  6.78e-03  9.47e-03  9.43e-03
       20Ne8+  1.25e-02  2.22e-02  3.11e-02
        Si11+  8.82e-03  1.24e-03  5.48e-03

C5+ does seem to move closer to a peak -- I don't know if it is the
C5+ peak.  

O6+ moves closer to it's presumed peak.

N5+ moves laterally in this step, not diagonally toward the O6+ peak
as in step 39.  This puts it in a slightly higher rate region, which,
I guess, is expected.

20Ne8+ starts centered on a low rate box and moves up to a much higher
rate one, by about .4sigma in esd.  This seems to make sense.  *But*,
in doing so it moves further away from another apparent peak at
(360,52).  There is no center on this peak, which makes me suspicious
of the forward model in this case.

Si11+ also moved up substantially, 1sig esd, and seemingly away from
the higher count regions.

step 41 --
----------

      Species   BoxRate SpillRate  ProbRate
          C5+  9.40e-03  1.02e-02  1.20e-02
          N6+  1.99e-02  2.78e-02  2.68e-02
          O7+  1.74e-02  2.11e-02  2.25e-02
       20Ne8+  7.12e-03  2.90e-10  4.93e-09
        Si11+  4.51e-03  2.54e-04  7.01e-03

C5+ is exhibiting correct behavior by moving away from overlap and
toward a small peak.

N6+ and O7+ are tough to analyze.  They start with big overlap, right
at 1sigma in both dimensions.  The data shows a similar overlap,
**though the fm centers for both is shifted by about
(-.5sigma,.5sigma)**, which makes me think something is wrong with the
forward model.  The counts go up for both ions, which is in accordance
with the plot.

--------------------

Big Question:  The way I'm plotting this, the sigmas do not change
through the calculation.  I don't think this is true in the actual
calculation.  Even though the prob. rates calc. uses the original
gaussians with the fm sigmas, the formula behaves to change the peak
widths in the prob. rates without direct dependence on sigmas.  (This
seems like a good think to me.  The peak widths have to be flexible
for minimizing overlap while maximizing counts.)  But, my plot is
certainly deceptive in this way.

At this point, I think the procedure is basically working.  But, I'm
increasingly wondering about the line up of the forward model.  I will
now look for other days which may line up better.

16May2002:
----------
[I spent the AM reading Menke, _Geophysical Data Analysis:  Inversion
Methods_.]

Another Big Question: Does MA enter P(tof,esd) calc. anywhere except
boxcounts?  If not, it must be really sensitive to FM *and* centers
will only tend to move away from overlap of peaks that had big
boxcounts.  If the FM missed the peak, I don't think there is anything
that will cause the center to move back onto it, or at least not strongly.

===> 23Apr2002 REALLY CATCHING UP again: building fm comparision plots

[I cannot believe that I have worked on this only a few days in the
last three months.  No wonder it is not done.]

Problems:
1) The forward model centers are hard to read.

2) The intensities are too low in the tof/esd plots

Working on (1) first.

2May2002:
---------

I fixed these by zooming the plots in two separate regions.  The first
covers on the non-iron fm centers and the second covers the irons.  I
also messed around with color plot a lot to get things looking good,
especially black lettering but with a white background.  For this I
had to learn how to set specific values in a color table (see
~/notes/idl_usage_notes.txt) and added an option, zmin_color, to color
plot so I could set the color of the minimum z manually at run time.

I hacked a fix to the intensities by simply specifying a zrange, from
a by-eye estimate.  This may need to be fixed more robustly.

Now the big problem is that the fm centers do not match the
axlv2_fmcomp.dat file in steps.  For example, O6+ has the following
data in axlv2_fmcomp.dat:

eoq  ion tof    tofsig
3.19 O6+ 366.37 4.54
2.97 O6+ 367.85 4.56
2.76 O6+ 369.25 4.58

*but*, fmcomp reports 369.25 for eoq 3.19 (step 39).  I fixed this
partly by getting the eoq in the stdout report from the fmcstep
structure.  This makes fmcomp stdout report the same as
axlv2_fmcomp.dat.  But, I still think the tof/esd data is not matched
up properly, step-wise, with the fmcomp data.

14May2002:
----------
This has been fixed.  The plotting is now usable, though not perfect.
I plot three plots for each step:  full tof/esd, a region covering the
non-iron ions and a region covering the irons.  These regions adjust
themselves automatically.  Most of the counts are in EDBs 28 - 49 so I
usually just plot that range.  The z-scaling is fixed at an eye-balled
value.  That will need to be changed later.

===> 19Feb2002 catching up again and more careful analysis of Ulysses overlap

For comparison, I ran this over a period that overlaps with analysis
of Ulysses data in von Steiger et. al. 1999.  this was
1998037-1998120.  Ulysses was finishing it's first/starting it's second
solar orbit during that period and was far away from the Sun, in the
vicinity of Jupiter's orbit (though I believe Jupiter was not there).

I looked particularly at the C/O ratio von Steiger 1999, the C/O ratio
should not change much from his reported value of 0.670 =/- 0.086.  In
von Steiger 1997 the authors assert that this value should not change
much in fast versus slow streams.

Nathan's suggestions:

1) Run a similar timeseries for sxd (ellipses, ell+br, full) and plot

2) Put in BR correction

3) Build graphical tools to examine E-T matrix slices to find location
of fwm and spill correction.  Run for slices (charge steps) where a
lot of ions are coming in.

- wondering: is spillover correction too aggressive?  

===> 18Jan2002 100 day time series

28Jan2002:
----------

I wrote run-axlv2.pl to run axlv2 over a series of days then used this
to runn from day 2000102 to 2000200.  (Day 2000101 had problems during
the 100 day average run (14/15Jan2002).)

The run seemed successful, in that there are a whole bunch of files in
the 18Jan2002.  I need to come up with a meaningful way to plot the
results.
 

===> 14Jan2002 100 day (average) run

28Jan2002:
----------

The run was successful.  Surprisingly, the o7+/O6+ ratio actually went
down to 0.18.


===> 4Jan2002 comparing with modified sxd results
[note: this entry was originally in sxdcomp2/notes.txt]

Filename:		   mean,var	Description
x2000158_flag0.sxd	   .84,.20	sxd with xelletm flag set to 0
x2000158_f0v0.sxd	   -NaN,-NaN	_flag0 with vH set to 0.0
x2000158_try3.sxd	   .7232,.1612	flag 0,calc vH, no brw, no dj, no eff,
x2000158_try4.sxd	   .7219,.1579	try3 plus $v_i = 1.0, $den=$nmom
x2000158_try5.sxd	   same		try4 but zero vH, call
					xelletm, restore vH
x2000158_try6.sxd	   .7218,.1579	try5 plus ?? (crap, I got
					interrupted and didn't write
					it down!)
*** switched to svsbare2.pl to better control and use native daily averages ***

x2000158_hefell.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
359.58  214.92  27.83   17.67   24.83   
O7+/O6+ = 0.60

x2000158_ell.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
359.58  214.92  27.83   17.67   24.83   
O7+/O6+ = 0.60

x2000158_box1.svsbare2:
-------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
155.08  89.08   13.17   6.75    9.33    
O7+/O6+ = 0.57

x2000158_box2.svsbare2:  changed boxes to be center +/- sigma like swindal
-------------------------------------------------------------------------
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     
494.33  294.42  42.33   23.00   34.75   
O7+/O6+ = 0.60

Wow, that really lines up with the others.  So, to recap., with
stripped sxd (sxdcomp) I get O7+/O6+ = .72.  With the same function in
svsbare2 I get .60.  Now, the only difference should be the cycles
averaged at the end (sxdcomp) versus native daily averages in
svsbare.  This needs to be double checked.  Wait, there is one more
difference:  sxdcomp uses counts while svsbare uses rates (divided by
12s).  This should be equivalent in ratios.

To test this, I put 'elipsecount', from svsbare2, into sxdcomp and
used that for $tcnt.  Result:  O7+/O6+ = .7218 with var .1579 -- this
is IDENTICAL to the last sxdcomp run with Simon's a3xelletm.  All
right!

Let's try some more variations of sxd to nail down what each of those
steps does to the ratio:

elipses (below) are from a3xelletm() unless otherwise noted.  Also,
O8+ correction for half elipse is left in place unless otherwise noted.

Filename:	   mean,var	Description
x2000158_try7.sxd  .7218,.1579	elipses only
x2000158_try8.sxd  .7218,.1579	elipses plus vel. filter
-----
This vel. filter did not change O7+ and O6+ at all but did get rid of
one count of Fe5+ in a number of cycles.  There could have been other
small changes as well, but my cursory glance only found Fe5+.
-----
x2000158_try9.sxd  .7218,.1579	elipses plus br weighting
x2000158_try10.sxd .7218,.1579	elipses plus dj
x2000158_try11.sxd .6852,.1422	elipses plus O and Fe eff
x2000158_try12.sxd .7150,.1508	elipses plus phase-space density f
x2000158_try13.sxd .7232,.1612	elipses plus velocity weighting in sum (v_i)
x2000158_try14.sxd .7208,.1564	elipses plus density
x2000158_try15.sxd .8365,.1989	all on (like sxd)
-----
This last result, with everything on like in sxd, exactly matched the
ratio calculated from the archived sxd file.  Good!
-----
x2000158_try16.sxd .8365,.1989	sxd minus vel. filter
x2000158_try17.sxd .6798,.1361	previous - brw
x2000158_try18.sxd .6798,.1361	previous - dj
x2000158_try19.sxd .7161,.1503	previous - eff for Fe and O
x2000158_try20.sxd .7232,.1612	previous - phase space density conv
x2000158_try21.sxd .7208,.1534	previous - vel. weighting (v_i)
x2000158_try22.sxd .7218,.1579	previous - density conv

Ok.  Let's take a different tack.  I'm going to switch to native daily
averages and do this again.  The program is
sxdcompda.pl.  All I did is accumulate (by ion) the $den which sxd
normally prints out ion every in every cycle.

Output files are names x2000158_tryDD.sxdcompda, where DD is the try
number below.

Other columns:
IDL-ave		IDL-averaged O7+/O6+ ratio; per cycle values read from
		file then summed and divided
natda		native daily average; dividing accumulated rates
		(summed from sxd $den variable)

Try #  IDL-ave	    natda explanation
-----  -------      ----- -----------
1      .7218,.1579  .60	  try22 with new program (native daily ave)
2      .8365,.1989  .67	  all of sxd
-----
Note: the IDL-ave results are identical to sxd.
-----
3	.8365,.1989  .67  sxd - vel filter
4	.6798,.1361  .56  prev - brw
5	.6798,.1361  .56  prev - dj
6	.7161,.1503  .59  prev - eff for Fe and O
7	.7232,.1612  .59  prev - phase-space density conv
8	.7208,.1564  .59  prev - vel weighting
9	.7218,.1579  .60  prev - density conv
10	.8923,.2307  .71  prev + brw
11	.7218,.1579  .60  try9 + dens conv
12	.7150,.1508  .60  prev + phase-space dens conv
-----
Now we are right back where we started, with sxdcompda set up like
swindal.

I finally figured out why the native and IDL averaging are different.
In IDL, I took the ratio of each pair, O7+ and O6+, for each cycle,
then averaged the ratios.  I swindal, and now in sxdcompda, I sum up
the values of each for all the cycles, then take the ratio.  Clearly,
these will be different numbers, as one is the sum of the ratios and
the other is the ratio of the sums.

So here is a summary of the comparison of swindal derived results to
sxd (or sxd like) derived results:

Two types of averaging:
ave1)  calc O7+/O6+ ratio for each cycle --> ave. ratios
ave2)  sum 07+ and O6+ counts (separately) over cycles --> take ratio
       of sums

sxd varient		ave1	ave2
-----------		----	----
straight sxd		.84	.67
sxd - brw		.68	.56
sxd - vel filter	.84	.67	affects some iron slightly
swindal-like		.72	.60
swindal-like + brw	.89	.71
swindal-like + dens	.72	.60

Swindal results from runs/18Dec2001/run2, ***all ave2 style***:

             Box           Spill            Prob         Density
----------------------------------------------------------------
   C5+  2.51e-02        3.19e-02        3.55e-02        5.74e-04
   N5+  3.01e-02        4.75e-02        4.40e-02        6.24e-04
   N6+  4.54e-02        5.49e-02        5.35e-02        7.54e-04
   O6+  7.04e-02        1.38e-01        1.36e-01        1.67e-03
   O7+  4.22e-02        5.83e-02        5.82e-02        7.11e-04
20Ne8+  3.10e-02        2.74e-02        4.01e-02        3.91e-04
----------------------------------------------------------------
 ^sum^  2.44e-01        3.58e-01        3.67e-01        4.72e-03
----------------------------------------------------------------
 O7/O6  5.99e-01        4.23e-01        4.28e-01        4.27e-01
----------------------------------------------------------------
   C/O  3.26e-01        2.65e-01        3.32e-01        4.43e-01
----------------------------------------------------------------

Nathan's opinion of this is that I should run a longer time series,
say 100 days.

I'll set to doing that.

===> 17Dec2001 Catching up with Nathan

Nathan feels we need to 

a) have data in a good presentation format (ions table)

b) know exactly what is going on with sxd

I'm working on the former right now.

===> 14Dec2001 Brainstorming the data simulator

 - input to axlv2 via ma.dat file
 - 

===> 5Dec2001 Adding probabilistic centers

I tried several methods from several sets of equations I had written
along the way.  I finally settled on calculating them hand in hand
with Nprob, since all the same elements were needed.

Right now, they are all identically zero so I'm debugging around the
oxygen box in edb 41:  340 <= tofch <= 348 , 46 <= esdch <= 57 .

10Dec2001:
----------

Ok, now they seem to work and I have the report table looking like I
want.  Unfortunately, there are a lot of these that are 'nan' from 0.0
prob. rates.  And, there are a lot that are > 1 sigma different from
the forward model.  I think the only way to understand this now is to
write the data simulator.

===> 3Dec2001 adding banner and version
[I haven't been able to work on this in 2.5 weeks.  Ugh!]

I did this just like libsms -- use the compiler strings __DATE__ and
__TIME__ and pass in the version with -DSWINDAL_VERSION= .  This stuff
is set up in initSwindal.cc.  I wanted to get rid of that routine, but
I'm beginning to think I'll just have to make it called
automatically.  That would be close enough.

===> 15Nov2001 cleaning it up

I added a number of flags to turn on the various rates calculations
and such. I also wrote MeasurementArray filter stuff and put that in
place.  (I tested the passRange function first of course.)

I made an input section which reads all the flags and settings from
the terminal so I could pass these things in via an input file.

It seemed to be working on all but the directory name, but now it is
not. I figured out it does not like the comment strings I left in the
file.  

Ok, now it works.  I added 'getline' commands after each input to read
through the comment.

===> 5Nov2001 catching up with Nathan.

I showed Nathan the following output:

axlv2 -I- writing comparison tables:

             Box           Spill            Prob         Density
----------------------------------------------------------------
   C5+  2.51e-02        3.19e-02        3.55e-02        5.74e-04
   N6+  4.54e-02        5.49e-02        5.35e-02        7.54e-04
   O6+  7.04e-02        1.38e-01        1.36e-01        1.67e-03
   O7+  4.22e-02        5.83e-02        5.82e-02        7.11e-04
----------------------------------------------------------------
  sum:  1.83e-01        2.83e-01        2.83e-01        3.71e-03
----------------------------------------------------------------
 O7/O6  5.99e-01        4.23e-01        4.28e-01        4.27e-01
----------------------------------------------------------------
   C/O  3.26e-01        2.65e-01        3.32e-01        4.43e-01
----------------------------------------------------------------

He had a few comments:

1.  He realized the the Spillover correction is not normalized, so all
the numbers will increase.

2. The fact that Spill and Prob are so close *is an excellent*
indication that things are working reasonably, since they are
completely different methods.

3. 

Nathan's suggestions for further work were:

1.  Clean up the package so it is ready to be run a lot.  That means
making neat output files, including the probabilistic centers.

2.  Make a data simulator using the prob. function and a few ions (one
charge step).  Include a calculation of relevant ratios and such. 

3.  Pump the simulated data through the method via a loaded MA and
compare.

4.  See if I can output that simulated data in a form that can be
pumped through sxd for comparison.

===> 24Oct2001 deleting species from S

After noticing that all the counts seemed to increase from box to
spill, Nathan and I came to the conclusion that species with a
negative Nspill (at a particular edb) should be removed from S.  The
new, smaller S should be re-inverted and the rest of the
calc. finished.  Note:  Nathan recommended *against* taking out
species in a second round thinking that the matrix would become
unstable.

I misunderstood and removed the species from all S (at all edbs) which
resulted in *all* species being removed from the analysis.  (Oops!)
This work is saved in

save -I- saving...SpillRates.cc as save.d/SpillRates.24Oct2001-12:45:26-1.cc 
save -I- saving...SpillRates.h as save.d/SpillRates.24Oct2001-08:41:44-1.h 

Now, I will try to re-write this again, this time writing a routine to
do all calc for a particular EDB with a wrapper routine that loops over
the edbs, takes out species and re-runs it.

5Nov2001:
---------
Done (between many other intervening tasks).  It turns out I needed to
write an assignment operator to be able to copy ions to a new group.

And, I found out that for vectors

vector<float> v;
v.reserve(MAXEDB);
size =  v.size();
cap  =  v.capacity();

In the preceeding code, size  = 0 and cap = MAXEDB.  In order for size
= MAXEDB, I needed to either load elements via v.push_back() (and omit
the v.reserve()) or v.assign(MAXEDB, 0.0) then assign them
individually with v[s] = value.

This was a problem because I used size() in the Ion::operator=
overloading but it wasn't set since I had used reserve() followed by
individual assignment to fill the forward model vectors.  I added
reserve() and assign(MAXEDB,0.0) to Ion::Ion so this wouldn't be a problem.

===> 22Oct2001 catching up with Nathan

 - looks like axlv2 differs from sxd by about 200,000 K, 1.7 vs. 1.9
 MK.  This is only about 20% and not all that bad

 -  He suggests finding the t,e centers of the prob. rates, like we
 talked about 17Aug2001.  I wrote this out in hardcopy notes (same
date and title as this entry).

 - The v problem (from DistFunc results) is likely just unit
conversion.  I checked and I am not doing any.  Nathan's ballpark
figure is 437 ;m/s * sqrt(E/q).  I should double check that.

===> 22Oct2001 Thomas' thoughts about comparison with SXD

 - .82 is likely a CME ratio, 0.3 - 0.5 is more 'normal'
   - CMEs mean more energetic particles which means more background
   - Simon's MMQ (emailed) does have some filters so these energetic
   particles may not show up there.
   - Since my MMQ (no filters) don't seem to have lots of spurious
   counts, background is likely not a problem

 - sxd's phase space correction corrects for the fact that more counts
 are received as the velocity goes up, even with the same populations
 of particles.  (If you literally sped up a particular time period,
 the instrument would count more.)  This is for primarily two reasons:

 1.  delta E/q (acceptance width) increases with E/q.  Faster
 particles would enter at higher E/q levels.

 2.  since the collection (integration) time at any E/q is fixed, more
 counts are received when particles are moving faster.  (More
 particles enter per second since they are moving faster.)


===> 16Oct2001 new MeasurementArray::fill method - no MAcounts

I changed fill so that it would do the following, thus avoiding having
a 70Mb MAcounts lying around just for accumulating counts:

1. change MA from rates to counts
2. loop over PHAs, accumulating with + 1.0 into corresponding MA
   location
3. get time for list of PHAs (analysis interval worth)
4. add that time to global accumulation time (gAccumTime)
5. change MA back from counts to rates by dividing all by gAccumTime

Now, compared to a save MA before this transformation, the new one is
4 bytes longer.  (See runs/17Oct2001 for both.)  When the sums over
slices were loaded into IDL, I found that the zeros are all in
*exactly* the same places.  They appear the same when plotted except
that the new one is a little lower intensity.  I took the difference
between the two and found that a) it was in the .0001 - .001 range and
b) not exactly uniform, i.e. the whole matrices are not related by a
constant factor.

I compared running fill as described above (MA as rates) to running it
without steps 1 and 5 (MA as counts).

Everything seems to be consistent in the new version.  When I run fill
with MA as counts and divide the ma sum by gAccumTime by hand
after execution, I get the same sum as when done with rates.  The same
holds true for the box rates.  If I take a box rate and multiply by
the last reported gAccumTime, I get the same box count as when I run
fill with MA as counts.  I got the same behavior when run over an
entire day.

I think this is behaving correctly.  I never compared MA as rates with
svsbare.pl so I'm not sure it was exactly right before anyway.  I'm
going to move on and consider this working fine for now.

===> 15Oct2001 new ProbRates methods

Ok, now that I have gotten rid of the MQ and MM arrays for conversion,
there should be plenty of memory to run ProbRates with MA still in
memory.

First, I'll get rid of the block in axlv2 that destroys MA after
accumulation.  Done, works fine.

Note: I stored two copies of MA (all ranges and range 1 only) in
runs/15Oct2001/{all_ranges, range1_only}.  I should be able to use
these without accumulating again..  I think I will do the same with a
15 day run.

[Recap.]  Here's the plan.  The new ProbRates scheme will have upper
and lower bounds, based on two different methods (but both
incorporating MA back into it):

1)  upper bound -- For each location in MA, give counts to the species
with the highest prob. at that location.

2)  lower bound -- For each location in MA, the each species a fraction
of the counts equal to it's probability at that location.

(2) is easy, but (1) requires a search.  I could just do this simply
mindedly and loop.  I'll decide after I count the number of possible
searches.

16Oct2001:
==========
According to Numerical Recipes, p. 344, the fastest way to do the search
involved with (1) is to just loop over all elements.  

RAM Problems:
-------------

In writing code for (2), I notice that axlv2 now uses 312Mb RAM when it
hits ProbRates.  Geez!  It makes sense, because though I'm saving
about 140Mb from MM/MQ, I'm now keeping MA even after ProbRates is
declared.  I think I'll have to get rid of MAcounts and go to the
method which Nathan suggested of multiplying by accum. time, adding
counts, then dividing.

ProbRates seems to use a fair amount of memory.  P and G themselves
are 30Mb each.

It turns out I had taken out all references to MM and MQ -- EXCEPT
in the header and constructor!  Geez.  That's why it was using so much
memory.  Now, is is using 189Mb when it hits ProbRates, which is much
more manageable.  I still might get rid of MAcounts, but at least it
isn't an emergency now.
-----

Prob. rates are too small, the O7+/O6+ ratio is 0.07.  Also, when I
run for just one edb (39 or 41) they are all 'nan'.  Since the O6+
boxrates peak in edb 39 and those of O7+ in edb 41, this cannot be
right.  In all of the ProbRates stuff, there is no crosstalk between
EDBs so the ones that held the max boxrates should also hold the
highest probability.

I'm wondering if the nan results are genuine numerical precision
problems.  Looking into

/usr/lib/gcc-lib/i386-redhat-linux/2.96/include/float.h

I can see that floats can be btween 1.17549435e-38 and 3.40282347e+38
. Since some of the spill rates are 10^-14, I'm wondering if this
might really be the problem.

I think if you add a nan to a number you get a nan.  That would
explain these problems.  When I put a threshold (1.0e-5) on P to
qualify for adding to Nprob, the prob rates came back.

18Oct2001:
==========



===> 9Oct2001 looking into MMQ conversion by funtions again.

This does not currently work.  It seems that if I could get it to work
then I could reduce the memory load significantly before sending MA
into ProbRates for the changes to that method.

1) I took out the range 1 filter so all PHAs would be included and the
MMQ plot would look normal.

2) I split up outputMats into outputMA and outputMMQ to save
processing time.  I also made it output the summed mmq using all three
conversion methods, the matrices, eqte2mmq and mmqbyhand.

The mmq plot from the matrices (MM and MQ) looks fine but the others
are zero.  I'm going to try putting the edb 41 O7+ box through the
converters to see if they are even close.

EDB 41 O7+ box:  340 <= tofch <= 348 and 46 <= esdch <= 57

These are completely off.  All masses are in the 10^30 range and all
m/q in the 10^13 range. *OBVIOUSLY* this is wrong since they should
all be around 16.0 and 2.3, resp.

Previously, I checked all the constants in mmqbyhand and eqte2mmq to
be sure that they the same as those actual arrays (via the
getparams.pl program).

I double checked the equations -- they are the same.

Just for fun, I removed the exponentiation, ie,

mm = lnm
mq = lnmq

That brought the numbers into a much closer ballpark.  For example,
for ntof=345, nesd=50, mm=69.88 and mq=31.46.  The MM and MQ matrices
have values of 14.39 and 2.246, resp.

I tried multiplying the pav by powers of ten -- it increased.
Dividing it by 10.0 made it go negative.  I guess I could find a
factor by trial and error but I don't see what that would prove.

I tried putting in the pav from a3gxpavdpu() -- only about a 3% change
down, so this is not the problem.

I tried decreasing the E/q by using a lower numbered EDB, this made
the mw value increase while the mm value stayed mostly the same.
Besides, I double checked E/q values while doing the sxd/svsbare
validation and they were identical to values from libhef/perl.

11Oct2001:
----------

I fixed a problem copying in the constant arrays in eqte2mmq and now
it gives the same results as mmqbyhand, which is nice.  Of course,
they are still the wrong results.

So, that means my constants are all ok in mmqbyhand.

I tried changing the ntof to try to match the proper mass -- ntof=135
is close.  This makes no sense.

I tried changing the E/q -- no E/q (with a huge mass) would get the
m/q into the ballpark.

15Oct2001:
---------

Ok, I finally figured it out and it works.  There were two major
problems:  1) the results from libhef's dswxlnmq and dswxlnm were in
CHANNELS!  Geez, I should have known that.  2) the constants arrays in
eqte2mmq were not static but only loaded on the first run and, thus,
only produced a valid result the first time. 

Anyway, I ran it for a whole day and it compared well with the
matrix.  Note that individual values were not identical, but that
could easily because of digitization error (I think).  The values were
allways within a few tenths (for O7+) and the whole plot does not
appear to be shifted.

I'm removing the HUGE matrix stuff.

===> 9Oct2001 end-to-end testing for reasonable results (not validation)

1) I went through and put a lot of the output statements used for the
sxd validation behind if(DbgLvl >=... statements.

2) I added 12s to the timeInterval to account for the missing time of
the last cycle (since cycle times are for the beginning of the cycle)

3) I made MA rates again (from counts for the validation with sxd)

DistFunc still dies so I turned it off for now.

Here is a summary of results for axlv2 right now.  I saved the
axlv2_ma_2000158_2000158.dat file in runs/9Oct2001:

O7+ 0.453501
O8+ 0.044666
axlv2 -I- BoxRates O7+/O6+ = 0.584714

O6+ 1.58706
O7+ 0.612553
axlv2 -I- SpillRates O7+/O6+ = 0.385967

O6+ 1125
O7+ 783
axlv2 -I- ProbRates O7+/O6+ = 0.696

This calculation took 26 minutes.  Analysis: The BoxRates ratio looks
consistent with all the svsbare validation.  (The sxd ratio is .82
when hourly averaged counts are averaged.)  The SpillRates ratio seems
low.  I would think since O7+ is less abudent that it's counts would
go up while O6+ would stay about the same, making for a ratio that is
higher than the BoxRates.  (This is consistent with Nathan's
thinking.)  On the other hand, the ProbRates ratio looks better, so
things may be working somewhat.

===> 24Sep2001 validating with SXD

Nathan feels we should nail this down first.  So, he suggests 1)
figure out exactly what sxd is doing and 2) run libhef boxes and/or
swindal elipses to be sure we are comparing apples to apples.

Here are some runs with different counting schemes:
---------------------------------------------------
shorthands: accidental coincidence filtering	ACF
	    basic rate weighting		BRW
	    geometric factor			GF

sxd.pl uses elipses, ACF, BRW, GF
factor, 
0.836477     0.198850

svs using boxcounts with BRW (no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_box.svs', data=svs
read_sxd: reading data...
     0.793559  6.38987e-10    -0.373765    -0.487334

svs using boxcounts (no BRW, no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_wgtbox_1sigma.svs', data=svs
read_sxd: reading data...
     0.793566  5.79648e-09     0.605941      19.5108

svs using boxcounts and 10sigma (no BRW, no ACF, no GF)
IDL> o7o6, file='~/ace/swindal/x2000158_box_10sigma.svs', data=svs10
read_sxd: reading data...
     0.793559  6.38987e-10    -0.373765    -0.487334

[I'm confused that these values are changing so little so I'm going to
try to intentionally distort them and see what happens.]

svs using boxcounts with tof/esd centers mult. by 5.0

... Ok, I had a logic problem in the box counter which I only
discovered after stripping the routine down to nothing (svsbare.pl).
Here are some more real results:

name: ./x2000158_wgt_halfsigma.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
779.77  557.18  295.91  169.78  450.83  787.44  
O7+/O6+ = 0.71


name: ./x2000158.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
726.08  540.00  310.17  188.08  452.92  753.33  
O7+/O6+ = 0.74

name: ./x2000158_wgt_2sigma.svsbare
nO6 O7 O8 nFe8 nFe9 nFe10 [counts/sec]
o_6     o_7     o_8     fe8     fe9     fea     
1510.37 1090.85 610.10  368.34  903.84  1512.89 
O7+/O6+ = 0.72

Combining these with my old results from swindal:
		     (1/2)s	    s		2s
boxrates(libhef)     .71	    .74		.72
Boxrates	     .51	    .57		.59
Spillrates	     .49	    .55		.81

-----
25Sep2001:

Working with Nathan, we decided to compare with svsbare.pl very
carefully.  We found a few problems
 - Eqtab was being pushed and was thus offset from forward model and
   everything else -- FIXED
 - loadPha was running over one more cycle than AnalysisInterval
 - BoxRates::calc could have exceeded MA bounds 


-----
26Sep2001:

For 5 cycles, axlv2 reads 52987 PHAs.  MeasurementArray::fill examines
this same number of PHAs and the elements of MA sum to this same
number (even in getSlice later).  I think MA is ok.

svsbare.pl reads exactly this same number of PHAs as well.

Also, for the first cycle in the file, 

 rec 73576, Mon 05-Jun-2000 (157) 23:59:47

There are exactly the same number of PHAs in each EDB in both
programs.

-----
28Sep2001:  Further comparision with svsbare and tracking of PHAs
through swindal to explain difference in boxrates (with svsbare)

Summary of tracking/comparing PHAs for first cycle of 2000158.nc:

 - loadPHA reads 10587 PHAs
 - those PHAs all make into MA
 - number of PHAs in each slice (of MA, from outputMA run after
   Boxrates::calc) is exactly the same as the number for the
   corresponding edb when read in *and* exactly the same in svsbare
 - BOTTOM LINE:  The correct number of PHAs in the correct slice are
   definitely making it into Boxrates::calc.

Examining Boxrates::calc:

 - the boxes look like the right sizes (approx.) -- they were not exactly!
 - in edb 41, swindal gets 21 counts compared to 37 in svsbare
   Since the same number of counts at edb 41 went *in* to both box
   counters (526), the box counting procedure has to be the culprit.
 - while it is possible that svsbare is wrong, the results, across
   edbs, looks more reasonable: There is a notable peak (prev/subsequent
   edbs having counts which lead up to the max) and there are a few
   counts away from the peak.

3Oct2001:
---------

I finally got svsbare.pl and swindal to produce the same boxcounts for
the first cycle of 2000158.nc.

I put in "by hand" box counters in BoxRates::calc and in
svsbare::boxcount.  These had boxes of 338 <= tofch <= 350 and 44 <= esdch
<= 59.  In swindal this box (called o7box) found 36 counts; in
svsbare::boxcount it found 37 (round off difference, ok).  But, I
realized that these boxes *WERE NOT* the exact sizes that would come
from the foward model (when rounded to integers).  These boxes would
be 340 <= tofch <= 348 and 46 <= esdch <= 57.  (I calculated these by
hand from the forward model When I set both "by
hand" box counters to these ranges, they came up with exactly the same
results: 21 counts.  

It turns out, there was a problem with my counting logic in
svsbare::boxcount which was counting PHAs which fell into the esdch
box only.  (Geez!)

When I increased the number of cycles to 5, they two programs still
gave the same answers. They were also the same for 10 cycles (one
analysis interval).

When I run the two for a whole day, the results are close but not
identical.  This is because axlv2 reads more PHAs (993396) than
svsbare (979356).  This may be related to running axlv2 in multiple
analysis intervals.  I'm looking at the totals as they increment with
each cycle to compare.  (I've tabled this examination for now.)

I wrote a simple elipse counter (elipsecount).  It came up with less
counts but the same ratio (.6) as the box counter.  (See svsbare_wd_el.out
and x2000158_wd_el.svsbare.)

I changed the sigmas to sqrt(sigma).  It came up with less counts
again but a similar ratio to the others (.57).

Nathan and I resolved that I should push my method through to
completion now and deal with the sxd differences later.

===> 20Sep2001 found error in SpillRates, got ProbRates working

SpillRates:  Added - sign in numerator of Btil calc. (result of my
algebra mistake)

ProbRates:  Nathan made two suggestions after which I got actual
numbers,

1)  if |tof - fmtof| > 3stof (or likewise for esd), set G = 0.0

2)  Instead of summing the prob. into Nprob, add integer counts
whenever the prob. > PCutOff.  I set this cutoff to 0.7 and got actual
values.  Now, there aren't rates but O7+/O6+ = .77  .

He also suggested that I might be interpreting the foward model sigmas
wrong, either as twice or half what they should be.

We also came up with a better method for doing the prob. rates, which
actually amounts to two methods which will provide a high and low
bound.  These also integrate the measurements *back* into the rates directly.

1)  upper bound -- Foreach location in MA, give counts to the species
with the highest prob. at that location.

2)  lower bound -- Foreach location in MA, the each species a fraction
of the counts equal to it's probability at that location.

===> 12Sep2001 referencing to sxd

I fired up read_sxd.pro from sxd/devel/read_sxd.pro and read in the
data from 2000158.  I then calculated the ave. O7+/O6+ ratio for the day. 

read_sxd, file='/home/acedata/sxd/2000/x2000158.sxd', data=sxd
o7o6 = sxd.no7/sxd.no6
m = moment(o7o6, /nan)   ; /nan causes nan values to be discarded
print, m

I stuck these commands into a macro called o7o6.

The result was (ave, std. dev, ...)

    0.825485     0.212449      2.04040      7.80170

axlv2 results were pretty far off:

BoxRates      .5732
SpillRates    .9031

I also added functions for calculating these ratios withing axlv2.

===> 11Sep2001 mmq has grid of zeros

[I was distracted by the terrorist attack on the World Trade Center
and Pentagon today and ended up going home to be with Stacey while she
waited to hear from Tommy.

He had been on Wall St., just 2 small blocks from the WTC. While
watching the smoke from the first strike, he saw the second crash and
had to take cover.  He was kept in his building by police until after
the 1st tower collapse.  His building was hit with debris causing
shaking, plaster falling and screams from the occupants.

He fled the building and was a few blocks away when the second tower
collapsed, having to run from the ensuing smoke cloud.  We didn't hear
from him until about 1.5 hrs after the second collapse.

He ended up shaken up but ok.  
]


Briefly, after long run there is a regular grid of zeros in the mmq
plot but not in the ma plots.

I ran it again over the bastille day event, 200019?, and stored it in
runs/11Sep2001.  The MA look similar but with higher count levels and
much higher background noise.  The mmq plot was compared similarly --
including the zero grid.

12Sep2001:

I realized that the mmq plot was transposed relative to the axis
labels/scale. This resulted in a large spot at mass 6, which was the
tip-off that something was wrong.  (I did not believe Li was hugely
prevalent in the solar wind that day.)

This turned out to be a mistake of mine in two places: 1) The order of
indices for mmq in MeasurementArray::EtSlice2Mmq.  2) I had
inadvertently made the labels inconsistent with the order of indices
so that *the labels/scale* looked ok.  I switch the indices  and the
mmq plots look like they should now.

===> 10Sep2001 He rates 0

After running over all of 2000158, the He rates were still zero.  This
*has* to be a problem.  Other rates seemed reasonable:

axlv2 -I- dumping boxrates...
He1+ 0
He2+ 0
C4+ 0.0221252
C5+ 0.151242
C6+ 0.0735497
N5+ 0.0831191
N6+ 0.190478
O5+ 0.0119596
O6+ 0.499359
O7+ 0.286246
O8+ 0.0343599
20Ne6+ 0.00850973
20Ne7+ 0.0408466
20Ne8+ 0.1362
Si7+ 0.0189513
Si8+ 0.0386851
Si9+ 0.0443424
Si10+ 0.0452623
Si11+ 0.0925483
Si12+ 0.0764949
Fe6+ 0.000459982
Fe7+ 0.00354191
Fe8+ 0.0119595
Fe9+ 0.0184454
Fe10+ 0.0216654
Fe11+ 0.0215735
Fe12+ 0.0183536
Fe13+ 0.0137074
Fe14+ 0.00929175
Fe15+ 0.0128332
Fe16+ 0.0281963

Just to check, I dumped He rates per edb, they were still all exactly
zero:

He1+ 0( 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
He2+ 0( 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)

This cannot be right.

My plan is to examine the relevant region (by eye) of the tof-esd
matrix and see if there are counts there.  According to AnalysisData
(with DbgLvl=4):

He2+	tof		esd
	178 +/- 2.21	66.0 +/- 2.52
	339 +/- 4.20	17.8 +/- 1.82

I used the the following idl commands to produce a nice postscript
plot of MA (all slices summed):

.run smrp.pro
@loadct
psplt, file='ma.ps', /color
smrp, file='axlv2_ma.dat', data=ma
hardcopy, file='ma.ps', label='swindal testing', printer='file'

loadct contains the following two lines

;this idl macro is used in the r* macros in this dir
loadct, 33

I realized that I am only selecting region 1, which, I believe, drops
He.

[I updated smrp.pro and smmq.pro to include postscript plotting to a
file of the same name as the .dat matrix file but with a .ps
extension.]

===> 10Sep2001 (E/q, tof, esd) to (mass, m/q) conversion using functions

The functions dswxlnm and dswxlnmq calculate mass and m/q (resp., from
acetools/asxdpusim.c) from (E/q, tof, esd) for a given PHA word.  The
only problem with using them, once I get the calling sequence straight
(!), is that they also require a pointer to the adcnq array.  I'll
have to think about how to use this array from libhef directly.

After some searching, I discovered that I had worked on this already
when trying to get conversion info. for Nathan.  In

/usr/home/jraines/ace/nal2/parameters-for-nathan/

See

constants.txt

and 

getparams.pl

9Oct2001:  See ~/ace/libhef/notes.txt

===> 6Sep2001 ProbRates are too small

[The results are from a run over 2000158 - 2000165.  I had similar
results from just 2000158.]

Main problem:  The Gs are very small because 

Other things that don't look right:

     BR                SR
He1+ 0                 -4.38139e-08
He2+ 0	               2.03353e-09

Fe6+ 7.84662e-30       -2.37105e-09
Fe7+ 5.29323e-08       1.038e-07

Fe15+ 1.21767e-11      -3.10869e-05
Fe17+ 0                -2.63465e-05
	
Ok, I figured out one of the problems:  I was doing the normalization
(to counts/sec) wrong in MeasurementArray::fill.  I was simply
accumulating into MA locations then dividing those values by the
accumulation time during each analysis interval.  

This is fine for one analysis interval, but for the next one the
process causes counts to be added to rates, then the sum is divided by
accumulation time -- resulting in *very* small numbers indeed.

I fixed it by making a separate matrix for each, MA for the rates and
MAcounts for the counts.  The latter is then only used for
accumulating counts through one analysis interval. 

===> 24Aug2001 work log -- debugging SpillRates

First the elements of S were all zero.  I was using integer division
(1/2) which returned zero.  Then, (I swear) the spill rates were
giving reasonable values, but some were negative.  Finally , after
accidentally deleting some code, they are large (70 - 100).

Wait a minute.  I commented out code that tests the matrix inversion
and the spill rates came back down.

It turns out that declaring an ofstream 

ofstream fout;

is the culprit.  I don't know why.  If I use 

cout << P; 

to avoid the ofstream, everything is fine.

===> 22Aug2001 work log -- passing big objects

 - I figured out how to overload << properly; declare as friend (found
 in Deitel and Deitel).

 - realized that functions with pointer arguments (e.g. the .calc functions)
 should receive constant references, e.g.

   const vector<Ion>& grp

 That way, they are not copied in (no overhead) but a) they cannot be
changed in the function and b) they are called looking as though they
are called by value.  The latter makes sense since they are not
changed, contrary to the implication of passing them in as &var.  See
Stroustrup section 5.5 and 7.2 for more info.

 I tried to change the MA arg. in BoxRates.calc, but it won't allow
any sort of constant:

const MeasurementArray& MA		// recommended first choice
const MeasurementArray* MA		

all cause an error:

BoxRates.cc: In method `int BoxRates::calc (const MeasurementArray *, 
const vector<Ion, allocator<Ion> > &)':
BoxRates.cc:95: passing `const MeasurementArray' as `this' argument of 
`TNT::Matrix<float> MeasurementArray::getSlice (int)' discards 
qualifiers

Declaring a const. pointer, i.e. a pointer which holds a constant
address, works but does not accomplish the same thing:

MeasurementArray *const MA

I tried to fix SpillRates.calc and got the same results for
BoxRates::getBoxrates.  Since it works with the STL-vector group, I'm
guessing it is something wrong with the way my functions are
declared.  

For now, I will just pass the rates into the subsequent functions, the
arrays are only about 10kb and only get copied in once for each type
of rate (minimal overhead IMO). At least this doesn't make it look
like they are modified.  I changed all of the 'group' passing to
const. references.

16Oct2001:
----------

Will adding use of MA to ProbRates::calc, I tried the same thing,
i.e., passing MA as a constant reference.  It compiled.  Note:  I had
not yet modified the code to actually *use* MA in anyway.

As soon as I tried to use MA, e.g.,

MA.getElement(...)

I got the same error as above.

Deitel and Deitel p. 454 explained this and offered a solution.
Apparently, you can not call member functions of objects passed as
const (reference or not) unless the member functions themselves are
declared constant.  

I changed the member to constant:

float MeasurementArray::getElement(int nedb, int tofch, int esdch) const;

float MeasurementArray::getElement(int nedb, int tofch, int esdch) const {
...
}

And the error went away.  Deitel and Deitel to the rescue again.
It goes on to recommend that member functions should be declared const
if they don't need to modify the object -- meaning that many of the
ones in swindal certainly could be.

Stroustrup p229-30 also explains this somewhat.  It also explains that
passing by const reference is the preferred method for passing large
objects to functions which will not modify them. 

===> 15Aug2001 work log -- model tof/esd out of range

He2+:
tof 170.0 +/- 2.108 -> 347.9 +/- 4.314
esd 72.73 +/- 2.582 -> 16.90 +/- 1.800

O6+:
tof 193.7 +/- 2.402 -> 400.0 +/- 4.960
esd 184.0 +/- 8.197 -> 37.33 +/- 4.990

O7+:
tof 179.3 +/- 2.224 -> 370.4 +/- 4.592
esd 218.6 +/- 8.624 -> 44.06 +/- 5.214

Fe9+:
tof 293.1 +/- 3.634 -> 601.7 +/- 7.461
esd 182.0 +/- 12.98 -> 28.46 +/- 4.875

I looked carefully at AnalysisData::AnalysisData and realized that I
was using a3gxpavdpu to get the post-accel. voltage (PAV).  When I was
working with sxd.pl a few months ago, I discovered that I did not
understand the output of that function -- it returned voltages which
were different than the commanded ones, e.g., (pav is command voltage)

a3gxpavdpu returned 24.868179
pavlev=171 pav=26.100000

a3gxpavdpu returned 21.299759
pavlev=127 pav=22.800000

Now, it could be that those lower voltages take into account the
losses to the foil, but the offset doesn't seem to be quite constant
(could it be a linear function?)

Whatever a3gxpavdpu really returns, I am not using it (for now).  I
set up a look up table of known levels to commanded voltages

  map<int,float> pavlev2kv;
  pavlev2kv[127] = 22.8;
  pavlev2kv[171] = 26.1;

and use a3xpavlev to get the level

  PapsVolt = pavlev2kv[a3xpavlev()];




===> 14Aug2001 work log -- inversion

Got NR svdcmp and svbksb to work and solve a 2x2 linear system.
(Yeah!)  It worked write out of the book except I printed the solution
vector values (floats) as %d which gave confusing results until I
discovered the error.

Got inversion working for a 2 x 2 matrix.

Figured out dim() and size() in TNT:

A.dim(1) gives size of 1st dimension
A.dim(2) gives size of 2nd dimension
A.size() gives A.dim(1)*A.dim(2) 

See exp/matdim.cc for example code.

Got Invert working for test case.

===> 13Aug2001 work log -- spillover aborts

BoxRates.calc() causes abort because the tof2tofch returns out of
bounds toflo.  This could be fixed but it doesn't solve the problem.
I wrote exp/phaconv to explore the range of the function and found
that it easily exceeds the size of MeasurementArray. 

So, it looks like I'll have to implement the variable resolution that
we talked about earlier and a different scheme for mass and m/q
conversion.

===> 27Jul2001 work log -- mmq plotting

I'm trying to find that good color table that I used before.  So far
the following are decent:

27 Eos B	  great but white axis
33 Blue-Red 2	  great but blue background and axis
13 Rainbow	  fine but black background

===> 23Jul2001 work log -- m-m/q matrix
[Updated 24Jul2001.]

Plan:
----

 - make log-scaled binning matrix
   - 

 - cycle through each grid point in E-T slice
   - convert (esd,tch) -> (m,m/q)
     - Which?
       - equations
       - axdpusim
       - store from pha read in (keeps instrument specific stuff in
       that module) into a look up table
         - that would be another 70Mb cube!!  Bad idea.
         - It looks like I will go with the look up matrix for now to
	 keep things moving.

   - increment value at (m,m/q) by one




===> 19Jul2001 work log -- making a long run

I created longrun.cc which is xboxrates.cc with looping over data
files from start to finish.  This was not all that difficult, except
for some fooling around with char arrays vs. strings.  I ended up
using char array then copying it to gCurLv1File (string).

I created a function for incrementing the day properly (incrementYdoy)
and made sure the rest of the functions were skipped if loadPha
returned an error.  loadPha was already set up to handle multiple
files and that (actually!) worked as it was.

The only other problem was mysterious core dumping.  Apparently I had
not done 'make clean' on libhef since the upgrade and this was the
cause of the problem.

I had to add an include in hefnum/hefnum.i because DBL_MAX was not
defined.  I found it (with find/grep) in
/usr/include/kpathsea/c-minmax.h.  Apparently, Kpathsea is a directory
searching package included with TeX stuff, such as Xdvik.  I have a
hard time believing that Simon would use this arbitrary package, I'm
guessing this used to be included so I will continue searching on pooh
to find what (must have) been the original reference.  If worse comes
to worse, I'll hardcoded it myself in libhef.h.

I ran from 2000150 - 2000165.  It didn't crash!

plotting:
---------

I moved smrp to the swindal directory but can't get the thing to plot
in color again.  I knew I should have figured it out better back in June.

27Jul2001:
I was using color table 33 (Blue-red 2) and writing to a PS file with

psplt,/color, file'tmp.ps'

I gave up trying to use the screen graphics.

===> 15Jun2001 work log -- why does tof vs. esd plot look random

 - I checked that the arrays are being referrenced the same between
 C++/TNT and IDL, this includes checking that the array is read into
 IDL right.

 **********WRONG*********See IDL item below

 - MeasurementArray::fill seems to be adding to the correct MA
element:

MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=
MeasurementArray::fill-D- before add MA[41,324,0]=0
MeasurementArray::fill-D- after add MA[41,324,0]=1
...
MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=0
MeasurementArray::fill-D- before add MA[41,324,0]=1
MeasurementArray::fill-D- after add MA[41,324,0]=2
...
 MeasurementArray::fill -D- dumping meas. array coord.:  nedb=41 tofch=324 esdch=0
MeasurementArray::fill-D- before add MA[41,324,0]=2
MeasurementArray::fill-D- after add MA[41,324,0]=3
...

 - this seems to progress normally: (grepped for the add lines only)
...
MeasurementArray::fill-D- before add MA[41,324,0]=3
MeasurementArray::fill-D- after add MA[41,324,0]=4
MeasurementArray::fill-D- before add MA[41,324,0]=4
MeasurementArray::fill-D- after add MA[41,324,0]=5
MeasurementArray::fill-D- before add MA[41,324,0]=5
MeasurementArray::fill-D- after add MA[41,324,0]=6
MeasurementArray::fill-D- before add MA[41,324,0]=6
MeasurementArray::fill-D- after add MA[41,324,0]=7
MeasurementArray::fill-D- before add MA[41,324,0]=7
MeasurementArray::fill-D- after add MA[41,324,0]=8
MeasurementArray::fill-D- before add MA[41,324,0]=8
MeasurementArray::fill-D- after add MA[41,324,0]=9
MeasurementArray::fill-D- before add MA[41,324,0]=9
MeasurementArray::fill-D- after add MA[41,324,0]=10
...
MeasurementArray::fill-D- before add MA[41,324,0]=57
MeasurementArray::fill-D- after add MA[41,324,0]=58
MeasurementArray::fill-D- before norm MA[41,324,0]=58
MeasurementArray::fill-D- after norm MA[41,324,0]=58
(normalization off)

 - for fun I grabbed slice 41 and printed element (324,0) in
xboxrates:

  a = MA.getSlice(41);
  cout << thisprog << " -D- MA[41,324,0]=" << a[324][0] << endl;

output:

xboxrates -D- MA[41,324,0]=58

I figure it out and it was one big stupid mistake:  I tested IDL with
symmetric matrices.  So, when I transposed them I got the same
elements as in C++.  But, my region of interest was not symmetric, so
when I read it in transposed it cycled the elements around.  To fix
it, I changed my IDL code to 

a) read in a matrix with the dimensions flipped

mat = fltarr(coldim,rowdim)

readf,lun, mat

b) transpose the array

mat = transpose(mat)

Before the transpose, accessing element (324,0) does not give 58 (as
it is known to be from C++).  After the transpose, several elements
matched perfectly.  (I tried (324,0) and (328,19).  There should be no
way for them to match by coincidence.)


===> 3Jun2001 work log -- using TNT and LAPACK++

I decided to use TNT (Template Numerical Toolkit) for most of the
numerical matrix stuff.  It is organized in a way very similar to the
STL so it's use is easy to learn.  

There are two caveats:  

1) It is a beta version.  This seems ok since the swindal code will be
under development for the next several months anyway so changes to TNT
could be incorporated relatively easily.

2) It does not (yet I presume) include SVD.  I plan to use it's
precursor, LAPACK++ for that.  I considered using just the latter, but
it seems to make more sense to use TNT since it is current and seems
cleaner to use.  I can isolate the LAPACK++ stuff to a few routines,
most likely.  Or, I could use Numerical Recipes code for SVD.  Either
way, I think it is better to use the up and coming package in a new
project, rather than the one that is being replaced.

===> 21May2001 work log -- making boxrates arrays
===> 8May2001 work log -- running out of memory

loadPha runs out of memory after loading 530612 PHA words (about
half way through day 2000158).  I checked with top, it reported
xboxrates using 66M before it ran out of memory and exited.

Checking memory usage:  Each Pha should use

10  doubles	4 bytes ea.	 40 bytes
2   floats	2 bytes ea.	  4 bytes
7   int		4 bytes ea.	 28 bytes
1   string      ? bytes ea.       0 bytes
-----------------------------------------
				 72 bytes / Pha

530612 * 72 = 38 204 064 =~ 38M

This sounds possible.

Nathan and I decided to analyze PHAs in 5 cycle blocks per now (with 5
being a parameter).

===> 2May2001 work log -- member iterators?

Should iterators be declared in the class with the data object or
locally?  For example, in AnalysisData, if an iterator to the Ions
map<string,Ion> is declared in the local code where it is being used,
then the type of Ions is hardcoded in to that local code.  This
doesn't seem too encapsulated.  Come to think of it, since Ions is
public, the type of this is hardcoded anyway whenever it is used (by
syntax, since a key subscript is used for access).

I guess the encapsulation is pretty much gone unless I make the data
private and access it all with public functions.  Is this extra work
worth it in this context?

I'm saying no.  However, I will do the little bit of extra
encapsulation that is provided by declaring the iterator in the class.

===> 29Apr2001 work log -- no at() member function for vector

I could not resolve this issue:  Both Stroustrup and Josuttis (The C++
Standard Template Library) refer to an at member function for STL
vectors.  This function provides range-checked random access.
However, the library on hobbes (libstdc++-2.96-54) does not seem to
contain this function.  I searched all headers in /usr/include/g++-3
and found no mention of it.  And, I manually looked through all of the
public member functions in stl_vector.h.  This library version (2.96)
is more recent than the last on mentioned in the libstdc++ section on
gnu.org, so I don't think it is out of date.  And, the SGI STL site
does not include at in vector's member functions.  Also, compiling
with g++ on login.engin.umich.edu yields the same result as hobbes,
that there is no at member function.

Go figure.  I'm moving on.

===> 29Mar2001 Names

Michigan Solar Wind Plasma Data Analysis Library
M S W P D A L

Michigan Solar Wind Data Analysis Library
MiSoWiDAL
MSoWiDAL

31Jul2003:
----------

I forgot to write out the chosen name:

Solar WINd Data Analysis Library -- Swindal

===> Keep-at-Bottom Too Done from To Do/Known Issues

------------------------------ Too Done ---------------------------------------

x Jan2004 - Re-derive moment and DF equations
x Mar2004 - add diagnostic data/plots per timestep (E-T, MMQ)
x Jan2004 - add MMQ FM and prob. centers
x Jan2004 - Check out C5+/C6+ for Ian Richardson -- prelim. done
x Fe in FM needs adjustment -- replaced with physics FM (Jan 2004)

x 30Sep2002 -- why using USWX deltaE/q = .043 instead of Simon's of
  .035 from sxd (see dist. func. calc.; top of file lists .043)?  
  Using 0.064 as per George Gloeckler and the ACE paper.

x 04Sep2003 - fix prob. centers?  They seem to be messed up now, but
  have they been superceded by plotting P slices?

x 04Sep2003 - re-work E-T plotting
   x re-write fmcomp to use modular parts like fmpick (should be easy!)
   x Add a range 0 (H/He) subsection to plot_ets/fmcomp
   x remove Prob. centers from subsection boundry determination?
   x Change axlv2_fmcomp.dat format to get rid of stuff no longer used
     (make it like axfmtweak format)?
   x split fmcomp and prob. center data; build an IDL prob_center
   routine on the basis of read_fmc so that it can be overlayed on fmcomp
   plot if present.  This would allow nice fmcomp without running full method.

 x 08Sep2003 - add pmulti-like keyword to plot_p_slices.pro
   Just add !p.multi = [0,2,2] before running it.

 x 05Sep2003 - change P slices to be plotted at max spill rate edb for
   each species.  ProbRates::calc loads SR anyway and this is the proper
   place to find the max.  Use count-weighted average of s to find s
   for max.

 x 08Aug2003 -- add something like MA's filter to AnalysisData for
determining whether to include range 0 ions.

 for Boxrates for TZ
 -------------------

 x 02Jul2003 - Verify that these boxrates are what I was getting before
 x 02Jul2003 - Verify the condition of the experimental FM for these ions
 x 02Jul2003 - Verify that these numbers seem reasonable
      07Jul -- They are continuous peaks, see axlv2_br_
      09Jul -- The fit of O, Fe and C looks good; so this is good enough.
 x 02Jul2003 - If easy, change boxrates to box counts for Thomas.  
   (I guess I could multiply by the number of cycles * 12s/cyc.)
 x 07Jul2003 - Build a plotter for these rates from the matrix rate
	       plotter I just made.

 for FM esd correction
 ---------------------
 x figure out why Carbon gets worse after C5+ fitting.  See 
   axfmtweak/runs/18Oct2002-writing-fmfit/ins_C_coef_again/{fmcomp,fmfit}.*.ps

 x fix fmcomp so that it looks at both fm centers and pc centers to
   determine plotting frames

 x 5Aug2002 - disentangle libhef and ACE-specific stuff from
   AnalysisData.  This libhef stuff could be put in class Pha (along
   with loadPha while we are at it).  The values could still be stored
   in an AnalysisData object, by passing an instance into loadPha and
   using a bunch of set functions for pav, fm, etc.  (Care would need
   to be taken to minimize the risk of misaligning fm data and the
   ad.ions vector.)


 x 16May2002 - check out forward model:
   o using right PAPS; use Simon's?
   o read FM technote
   o run on other, nominal days
   o trace through code to eliminate mishandling


